{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W_KnfMBQ2Hb"
      },
      "source": [
        "<h1><center> Passage Based Question Answer Model </center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqVRz-PoQ2He"
      },
      "source": [
        "<h3><center> NLP project\n",
        " </center></h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxJs3lCctaWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7af55ec-10b5-4d00-bd69-3fce063a46e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.10/dist-packages (from PyDrive) (2.84.0)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.10/dist-packages (from PyDrive) (6.0.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.22.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->PyDrive) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->PyDrive) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->PyDrive) (4.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.9)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive) (1.61.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.2->PyDrive) (5.3.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.2->PyDrive) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*italicized text*# New Section"
      ],
      "metadata": {
        "id": "3V5o0wQ0cNc8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moqAv_5-bky9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16709da6-e5a9-47b8-f21b-ea5e4c125efb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==1.0.3\n",
            "  Downloading pandas-1.0.3.tar.gz (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "!pip install --user pandas==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZCu-VFh9xJ8n"
      },
      "outputs": [],
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1vXUB_l_xOAb"
      },
      "outputs": [],
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXhH7VMG2L2a"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qVic_jGHW3KB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "938c6696-3139-4e32-fd38-4645e6072418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xgSo872S2Q54"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import json\n",
        "from pandas.io.json import json_normalize\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xLLFDob2pU4"
      },
      "source": [
        "**Load the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_json('/content/drive/MyDrive/NLP/train-v2.0.json')\n",
        "dev = pd.read_json('/content/drive/MyDrive/NLP/dev-v2.0.json')"
      ],
      "metadata": {
        "id": "fPRlGFh35D4D"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-qiDKfl15PmT",
        "outputId": "ce138bf9-4a12-46c7-a336-c778737e52d3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  version                                               data\n",
              "0    v2.0  {'title': 'Beyoncé', 'paragraphs': [{'qas': [{...\n",
              "1    v2.0  {'title': 'Frédéric_Chopin', 'paragraphs': [{'...\n",
              "2    v2.0  {'title': 'Sino-Tibetan_relations_during_the_M...\n",
              "3    v2.0  {'title': 'IPod', 'paragraphs': [{'qas': [{'qu...\n",
              "4    v2.0  {'title': 'The_Legend_of_Zelda:_Twilight_Princ..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-45d297bd-93b4-41cf-bb54-2bf4ecf54d60\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>version</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Beyoncé', 'paragraphs': [{'qas': [{...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Frédéric_Chopin', 'paragraphs': [{'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Sino-Tibetan_relations_during_the_M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'IPod', 'paragraphs': [{'qas': [{'qu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'The_Legend_of_Zelda:_Twilight_Princ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-45d297bd-93b4-41cf-bb54-2bf4ecf54d60')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-45d297bd-93b4-41cf-bb54-2bf4ecf54d60 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-45d297bd-93b4-41cf-bb54-2bf4ecf54d60');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-765357dd-a681-4044-a80e-234e5a128e9b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-765357dd-a681-4044-a80e-234e5a128e9b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-765357dd-a681-4044-a80e-234e5a128e9b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of data =\",train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF1tGLyx5Qnx",
        "outputId": "a76bfd77-3bcf-464e-e605-64e486c971e4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data = (442, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "aKu-Ixei5ZsS",
        "outputId": "0c14a0a5-f060-43dd-ae9b-c28581c52c44"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  version                                               data\n",
              "0    v2.0  {'title': 'Normans', 'paragraphs': [{'qas': [{...\n",
              "1    v2.0  {'title': 'Computational_complexity_theory', '...\n",
              "2    v2.0  {'title': 'Southern_California', 'paragraphs':...\n",
              "3    v2.0  {'title': 'Sky_(United_Kingdom)', 'paragraphs'...\n",
              "4    v2.0  {'title': 'Victoria_(Australia)', 'paragraphs'..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a115714d-5fcb-4e32-8276-3a5782398ece\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>version</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Normans', 'paragraphs': [{'qas': [{...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Computational_complexity_theory', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Southern_California', 'paragraphs':...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Sky_(United_Kingdom)', 'paragraphs'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Victoria_(Australia)', 'paragraphs'...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a115714d-5fcb-4e32-8276-3a5782398ece')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a115714d-5fcb-4e32-8276-3a5782398ece button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a115714d-5fcb-4e32-8276-3a5782398ece');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7aa860cd-22b5-45f2-9d59-1c8c2973f54f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7aa860cd-22b5-45f2-9d59-1c8c2973f54f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7aa860cd-22b5-45f2-9d59-1c8c2973f54f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of dev data =\", dev.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghvoQomv5ZbY",
        "outputId": "4201491c-925b-4871-dc6a-ad3b1ce75ef6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of dev data = (35, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieUiy50SZYlD"
      },
      "source": [
        "Put dev file in a dataframe to be readable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KxlcVcxYZddv"
      },
      "outputs": [],
      "source": [
        "#This has multiple answers for same questions unlike traing data\n",
        "def dtodf(dfile, record_path = ['data','paragraphs','qas','answers'],\n",
        "                           verbose = 1):\n",
        "    #dfile: path to the squad json file.\n",
        "    #record_path: path to deepest level in json file\n",
        "    #verbose controls level of detail of progress messages\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Reading the json file\")\n",
        "    file = json.loads(open(dfile).read())\n",
        "    if verbose:\n",
        "        print(\"processing...\")\n",
        "    # parsing different level's in the json file\n",
        "    js = pd.io.json.json_normalize(file , record_path )\n",
        "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
        "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
        "\n",
        "    #combining it into single dataframe\n",
        "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
        "    m['context'] = idx\n",
        "    main = m[['id','question','context','answers']].set_index('id').reset_index()\n",
        "    main['c_id'] = main['context'].factorize()[0]\n",
        "    if verbose:\n",
        "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
        "        print(\"Done\")\n",
        "    return main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "o8Dz1GI2x0Bw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da0bd1e6-b0fd-49c1-9da9-f395ecd7a342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading the json file\n",
            "processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-e6a7f4e12b41>:14: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead.\n",
            "  js = pd.io.json.json_normalize(file , record_path )\n",
            "<ipython-input-7-e6a7f4e12b41>:15: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead.\n",
            "  m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
            "<ipython-input-7-e6a7f4e12b41>:16: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead.\n",
            "  r = pd.io.json.json_normalize(file,record_path[:-2])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of the dataframe is (11873, 5)\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "dfile = '/content/drive/MyDrive/NLP/dev-v2.0.json'\n",
        "record_path = ['data','paragraphs','qas','answers']\n",
        "dev1 = dtodf(dfile=dfile,record_path=record_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6Z1mjT20yMXr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "9b3dded1-eded-4d5e-fb28-2f489db78da6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                         id  \\\n",
              "0  56ddde6b9a695914005b9628   \n",
              "1  56ddde6b9a695914005b9629   \n",
              "2  56ddde6b9a695914005b962a   \n",
              "3  56ddde6b9a695914005b962b   \n",
              "4  56ddde6b9a695914005b962c   \n",
              "\n",
              "                                            question  \\\n",
              "0               In what country is Normandy located?   \n",
              "1                 When were the Normans in Normandy?   \n",
              "2      From which countries did the Norse originate?   \n",
              "3                          Who was the Norse leader?   \n",
              "4  What century did the Normans first gain their ...   \n",
              "\n",
              "                                             context  \\\n",
              "0  The Normans (Norman: Nourmands; French: Norman...   \n",
              "1  The Normans (Norman: Nourmands; French: Norman...   \n",
              "2  The Normans (Norman: Nourmands; French: Norman...   \n",
              "3  The Normans (Norman: Nourmands; French: Norman...   \n",
              "4  The Normans (Norman: Nourmands; French: Norman...   \n",
              "\n",
              "                                             answers  c_id  \n",
              "0  [{'text': 'France', 'answer_start': 159}, {'te...     0  \n",
              "1  [{'text': '10th and 11th centuries', 'answer_s...     0  \n",
              "2  [{'text': 'Denmark, Iceland and Norway', 'answ...     0  \n",
              "3  [{'text': 'Rollo', 'answer_start': 308}, {'tex...     0  \n",
              "4  [{'text': '10th century', 'answer_start': 671}...     0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1cf14096-8a43-4401-9cf7-0d17fa669348\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answers</th>\n",
              "      <th>c_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56ddde6b9a695914005b9628</td>\n",
              "      <td>In what country is Normandy located?</td>\n",
              "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
              "      <td>[{'text': 'France', 'answer_start': 159}, {'te...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56ddde6b9a695914005b9629</td>\n",
              "      <td>When were the Normans in Normandy?</td>\n",
              "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
              "      <td>[{'text': '10th and 11th centuries', 'answer_s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56ddde6b9a695914005b962a</td>\n",
              "      <td>From which countries did the Norse originate?</td>\n",
              "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
              "      <td>[{'text': 'Denmark, Iceland and Norway', 'answ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56ddde6b9a695914005b962b</td>\n",
              "      <td>Who was the Norse leader?</td>\n",
              "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
              "      <td>[{'text': 'Rollo', 'answer_start': 308}, {'tex...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56ddde6b9a695914005b962c</td>\n",
              "      <td>What century did the Normans first gain their ...</td>\n",
              "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
              "      <td>[{'text': '10th century', 'answer_start': 671}...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1cf14096-8a43-4401-9cf7-0d17fa669348')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1cf14096-8a43-4401-9cf7-0d17fa669348 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1cf14096-8a43-4401-9cf7-0d17fa669348');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fff8dfa1-e0f8-4bd8-a015-05057072b264\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fff8dfa1-e0f8-4bd8-a015-05057072b264')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fff8dfa1-e0f8-4bd8-a015-05057072b264 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "dev1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFHqUVue3CS2"
      },
      "source": [
        "**Put the training json file into a dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GtEM0XtK-hGw"
      },
      "outputs": [],
      "source": [
        "def ttodf(dfile, record_path = ['data','paragraphs','qas','answers'], verbose = 1):\n",
        "    #dfile: path to the squad json file.\n",
        "    #record_path: path to last level in the json file\n",
        "    if verbose:\n",
        "        print(\"Reading the json file\")\n",
        "    file = json.loads(open(dfile).read())\n",
        "    if verbose:\n",
        "        print(\"processing...\")\n",
        "    # parsing different levels in the json file\n",
        "    js = pd.json_normalize(file , record_path )\n",
        "    m = pd.json_normalize(file, record_path[:-1] )\n",
        "    r = pd.json_normalize(file,record_path[:-2])\n",
        "\n",
        " #combining it into single dataframe\n",
        "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
        "    ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
        "    m['context'] = idx\n",
        "    js['q_idx'] = ndx\n",
        "    main = pd.concat([ m[['id','question','context']].set_index('id'),js.set_index('q_idx')],1,sort=False).reset_index()\n",
        "    main['c_id'] = main['context'].factorize()[0]\n",
        "    if verbose:\n",
        "        print(\"\\nJson has been successfully converted into a dataframe\")\n",
        "    return main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KOEawOiNLu4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbe81d69-efd3-475b-a09d-3d0b9afe3387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading the json file\n",
            "processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-4312e9927db0>:19: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only.\n",
            "  main = pd.concat([ m[['id','question','context']].set_index('id'),js.set_index('q_idx')],1,sort=False).reset_index()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Json has been successfully converted into a dataframe\n"
          ]
        }
      ],
      "source": [
        "dfile = '/content/drive/MyDrive/NLP/train-v2.0.json'\n",
        "record_path = ['data','paragraphs','qas','answers']\n",
        "train1 = ttodf(dfile=dfile,record_path=record_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TDrY8Vq4XMR"
      },
      "source": [
        "**Checking Structure of the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HAWgC5nGMpqM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "7948874a-5f21-4d46-d62b-5176c14e58e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      index  \\\n",
              "0  56be85543aeaaa14008c9063   \n",
              "1  56be85543aeaaa14008c9065   \n",
              "2  56be85543aeaaa14008c9066   \n",
              "3  56bf6b0f3aeaaa14008c9601   \n",
              "4  56bf6b0f3aeaaa14008c9602   \n",
              "\n",
              "                                            question  \\\n",
              "0           When did Beyonce start becoming popular?   \n",
              "1  What areas did Beyonce compete in when she was...   \n",
              "2  When did Beyonce leave Destiny's Child and bec...   \n",
              "3      In what city and state did Beyonce  grow up?    \n",
              "4         In which decade did Beyonce become famous?   \n",
              "\n",
              "                                             context                 text  \\\n",
              "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...    in the late 1990s   \n",
              "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  singing and dancing   \n",
              "2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...                 2003   \n",
              "3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...       Houston, Texas   \n",
              "4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...           late 1990s   \n",
              "\n",
              "   answer_start  c_id  \n",
              "0         269.0     0  \n",
              "1         207.0     0  \n",
              "2         526.0     0  \n",
              "3         166.0     0  \n",
              "4         276.0     0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-32b67099-dd7f-4e03-a541-5d1fb07cd97a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>text</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>c_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>269.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>207.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56be85543aeaaa14008c9066</td>\n",
              "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>2003</td>\n",
              "      <td>526.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
              "      <td>In what city and state did Beyonce  grow up?</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>Houston, Texas</td>\n",
              "      <td>166.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
              "      <td>In which decade did Beyonce become famous?</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>late 1990s</td>\n",
              "      <td>276.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-32b67099-dd7f-4e03-a541-5d1fb07cd97a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-32b67099-dd7f-4e03-a541-5d1fb07cd97a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-32b67099-dd7f-4e03-a541-5d1fb07cd97a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-82f512f6-cfc7-426b-b26e-e7347d9e0623\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-82f512f6-cfc7-426b-b26e-e7347d9e0623')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-82f512f6-cfc7-426b-b26e-e7347d9e0623 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "train1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8FK8zamUrBe"
      },
      "source": [
        "\n",
        "**Text Data Pre-processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dWx76Gz4oT7"
      },
      "source": [
        "Data dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "r6GbGR0HT1yn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2373ec03-049a-4bdd-dc41-babfe6eae759"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data = (130319, 6)\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape of data =\", train1.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHT3hkap4ua9"
      },
      "source": [
        "Checking for Null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2UYGfG3XUxEH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f17920f-f1fa-4143-bdf6-6a5ced6b6077"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "index               0\n",
              "question            0\n",
              "context             0\n",
              "text            43498\n",
              "answer_start    43498\n",
              "c_id                0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "train1.isnull().sum()\n",
        "#no null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tsYQ-IU2Vf9Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1472b2bd-5c24-47d9-b4fc-8a753ce0b64d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['When did Beyonce start becoming popular?',\n",
              "       'What areas did Beyonce compete in when she was growing up?',\n",
              "       \"When did Beyonce leave Destiny's Child and become a solo singer?\",\n",
              "       ..., 'What is another name for anti-matter?',\n",
              "       'Matter usually does not need to be used in conjunction with what?',\n",
              "       'What field of study has a variety of unusual contexts?'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#take a look at the text present in the columns\n",
        "train1['question'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GbBSLHBNbM8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e7b524b-1521-40a1-9fbe-bd5c17beab0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
              "       'Following the disbandment of Destiny\\'s Child in June 2005, she released her second solo album, B\\'Day (2006), which contained hits \"Déjà Vu\", \"Irreplaceable\", and \"Beautiful Liar\". Beyoncé also ventured into acting, with a Golden Globe-nominated performance in Dreamgirls (2006), and starring roles in The Pink Panther (2006) and Obsessed (2009). Her marriage to rapper Jay Z and portrayal of Etta James in Cadillac Records (2008) influenced her third album, I Am... Sasha Fierce (2008), which saw the birth of her alter-ego Sasha Fierce and earned a record-setting six Grammy Awards in 2010, including Song of the Year for \"Single Ladies (Put a Ring on It)\". Beyoncé took a hiatus from music in 2010 and took over management of her career; her fourth album 4 (2011) was subsequently mellower in tone, exploring 1970s funk, 1980s pop, and 1990s soul. Her critically acclaimed fifth studio album, Beyoncé (2013), was distinguished from previous releases by its experimental production and exploration of darker themes.',\n",
              "       'A self-described \"modern-day feminist\", Beyoncé creates songs that are often characterized by themes of love, relationships, and monogamy, as well as female sexuality and empowerment. On stage, her dynamic, highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music. Throughout a career spanning 19 years, she has sold over 118 million records as a solo artist, and a further 60 million with Destiny\\'s Child, making her one of the best-selling music artists of all time. She has won 20 Grammy Awards and is the most nominated woman in the award\\'s history. The Recording Industry Association of America recognized her as the Top Certified Artist in America during the 2000s decade. In 2009, Billboard named her the Top Radio Songs Artist of the Decade, the Top Female Artist of the 2000s and their Artist of the Millennium in 2011. Time listed her among the 100 most influential people in the world in 2013 and 2014. Forbes magazine also listed her as the most powerful female musician of 2015.',\n",
              "       ...,\n",
              "       'In the late 19th century with the discovery of the electron, and in the early 20th century, with the discovery of the atomic nucleus, and the birth of particle physics, matter was seen as made up of electrons, protons and neutrons interacting to form atoms. Today, we know that even protons and neutrons are not indivisible, they can be divided into quarks, while electrons are part of a particle family called leptons. Both quarks and leptons are elementary particles, and are currently seen as being the fundamental constituents of matter.',\n",
              "       'These quarks and leptons interact through four fundamental forces: gravity, electromagnetism, weak interactions, and strong interactions. The Standard Model of particle physics is currently the best explanation for all of physics, but despite decades of efforts, gravity cannot yet be accounted for at the quantum level; it is only described by classical physics (see quantum gravity and graviton). Interactions between quarks and leptons are the result of an exchange of force-carrying particles (such as photons) between quarks and leptons. The force-carrying particles are not themselves building blocks. As one consequence, mass and energy (which cannot be created or destroyed) cannot always be related to matter (which can be created out of non-matter particles such as photons, or even out of pure energy, such as kinetic energy). Force carriers are usually not considered matter: the carriers of the electric force (photons) possess energy (see Planck relation) and the carriers of the weak force (W and Z bosons) are massive, but neither are considered matter either. However, while these particles are not considered matter, they do contribute to the total mass of atoms, subatomic particles, and all systems that contain them.',\n",
              "       'The term \"matter\" is used throughout physics in a bewildering variety of contexts: for example, one refers to \"condensed matter physics\", \"elementary matter\", \"partonic\" matter, \"dark\" matter, \"anti\"-matter, \"strange\" matter, and \"nuclear\" matter. In discussions of matter and antimatter, normal matter has been referred to by Alfvén as koinomatter (Gk. common matter). It is fair to say that in physics, there is no broad consensus as to a general definition of matter, and the term \"matter\" usually is used in conjunction with a specifying modifier.'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "train1['context'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Va3Otgakv0on",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52cf0f7c-ee25-4096-883e-01fbda6c44f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['in the late 1990s', 'singing and dancing', '2003', ..., 'Oregon',\n",
              "       'Minsk', 'Kathmandu Metropolitan City'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "train1['text'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8lEzBYq0EmK"
      },
      "source": [
        "**Cleaning Text Data for word embeddings**\n",
        "\n",
        "When cleaning the text, we will perform the following task\n",
        "\n",
        "1.   Convert text to lowercase\n",
        "2.   Remove punctuations\n",
        "3.   Remove extra space\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHf4kNYrvZqa"
      },
      "source": [
        "1. Convert text to lowercase (because in NLP casing matters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7aQmu9anbpAF"
      },
      "outputs": [],
      "source": [
        "train1['clean_question']=train1['question'].apply(lambda x: x.lower() if isinstance(x, str) else str(x))\n",
        "train1['clean_context']=train1['context'].apply(lambda x: x.lower() if isinstance(x, str) else str(x))\n",
        "train1['clean_text']=train1['text'].apply(lambda x: x.lower() if isinstance(x, str) else str(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVwOl5KzvOCv"
      },
      "source": [
        "2. Removing punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "CJ5K3RpcbpmX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "276f43ad-3eb1-42e4-de7f-d5c4a36b44ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-3e5e0fdaaa50>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train1['clean_question']=train1['clean_question'].str.replace('[^\\w\\s]','')\n",
            "<ipython-input-19-3e5e0fdaaa50>:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train1['clean_context']=train1['clean_context'].str.replace('[^\\w\\s]','')\n",
            "<ipython-input-19-3e5e0fdaaa50>:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train1['clean_text']=train1['clean_text'].str.replace('[^\\w\\s]','')\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "train1['clean_question']=train1['clean_question'].str.replace('[^\\w\\s]','')\n",
        "train1['clean_context']=train1['clean_context'].str.replace('[^\\w\\s]','')\n",
        "train1['clean_text']=train1['clean_text'].str.replace('[^\\w\\s]','')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtlKNpYWvrWs"
      },
      "source": [
        "3. Removing extra space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "F4xJgv3BexEu"
      },
      "outputs": [],
      "source": [
        "train1['clean_question']=train1['clean_question'].apply(lambda x: re.sub(' +',' ',x) )\n",
        "train1['clean_context']=train1['clean_context'].apply(lambda x: re.sub(' +',' ',x))\n",
        "train1['clean_text']=train1['clean_text'].apply(lambda x: re.sub(' +',' ',x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3cHv1l_gA9k"
      },
      "source": [
        "Checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oCFHZtabfxbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ab3b8d3-d016-47bb-eace-84b0b7f1a1a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['when did beyonce start becoming popular',\n",
              "       'what areas did beyonce compete in when she was growing up',\n",
              "       'when did beyonce leave destinys child and become a solo singer',\n",
              "       ..., 'what is another name for antimatter',\n",
              "       'matter usually does not need to be used in conjunction with what',\n",
              "       'what field of study has a variety of unusual contexts'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "train1['clean_question'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "BxwakbwBf5tN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a24369a3-9d15-453e-d7e7-5558211c04d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['beyoncé giselle knowlescarter biːˈjɒnseɪ beeyonsay born september 4 1981 is an american singer songwriter record producer and actress born and raised in houston texas she performed in various singing and dancing competitions as a child and rose to fame in the late 1990s as lead singer of rb girlgroup destinys child managed by her father mathew knowles the group became one of the worlds bestselling girl groups of all time their hiatus saw the release of beyoncés debut album dangerously in love 2003 which established her as a solo artist worldwide earned five grammy awards and featured the billboard hot 100 numberone singles crazy in love and baby boy',\n",
              "       'following the disbandment of destinys child in june 2005 she released her second solo album bday 2006 which contained hits déjà vu irreplaceable and beautiful liar beyoncé also ventured into acting with a golden globenominated performance in dreamgirls 2006 and starring roles in the pink panther 2006 and obsessed 2009 her marriage to rapper jay z and portrayal of etta james in cadillac records 2008 influenced her third album i am sasha fierce 2008 which saw the birth of her alterego sasha fierce and earned a recordsetting six grammy awards in 2010 including song of the year for single ladies put a ring on it beyoncé took a hiatus from music in 2010 and took over management of her career her fourth album 4 2011 was subsequently mellower in tone exploring 1970s funk 1980s pop and 1990s soul her critically acclaimed fifth studio album beyoncé 2013 was distinguished from previous releases by its experimental production and exploration of darker themes',\n",
              "       'a selfdescribed modernday feminist beyoncé creates songs that are often characterized by themes of love relationships and monogamy as well as female sexuality and empowerment on stage her dynamic highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music throughout a career spanning 19 years she has sold over 118 million records as a solo artist and a further 60 million with destinys child making her one of the bestselling music artists of all time she has won 20 grammy awards and is the most nominated woman in the awards history the recording industry association of america recognized her as the top certified artist in america during the 2000s decade in 2009 billboard named her the top radio songs artist of the decade the top female artist of the 2000s and their artist of the millennium in 2011 time listed her among the 100 most influential people in the world in 2013 and 2014 forbes magazine also listed her as the most powerful female musician of 2015',\n",
              "       ...,\n",
              "       'in the late 19th century with the discovery of the electron and in the early 20th century with the discovery of the atomic nucleus and the birth of particle physics matter was seen as made up of electrons protons and neutrons interacting to form atoms today we know that even protons and neutrons are not indivisible they can be divided into quarks while electrons are part of a particle family called leptons both quarks and leptons are elementary particles and are currently seen as being the fundamental constituents of matter',\n",
              "       'these quarks and leptons interact through four fundamental forces gravity electromagnetism weak interactions and strong interactions the standard model of particle physics is currently the best explanation for all of physics but despite decades of efforts gravity cannot yet be accounted for at the quantum level it is only described by classical physics see quantum gravity and graviton interactions between quarks and leptons are the result of an exchange of forcecarrying particles such as photons between quarks and leptons the forcecarrying particles are not themselves building blocks as one consequence mass and energy which cannot be created or destroyed cannot always be related to matter which can be created out of nonmatter particles such as photons or even out of pure energy such as kinetic energy force carriers are usually not considered matter the carriers of the electric force photons possess energy see planck relation and the carriers of the weak force w and z bosons are massive but neither are considered matter either however while these particles are not considered matter they do contribute to the total mass of atoms subatomic particles and all systems that contain them',\n",
              "       'the term matter is used throughout physics in a bewildering variety of contexts for example one refers to condensed matter physics elementary matter partonic matter dark matter antimatter strange matter and nuclear matter in discussions of matter and antimatter normal matter has been referred to by alfvén as koinomatter gk common matter it is fair to say that in physics there is no broad consensus as to a general definition of matter and the term matter usually is used in conjunction with a specifying modifier'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "train1['clean_context'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Ud2hFLp0f-FD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53de9c03-7507-4284-973a-bb770072cc72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['in the late 1990s', 'singing and dancing', '2003', ..., 'oregon',\n",
              "       'minsk', 'kathmandu metropolitan city'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "train1['clean_text'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D6us36025Mc"
      },
      "source": [
        "Statistically checking the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "N9YM53owLTg9"
      },
      "outputs": [],
      "source": [
        "#dropping columns\n",
        "train1 = train1.drop(train1.columns[[0, 1, 2, 4, 5]], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "EyApZxkHOkr1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "feeb603f-57a3-4ee8-ac3f-d524ad918e2b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  text                                     clean_question  \\\n",
              "0    in the late 1990s            when did beyonce start becoming popular   \n",
              "1  singing and dancing  what areas did beyonce compete in when she was...   \n",
              "2                 2003  when did beyonce leave destinys child and beco...   \n",
              "3       Houston, Texas        in what city and state did beyonce grow up    \n",
              "4           late 1990s          in which decade did beyonce become famous   \n",
              "\n",
              "                                       clean_context           clean_text  \n",
              "0  beyoncé giselle knowlescarter biːˈjɒnseɪ beeyo...    in the late 1990s  \n",
              "1  beyoncé giselle knowlescarter biːˈjɒnseɪ beeyo...  singing and dancing  \n",
              "2  beyoncé giselle knowlescarter biːˈjɒnseɪ beeyo...                 2003  \n",
              "3  beyoncé giselle knowlescarter biːˈjɒnseɪ beeyo...        houston texas  \n",
              "4  beyoncé giselle knowlescarter biːˈjɒnseɪ beeyo...           late 1990s  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cbee0520-8c82-4ce1-930d-d2b52ee4084d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>clean_question</th>\n",
              "      <th>clean_context</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>when did beyonce start becoming popular</td>\n",
              "      <td>beyoncé giselle knowlescarter biːˈjɒnseɪ beeyo...</td>\n",
              "      <td>in the late 1990s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>what areas did beyonce compete in when she was...</td>\n",
              "      <td>beyoncé giselle knowlescarter biːˈjɒnseɪ beeyo...</td>\n",
              "      <td>singing and dancing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2003</td>\n",
              "      <td>when did beyonce leave destinys child and beco...</td>\n",
              "      <td>beyoncé giselle knowlescarter biːˈjɒnseɪ beeyo...</td>\n",
              "      <td>2003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Houston, Texas</td>\n",
              "      <td>in what city and state did beyonce grow up</td>\n",
              "      <td>beyoncé giselle knowlescarter biːˈjɒnseɪ beeyo...</td>\n",
              "      <td>houston texas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>late 1990s</td>\n",
              "      <td>in which decade did beyonce become famous</td>\n",
              "      <td>beyoncé giselle knowlescarter biːˈjɒnseɪ beeyo...</td>\n",
              "      <td>late 1990s</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cbee0520-8c82-4ce1-930d-d2b52ee4084d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cbee0520-8c82-4ce1-930d-d2b52ee4084d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cbee0520-8c82-4ce1-930d-d2b52ee4084d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c6a5f95e-28d8-429e-90df-8e121cfb58bd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c6a5f95e-28d8-429e-90df-8e121cfb58bd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c6a5f95e-28d8-429e-90df-8e121cfb58bd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "train1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53OYYlIWsFJN"
      },
      "source": [
        "Transfer Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wtHiCTZSEUwP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "668037cc109149c2aa4abe052f9d4294",
            "483f14d05409436e82a42cb8de3d3170",
            "d06a6fd3d18b452e9697458889090cd5",
            "c2a6234013fd4d72b03b27d848be3899",
            "8974f24207ec493684c1d72a1462d968",
            "9212d4d5ed9840988f71290a32d77944",
            "a3f0bb117c5c48c7ae7ccf2bd016d925",
            "d73b080d1aee47afa75e0eb414c6f7c7",
            "e3bced5575fb40d1a26bd2f11120d389",
            "b67aabd0ac324737bf1dba4ddd5652e0",
            "add678f153a14148935615c2d39591ce",
            "c3760f99ad8c4e10bff230a64a472634",
            "c5ab5cb5f4d4477cb580b91b2188fd6e",
            "191641813c504439b9cc610f7bd9fcf7",
            "ec2c8848d8814c989e4714a031755bd9",
            "c4cd9af8f68b49a385ccc9b0db8a4329",
            "c825dee2f13a435aad2573a6476684f2",
            "fbde67809a2542febf325435feae88f2",
            "6fd7df06314449f995dd4217849a38a7",
            "f7ed31eaaa1d466297e0b8f397a12594",
            "23547b7f5e1243db88f809ec71d3a97c",
            "74e9308bc8d54019be58e4949cf6269f",
            "760e0af64a8b4b26acb429f9a1858e7a",
            "b4694afb2f6648e6a15cb25a3deabdfd",
            "b42a8fc4ec3a44a7bd3285962118fc08",
            "e93538224f124fef98c04d1393ebd8e3",
            "c388568e8ace4b058b0aecae41347ea9",
            "7258d917853945248afed358fc1052fe",
            "95e86e23e01246dbbf8a807c789123e4",
            "a048f1f216774d489df08d3eed6cf0ac",
            "53a5a395f9c94150b860ffb97d6c151c",
            "e1153ed326a3452882f83a9156525b56",
            "2ec4af78b15940c7acdfcbe3238a1b7f",
            "4fc576a5911e4b3eabcb427a20fddcca",
            "b6dc80151e474432a4fe9319bd6225e5",
            "ab13a15b1c08443d8151361d79cce0f1",
            "ad1d398e14e74d4d9b9705e1eaeca4c0",
            "1fea7acc27374523a9e46e2ef3718cc7",
            "33ae6fc55bfd405aade725dda00ae365",
            "06d6d8769e1f4e56bd329935f92431b3",
            "563cc5116f624a4bacd450331b01735f",
            "416e7eeef4e64452959c95810a0ffbd5",
            "7f91ba27385144a692b9af1ade12ad0e",
            "1eddd377325143918b9ecf58d2b11f09",
            "1092d94ab1da47129972661bca79f678",
            "73f997a414094693bf653d4d07342235",
            "c1d2848a539147658eac99a6ffb93776",
            "5c203093397747b9be13fcca0bdb056e",
            "85a3d034a610470ba1563650ccc5b95e",
            "5c97a79cfdd6487c89bda1270f530f1b",
            "3683fd8c3e844b92b3de4cf6496a0b3b",
            "166da64887b747518e8945a89e76cac9",
            "9c2c2f46ff404083a9b58aa9fc85718d",
            "9af0d0f971b34749972d9b8f021d11c8",
            "be9db368c28d4b11be78a309177074b5"
          ]
        },
        "outputId": "c3db414b-cb6f-400e-b9a7-9e8ae09b48ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.37.1-py2.py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.7/251.7 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.37.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=11170591dfce32f4aa34187d0b1813770168a4646b51fbd15d42c4b64705f789\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Collecting tensorboardx\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardx) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardx) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardx) (3.20.3)\n",
            "Installing collected packages: tensorboardx\n",
            "Successfully installed tensorboardx-2.6.2.2\n",
            "Collecting simpletransformers\n",
            "  Downloading simpletransformers-0.64.3-py3-none-any.whl (250 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.8/250.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.66.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2023.6.3)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.35.2)\n",
            "Collecting datasets (from simpletransformers)\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.2.2)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.2.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.14.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.5.3)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.15.0)\n",
            "Requirement already satisfied: wandb>=0.10.32 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.16.0)\n",
            "Collecting streamlit (from simpletransformers)\n",
            "  Downloading streamlit-1.28.2-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from simpletransformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (0.19.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (6.0.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (0.4.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (3.1.40)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (1.37.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (3.20.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2023.7.22)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets->simpletransformers)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets->simpletransformers)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (3.4.1)\n",
            "Collecting multiprocess (from datasets->simpletransformers)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (3.8.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (3.2.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit->simpletransformers) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (5.3.2)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (6.8.0)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (9.4.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (13.7.0)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.5.0)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (5.2)\n",
            "Collecting validators<1,>=0.2 (from streamlit->simpletransformers)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit->simpletransformers)\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (6.3.2)\n",
            "Collecting watchdog>=2.1.5 (from streamlit->simpletransformers)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.59.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (3.5.1)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (3.0.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.12.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.11)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit->simpletransformers) (3.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.31.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.13.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->simpletransformers) (3.2.2)\n",
            "Installing collected packages: sentencepiece, watchdog, validators, pyarrow-hotfix, dill, pydeck, multiprocess, datasets, streamlit, simpletransformers\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6 pydeck-0.8.1b0 sentencepiece-0.1.99 simpletransformers-0.64.3 streamlit-1.28.2 validators-0.22.0 watchdog-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "668037cc109149c2aa4abe052f9d4294"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3760f99ad8c4e10bff230a64a472634"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "760e0af64a8b4b26acb429f9a1858e7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4fc576a5911e4b3eabcb427a20fddcca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1092d94ab1da47129972661bca79f678"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install wandb  #monitor training in realtime\n",
        "!pip install transformers  #make computer understand how humans understand language\n",
        "!pip install seqeval  #used for sequence labelling evaluations,, evaluates NLP\n",
        "!pip install tensorboardx #used for visualizing matrix through histograms or plots\n",
        "!pip install simpletransformers #used to quickly evaluate transformer model, uses SIMPLE one line code to train model\n",
        "\n",
        "from simpletransformers.question_answering import QuestionAnsweringModel\n",
        "\n",
        "train_args = {\n",
        "    'learning_rate': 3e-5,\n",
        "    'num_train_epochs': 2,  #no of training iterations\n",
        "    'max_seq_length': 384, #max length of answer\n",
        "    'doc_stride': 128,  #used to split input text into smaller seq, length of each seq=128\n",
        "    'overwrite_output_dir': True,\n",
        "    'reprocess_input_data': False,\n",
        "    'train_batch_size': 2,\n",
        "    'gradient_accumulation_steps': 8, #before model is implemented\n",
        "}\n",
        "\n",
        "model = QuestionAnsweringModel('bert', 'bert-base-cased', args=train_args)\n",
        "#bert is model type and bert-base-cased is the model name\n",
        "#specifiying pretrained model preserves casing of input text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "OnxjCMUtvEj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42b1723e-e7b8-4026-f938-5662721992be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "#converting the json files to list of dictionary which is the suitable format to perform QA in simple transformers\n",
        "train = [item for topic in train['data'] for item in topic['paragraphs'] ]\n",
        "\n",
        "type(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8dQhRDFiA7qn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eacb9d96-978b-49f4-90aa-07f4114c9d7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19035"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "len(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Gl59LIYfBxwA"
      },
      "outputs": [],
      "source": [
        "#getting a random sample for training\n",
        "import random\n",
        "random.seed(3)\n",
        "randomtrain=random.sample(train, 5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Pyz4sncbEHtM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7987ea1f-aff3-4571-8994-3684d407ff3d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "len(randomtrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "hzhbZx3XIQKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9a847e1-89e9-479e-e366-a4170650959b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14035"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "remain = [ele for ele in train if ele not in randomtrain]\n",
        "len(remain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "EufebeKp68H3"
      },
      "outputs": [],
      "source": [
        "#getting a random sample for testing\n",
        "random.seed(6)\n",
        "randomtest=random.sample(remain, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "b-nGiVE0Eb4o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165,
          "referenced_widgets": [
            "71240df29c124fac8439e45da3cec1cb",
            "f8aa8f03329c4550be002e265902fbb6",
            "0cb021f9fb9d42eeb7a9865aa5669592",
            "b559c02168f049c097d31b21c6c17f9b",
            "1ace84ff3dcb4ba9b30718646254f05d",
            "bf690c41b74542aca2abfe43bc502a1a",
            "483843f70d83498ebb394212951173ba",
            "cbd08f353f524ebaa3b3db54c1676575",
            "ceea9e55b81c437dba528722f36121c1",
            "604e38262fd14fa4aae7b68dc0ea3a13",
            "ae3951fd83c94f4ab70908a4a08ea86b",
            "e00ddc2dcd5e425e9a608157da4bcfc0",
            "747ac935bc7d43ac915672f825acb1aa",
            "b6c90e447d5e49f69446227751833901",
            "e0360b3768944a909baaad7e7971a25a",
            "d8394eb1ee7d4f4f974ad0209b4519a7",
            "f9c2870d4c2746199a28a642fd50a967",
            "0758692742c8478ab83528972a33a543",
            "c6eb9446075742cc932234abbde338be",
            "bfca722076ba40449aeedfee080aaab5",
            "375bd00f440847278f0758e8fe2cd8ab",
            "0f0579e9abfc487895755650020e55d7",
            "26a3fe5cec4b43b5abec4630f0072bfa",
            "e44185a78766441ca633cfdff6db8d9b",
            "fc80aa2859a24d9ba7a90978c7c700ad",
            "4ea2d94d9edd4b61bca0cf497e42a758",
            "1021f0c5a24641cc93c57bfbbc2b7694",
            "7f275f411d0946ab80d81c18b7c1c6df",
            "a9d8bac6278e4f749955488164851fc3",
            "33c391544b3242078847cd646250b1ca",
            "17e4a7a5b61b43f1bac36ea7d1ca9ec3",
            "b7b78e995a4740089913a9f29ca0c619",
            "6cb62d37b0284413baedbf0378e600eb"
          ]
        },
        "outputId": "6422a52d-f0c3-45c7-9acb-de4cbb7727b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "convert squad examples to features: 100%|██████████| 34389/34389 [03:44<00:00, 153.32it/s]\n",
            "add example index and unique id: 100%|██████████| 34389/34389 [00:00<00:00, 858605.05it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71240df29c124fac8439e45da3cec1cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running Epoch 0 of 2:   0%|          | 0/17470 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e00ddc2dcd5e425e9a608157da4bcfc0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running Epoch 1 of 2:   0%|          | 0/17470 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26a3fe5cec4b43b5abec4630f0072bfa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4366, 1.3764966590363281)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "import sklearn\n",
        "model.train_model(randomtrain, acc=sklearn.metrics.accuracy_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "gszl62A4f2eR"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "pickle.dump(model, open('model.pkl', 'wb'))\n",
        "#into binary format (serializing data)\n",
        "\n",
        "! cp model.pkl \"/content/drive/MyDrive/NLP\"\n",
        "model = pickle.load(open('/content/drive/MyDrive/NLP/model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErmI6eue-16c"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Cirsrc0m-0LX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "73d38ffbbd7948349e3bea09bfca5577",
            "c3a57b9b1d6a409aae22d6dbde3212fc",
            "077132bb0066475c855cedb3471b64a5",
            "7d221bbfc8c5489aa53d5e929c8208aa",
            "e326c8cb76f04d3ca798987812b33655",
            "76746b6894b14f4999b3fe10f5141c3a",
            "26c3f922361c462b8ff3a1a661b71724",
            "94aae4ebc8ba4f92be77bb79b9cf2f2e",
            "7105c68d674847c2b9a9a51702602014",
            "5733371449f044a7a28b225c21e20ecf",
            "233126b957cf42cebadcb1fd7705fddc"
          ]
        },
        "outputId": "0dbc39a5-8777-4b80-8771-2507667ece7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "convert squad examples to features: 100%|██████████| 6857/6857 [00:43<00:00, 156.20it/s]\n",
            "add example index and unique id: 100%|██████████| 6857/6857 [00:00<00:00, 671845.04it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running Evaluation:   0%|          | 0/864 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73d38ffbbd7948349e3bea09bfca5577"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'correct': 4103, 'similar': 2334, 'incorrect': 420, 'acc': 0.5983666326381799, 'eval_loss': -7.011013454861111}\n",
            "{'correct_text': {'5731e624e17f3d1400422519': '1936', '5731e624e17f3d140042251c': 'three quarters', '572805f84b864d190016425f': 'The Computer and the Brain', '57326c5ae99e3014001e6797': 'New York Times', '57326c5ae99e3014001e6798': 'Edwin Pagan', '56de6a2e4396321400ee28ae': '1938', '56de6a2e4396321400ee28af': 'RCA', '56de6a2e4396321400ee28b0': 'a British television set', '5a83215fe60761001a2eb425': '', '5a83215fe60761001a2eb426': '', '5726dad6708984140094d3ac': 'two and a half months', '5726dad6708984140094d3ad': 'two and a half months', '5ad28656d7d075001a4298dc': '', '5ad28656d7d075001a4298dd': '', '5ad28656d7d075001a4298df': '', '5ad28656d7d075001a4298e0': '', '57309103069b531400832191': 'air traffic control', '5a4e914a755ab9001a10f4fb': '', '5a4e914a755ab9001a10f4fc': '', '5a4e914a755ab9001a10f4fd': '', '5a4e914a755ab9001a10f4fe': '', '5a4e914a755ab9001a10f4ff': '', '5709b308ed30961900e84431': 'Civil War', '5709b308ed30961900e84432': 'War of 1812', '5a8cdd89fd22b3001a8d8f6b': '', '5a8cdd89fd22b3001a8d8f6d': '', '56d1ef6ae7d4791d0090259a': 'Samsara', '56d1ef6ae7d4791d0090259b': 'suffering', '56be932e3aeaaa14008c90f9': '541,000', '56be932e3aeaaa14008c90fa': 'Déjà Vu', '56bf940da10cfb140055118a': 'Jay Z', '56bf940da10cfb140055118b': 'top five', '56d4bc642ccc5a1400d83190': \"B'Day\", '56d4bc642ccc5a1400d83191': '541,000', '56d4bc642ccc5a1400d83192': 'Jay Z', '56d4bc642ccc5a1400d83193': 'Green Light', '572b8eb4111d821400f38f06': 'linguist and historian', '572b8eb4111d821400f38f07': '1809', '572b8eb4111d821400f38f08': 'a major language', '572b8eb4111d821400f38f0a': 'Modern scholars', '5a7a175117ab25001a8a0316': '', '5a7a175117ab25001a8a0317': '', '5a7a175117ab25001a8a0318': '', '5a7a175117ab25001a8a0319': '', '5a7a175117ab25001a8a031a': '', '57281dc22ca10214002d9e28': 'Nazi Germany', '57281dc22ca10214002d9e29': 'London', '57281dc22ca10214002d9e2b': '1942', '5a36b2f895360f001af1b301': '', '5a36b2f895360f001af1b302': '', '5a36b2f895360f001af1b303': '', '5730487aa23a5019007fd071': '38.8%', '5730487aa23a5019007fd072': 'Themba Dlamini', '5730487aa23a5019007fd073': '80% coverage or greater', '5730487aa23a5019007fd075': '18%', '5a56991a6349e2001acdce72': '', '5a56991a6349e2001acdce75': '', '5733e6a54776f41900661472': '1992–93', '5733e6a54776f41900661473': '22', '5733e6a54776f41900661474': 'Brian Deane', '5733e6a54776f41900661476': 'Manchester United', '5ad0c457645df0001a2d026e': '', '5ad0c457645df0001a2d026f': '', '5ad0c457645df0001a2d0270': '', '5725f36689a1e219009ac0f0': 'Herbert Chapman', '5725f36689a1e219009ac0f1': 'pillar box red', '5acd0cc307355d001abf3256': '', '5acd0cc307355d001abf3259': '', '5acd0cc307355d001abf325a': '', '570e6d560dc6ce190020504f': '1940', '570e6d560dc6ce1900205052': '14 January 1958', '5a271a77c93d92001a4003eb': '', '5a271a77c93d92001a4003ec': '', '5a271a77c93d92001a4003ed': '', '5a271a77c93d92001a4003ee': '', '56cf306baab44d1400b88de7': 'National Park Service', '56cf306baab44d1400b88de8': \"Grant's Tomb\", '56cf306baab44d1400b88de9': 'Greenwich Village', '56cfe6d2234ae51400d9c045': 'National Park Service', '56cfe6d2234ae51400d9c047': 'New Jersey', '56cfe6d2234ae51400d9c048': 'Stonewall Inn', '56cfe6d2234ae51400d9c049': \"Grant's Tomb\", '570af6876b8089140040f647': 'mobile', '5a1f38073de3f40018b26540': '', '5a1f38073de3f40018b26541': '', '5a1f38073de3f40018b26543': '', '5a1f38073de3f40018b26544': '', '5727d383ff5b5019007d962c': 'sunspots', '5727d383ff5b5019007d962d': 'eclipses', '5727d383ff5b5019007d962f': '635', '5727d383ff5b5019007d9630': '112', '573198280fdd8d15006c63c9': 'May 2005', '573198280fdd8d15006c63cb': '$3.5 million', '573198280fdd8d15006c63cc': 'December 15, 2005', '573198280fdd8d15006c63cd': '14 billion', '5acd677a07355d001abf40d6': '', '5acd677a07355d001abf40d7': '', '5acd677a07355d001abf40d9': '', '5acd677a07355d001abf40da': '', '5ace125632bba1001ae49a39': '', '5ace125632bba1001ae49a3a': '', '5ace125632bba1001ae49a3b': '', '5ace125632bba1001ae49a3c': '', '5ace125632bba1001ae49a3d': '', '56f9313f9b226e1400dd1284': 'three', '56f9313f9b226e1400dd1286': 'Greenwich Avenue', '572fcc11947a6a140053ccd4': 'Borrelia burgdorferi', '572fcc11947a6a140053ccd5': 'single linear chromosome', '5726c029f1498d1400e8ea32': 'Brian May and Roger Taylor', '5726c029f1498d1400e8ea34': 'Modena, Italy', '5726c029f1498d1400e8ea35': 'Robbie Williams', '5727dad64b864d1900163e9d': 'to minimize physical incompatibilities in connectors from different vendors', '572800942ca10214002d9b15': 'when the device is first connected', '572e82aacb0c0d14000f120d': 'steam power', '572e82aacb0c0d14000f120e': 'moving goods in bulk in mines and factories', '572e82aacb0c0d14000f120f': 'Burton and Hormer', '572e82aacb0c0d14000f1210': 'London', '56df20e5c65bf219000b3f79': 'Messiah', '56df20e5c65bf219000b3f7b': 'Nazareth', '56df20e5c65bf219000b3f7c': 'northern Israel', '5ad2d8e3d7d075001a42a456': '', '5ad2d8e3d7d075001a42a457': '', '56f89cb39e9bad19000a01c9': 'George C. Williams', '56f89cb39e9bad19000a01cb': 'Richard Dawkins', '5728b45f3acd2414000dfd15': 'Marathas', '5727e6f8ff5b5019007d97f9': 'ethnic Tibetans', '5727e6f8ff5b5019007d97fa': 'India', '572f9a2ba23a5019007fc7cc': 'fining', '572f9a2ba23a5019007fc7cd': 'pig iron', '5731c7ade17f3d14004223d7': '31 October 1517', '5731c7ade17f3d14004223d9': 'Virgin Mary', '5731c7ade17f3d14004223db': 'the Church and the papacy', '572f9e5504bcaa1900d76aeb': 'morphologies', '572f9e5504bcaa1900d76aec': 'one-tenth the size', '572f9e5504bcaa1900d76aed': '0.7 mm', '572f9e5504bcaa1900d76aef': 'not well-studied', '570a661f6d058f1900182e0e': 'Dutch', '5ad27982d7d075001a4295b0': '', '5ad27982d7d075001a4295b2': '', '5ad27982d7d075001a4295b3': '', '572b8f5d111d821400f38f10': 'Czech', '572b8f5d111d821400f38f11': 'since 2004', '572b8f5d111d821400f38f13': 'Jonathan van Parys', '5a7a185f17ab25001a8a0321': '', '5a7a185f17ab25001a8a0322': '', '5a7a185f17ab25001a8a0323': '', '5731b71e0fdd8d15006c6483': 'Baptist theologian', '5731b71e0fdd8d15006c6485': 'Rhode Island', '5731b71e0fdd8d15006c6486': '1644', '5731b71e0fdd8d15006c6487': 'Thomas Jefferson', '5ad13aec645df0001a2d12ee': '', '5ad13aec645df0001a2d12ef': '', '5ad13aec645df0001a2d12f0': '', '5ad13aec645df0001a2d12f1': '', '5728c1482ca10214002da71a': '1848', '5728c1482ca10214002da71b': '1860', '5728c1482ca10214002da71c': 'Georges-Eugène Haussmann', '572eb077c246551400ce4518': 'Voyager 2', '572eb077c246551400ce4519': '25 August 1989', '5acee61232bba1001ae4b8f5': '', '5acee61232bba1001ae4b8f6': '', '5acee61232bba1001ae4b8f7': '', '5acee61232bba1001ae4b8f8': '', '5acee61232bba1001ae4b8f9': '', '56df6c5a56340a1900b29af6': '26.2%', '56df6c5a56340a1900b29af7': '78.3 years', '56df6c5a56340a1900b29af8': '82.1', '5733f55e4776f419006615ab': 'five waters', '5733f55e4776f419006615ae': '91.379.615', '5733f55e4776f419006615af': 'Lahore', '5a68b42b8476ee001a58a76a': '', '5a68b42b8476ee001a58a76b': '', '5a68b42b8476ee001a58a76d': '', '5a68b42b8476ee001a58a76e': '', '57274eb3f1498d1400e8f60b': '2000', '571adfb39499d21900609b71': 'limited to the Septuagint', '571adfb39499d21900609b73': 'Septuagint Greek', '5ace995332bba1001ae4ac34': '', '5ace995332bba1001ae4ac35': '', '5ace995332bba1001ae4ac36': '', '5aceb60032bba1001ae4b11a': '', '5aceb60032bba1001ae4b11c': '', '5acd198007355d001abf34ca': '', '5acd198007355d001abf34cb': '', '5acd198007355d001abf34cc': '', '5acd198007355d001abf34cd': '', '5acd198007355d001abf34ce': '', '5a0cdf07f5590b0018dab5fc': '', '5a0cdf07f5590b0018dab5fd': '', '5a0cdf07f5590b0018dab5fe': '', '5731f389e99e3014001e6414': 'limited', '5731f389e99e3014001e6415': 'animal sacrifice', '5731f389e99e3014001e6416': 'Vestals', '5731f389e99e3014001e6417': 'Bona Dea', '5731f389e99e3014001e6418': 'motherhood', '57264dedf1498d1400e8db8d': '1883', '57264dedf1498d1400e8db8f': '1893', '5a157229a54d420018529435': '', '5a157229a54d420018529438': '', '5a157229a54d420018529439': '', '5731e0ad0fdd8d15006c65ed': 'John F. Kennedy', '5731e0ad0fdd8d15006c65ee': 'God', '5ad14c5d645df0001a2d164c': '', '5ad14c5d645df0001a2d164d': '', '5ad14c5d645df0001a2d164e': '', '5ad14c5d645df0001a2d164f': '', '57094dde9928a81400471514': 'Best Places for Business and Careers', '57094dde9928a81400471515': '92', '57094dde9928a81400471516': 'Forty', '57094dde9928a81400471517': 'Twenty-five', '5709c252200fba14003682b1': '2006', '5709c252200fba14003682b3': '92', '5709c252200fba14003682b4': 'Forty', '5709c252200fba14003682b5': '13', '5ad420c6604f3c001a400715': '', '5ad420c6604f3c001a400716': '', '5ad420c6604f3c001a400718': '', '5ad420c6604f3c001a400719': '', '57273aa2dd62a815002e99c3': 'Charles I of Hungary', '57273aa2dd62a815002e99c4': '1325', '57273aa2dd62a815002e99c6': '1348', '5ad022b677cf76001a686b5e': '', '5ad022b677cf76001a686b60': '', '5ad022b677cf76001a686b61': '', '5ad022b677cf76001a686b62': '', '572f9e8204bcaa1900d76af5': 'Timber', '572f9e8204bcaa1900d76af8': 'ceramic roof tiles', '572f9e8204bcaa1900d76af9': 'Timber', '56def0acc65bf219000b3e3b': 'conflict', '56def0acc65bf219000b3e3c': 'significance', '5ad2ccebd7d075001a42a2ac': '', '5ad2ccebd7d075001a42a2ae': '', '5ad2ccebd7d075001a42a2af': '', '57301bfca23a5019007fcd81': 'increased body mass', '57301bfca23a5019007fcd82': 'subtherapeutic antibiotic treatment', '57328cf2b3a91d1900202e33': 'increased body mass', '57328cf2b3a91d1900202e34': 'Early life', '5a65bfbec2b11c001a425d2f': '', '5a65bfbec2b11c001a425d31': '', '5a65bfbec2b11c001a425d32': '', '5a65bfbec2b11c001a425d33': '', '56f8ee329e9bad19000a0718': '7 to 13', '56f8ee329e9bad19000a071a': '53.5%', '572eaae1dfa6aa1500f8d287': '1370', '572eaae1dfa6aa1500f8d28a': 'dots and chapter separators', '5ad218cfd7d075001a428406': '', '5ad218cfd7d075001a428407': '', '5ad218cfd7d075001a428408': '', '5ad218cfd7d075001a428409': '', '5ace1dc132bba1001ae49b1f': '', '5ace1dc132bba1001ae49b20': '', '5ace1dc132bba1001ae49b21': '', '5ace1dc132bba1001ae49b22': '', '5ace1dc132bba1001ae49b23': '', '57318f33a5e9cc1400cdc07f': 'Calçada Portuguesa', '57318f33a5e9cc1400cdc080': 'two-tone stone mosaic paving', '57318f33a5e9cc1400cdc082': 'Lisbon', '570e63fe0dc6ce1900205003': '1956', '570e63fe0dc6ce1900205004': 'Europe and the United States', '570e63fe0dc6ce1900205005': '2006, 2008 and 2010', '570e63fe0dc6ce1900205007': 'Melbourne', '56df56288bc80c19004e4ac9': 'May 2015', '56d3847159d6e414001465fd': 'Taylor Hicks', '56d3847159d6e414001465fe': 'Alabama', '56db3ebee7c41114004b4f99': 'Alabama', '5acfc10177cf76001a685cd0': '', '5acfc10177cf76001a685cd1': '', '5acfc10177cf76001a685cd2': '', '5acfc10177cf76001a685cd3': '', '5acfc10177cf76001a685cd4': '', '5733f37ed058e614000b664f': 'three', '5733f37ed058e614000b6650': 'Navy, Army and Air Force', '5733f37ed058e614000b6652': '7,500', '5726069c38643c19005acf62': 'Greeks', '5726069c38643c19005acf64': '2.5', '57270870dd62a815002e9823': 'degradation of the dielectric', '5acf5e9177cf76001a684c88': '', '5acf5e9177cf76001a684c89': '', '5acf5e9177cf76001a684c8a': '', '5732a8a6328d981900601fe9': 'ice cap', '5732a8a6328d981900601fea': 'the Alps', '5732a8a6328d981900601feb': 'Oligocene', '5732a8a6328d981900601fec': 'Antarctica', '5a4ebc82af0d07001ae8cc15': '', '5a4ebc82af0d07001ae8cc17': '', '5a4ebc82af0d07001ae8cc18': '', '56f8b9549e9bad19000a03b6': 'a union of genomic sequences encoding a coherent set of potentially overlapping functional products', '56f8b9549e9bad19000a03b8': 'their functional products (proteins or RNA)', '56f8b9549e9bad19000a03b9': 'regulatory elements', '572841842ca10214002da1b0': 'General Advisory Committee', '572a1ef73f37b31900478700': 'social, economic and cultural characteristics', '5a7d103570df9f001a874f4d': '', '5a7d103570df9f001a874f4f': '', '5a7d103570df9f001a874f50': '', '5a7d103570df9f001a874f51': '', '572ecec0cb0c0d14000f15b4': 'Wang Mang', '572ecec0cb0c0d14000f15b5': 'Gradual silt buildup', '572ecec0cb0c0d14000f15b6': 'Han engineers', '572ecec0cb0c0d14000f15b8': 'massive floods', '5731ce3ab9d445190005e577': 'between 25% and 40%', '5731ce3ab9d445190005e579': 'the Bavarian, Thuringian and Swabian principalities', '572774e65951b619008f8a5b': 'Carnivals', '572774e65951b619008f8a5d': 'Blancs-Moussis', '572774e65951b619008f8a5e': 'Laetare Sunday', '572774e65951b619008f8a5f': 'Carnival Parade of Maaseik', '56cf54baaab44d1400b89010': '1963', '56cf54baaab44d1400b89011': '21', '56d39d2359d6e41400146821': '1963', '572a2c791d04691400779812': 'Little Mosque on the Prairie and The Border', '5a54e92d134fea001a0e1773': '', '5a54e92d134fea001a0e1775': '', '57261f2fec44d21400f3d92b': 'Zeuxis', '57261f2fec44d21400f3d92d': 'Seleucus', '57261f2fec44d21400f3d92e': 'basileion', '5706ef6c9e06ca38007e9227': 'uncial', '5706ef6c9e06ca38007e9228': '835', '5728e7254b864d1900165060': 'inductivist', '5728e7254b864d1900165061': 'Imre Lakatos', '5ad27cb0d7d075001a429696': '', '5ad27cb0d7d075001a429697': '', '5ad27cb0d7d075001a429698': '', '5ad27cb0d7d075001a429699': '', '5ad27cb0d7d075001a42969a': '', '5a79d0b117ab25001a8a00c6': '', '5a79d0b117ab25001a8a00c7': '', '5a79d0b117ab25001a8a00c8': '', '5a79d0b117ab25001a8a00c9': '', '5a79d0b117ab25001a8a00ca': '', '570d61a5b3d812140066d7a5': 'Salvados', '570d61a5b3d812140066d7a7': 'book of train breakdowns', '570d61a5b3d812140066d7a8': 'El Mundo', '570d61a5b3d812140066d7a9': 'March 2012', '5725d45b89a1e219009abf62': 'Chess', '5725d45b89a1e219009abf63': 'Beersheba', '5725d45b89a1e219009abf64': 'Boris Gelfand', '57282a944b864d1900164638': 'body cavities', '57282a944b864d1900164639': 'septa', '57282a944b864d190016463a': 'vertical mesenteries', '57282a944b864d190016463b': 'modified epitheliomuscular cells', '5ace853932bba1001ae4a953': '', '5ace853932bba1001ae4a956': '', '56e087957aa994140058e5c4': '25%', '56e087957aa994140058e5c5': '75%', '56df81eb5ca0a614008f9bbd': 'Tavistock', '56df81eb5ca0a614008f9bbe': 'El Draco', '56df81eb5ca0a614008f9bbf': '1596', '56df81eb5ca0a614008f9bc0': 'dysentery', '5727a6233acd2414000de8d6': 'Gateway Community College', '5727a6233acd2414000de8d7': 'Southern Connecticut State University', '5727a6233acd2414000de8d8': 'Yale University', '5727a6233acd2414000de8d9': '2012', '572a37a3af94a219006aa8be': 'Southern Connecticut State University', '572a37a3af94a219006aa8bf': 'Albertus Magnus College', '572a37a3af94a219006aa8c0': 'Long Wharf district', '572a37a3af94a219006aa8c1': 'Fall 2012', '572ea0bedfa6aa1500f8d21d': 'high school grades', '572ea0bedfa6aa1500f8d220': '25%', '56f957d89e9bad19000a0854': 'Morningside Park', '56f957d89e9bad19000a0855': 'six', '56f957d89e9bad19000a0856': 'Mount Morris', '56f957d89e9bad19000a0857': 'Lenox Avenue', '5729234a1d046914007790a6': 'continental', '5729234a1d046914007790a7': 'racial classification', '5729234a1d046914007790a8': 'racial groupings', '572b8b54111d821400f38efc': 'widespread national pride', '572b8b54111d821400f38eff': 'the return of the language to high culture', '572b8b54111d821400f38f00': 'Czech National Revival', '5a7a161417ab25001a8a030d': '', '5a7a161417ab25001a8a030e': '', '5a7a161417ab25001a8a030f': '', '570a85944103511400d59800': 'Ælfric of Eynsham', '570a85944103511400d59801': 'Bishop Æthelwold of Winchester', '570a85944103511400d59803': 'Late West Saxon', '5a678d67f038b7001ab0c2b2': '', '5a678d67f038b7001ab0c2b3': '', '5a678d67f038b7001ab0c2b4': '', '570ddc210dc6ce1900204ccd': 'more likely', '570ddc210dc6ce1900204cce': 'lose weight', '572fde5e04bcaa1900d76dfd': 'eight', '572fde5e04bcaa1900d76dfe': 'full-time or part-time', '572fde5e04bcaa1900d76dff': 'Mahendra R. Gupta', '572fde5e04bcaa1900d76e00': '40–60%', '5ace2b5c32bba1001ae49c7a': '', '56f8d8959e9bad19000a05e0': '2012', '5726c9c25951b619008f7e20': '1950', '5726e581f1498d1400e8ef28': 'Hoog Catharijne', '5a580e33770dc0001aeeffa2': '', '5a8209af31013a001a335127': '', '5a8209af31013a001a335128': '', '5a8209af31013a001a335129': '', '5a8209af31013a001a33512a': '', '5a8209af31013a001a33512b': '', '56e14acbcd28a01900c6774c': 'April 1927', '5ad13864645df0001a2d122c': '', '5ad13864645df0001a2d122e': '', '5728d51a4b864d1900164f06': 'The Estonian Academy of Arts', '5728d51a4b864d1900164f07': 'Viljandi Culture Academy of University of Tartu', '5728d51a4b864d1900164f09': '245', '56fa85dd8f12f31900630170': 'December 2009', '56fa85dd8f12f31900630171': 'DVB-T2', '56fa85dd8f12f31900630172': 'Digital TV Group', '5ad3bee3604f3c001a3fef4d': '', '5732bcead6dcfa19001e8a99': 'batons, tear gas, riot control agents, rubber bullets, riot shields, water cannons and electroshock weapons', '5732bcead6dcfa19001e8a9a': 'The use of firearms or deadly force', '5acece2e32bba1001ae4b4ac': '', '5acece2e32bba1001ae4b4af': '', '572ee3a8c246551400ce477e': 'the eunuchs', '572ee3a8c246551400ce4781': 'the eunuchs', '572ee3a8c246551400ce4782': \"coup d'état\", '5727903ddd62a815002ea084': 'high cost', '5727903ddd62a815002ea086': '1925', '5727903ddd62a815002ea087': '$20', '5727903ddd62a815002ea088': 'cabinetry', '56cd828562d2951400fa6670': '2010', '56cd828562d2951400fa6672': 'Apple prototype', '56d13543e7d4791d00902004': '2010', '56f81f0ea6d7ea1400e173d7': 'Europe', '56f81f0ea6d7ea1400e173d9': 'over tens of millions of years', '56f81f0ea6d7ea1400e173da': 'Mont Blanc', '57313c12a5e9cc1400cdbd6b': 'Alphanumeric', '57313c12a5e9cc1400cdbd6e': '5x7', '5ad194ca645df0001a2d2066': '', '5ad194ca645df0001a2d2067': '', '5ad194ca645df0001a2d2068': '', '5ad194ca645df0001a2d2069': '', '5ad194ca645df0001a2d206a': '', '56f744beaef2371900625a77': 'The common practice period', '56f744beaef2371900625a79': 'Baroque era', '56f744beaef2371900625a7a': 'around 1820', '56ceb9c4aab44d1400b8892b': '2,300', '56ceb9c4aab44d1400b8892d': '9,000', '56ceb9c4aab44d1400b8892e': '3,000 to 5,000', '56ceb9c4aab44d1400b8892f': '10,000', '56d5307f2593cc1400307abb': '2,300', '56d5307f2593cc1400307abd': '3,000 to 5,000', '56d5307f2593cc1400307abe': '10,000', '572a69cefed8de19000d5bfd': 'promoting better eating practices', '5a0e3462d7c850001886454a': '', '5a0e3462d7c850001886454c': '', '5a0e3462d7c850001886454d': '', '57110273b654c5140001fab1': 'alphabetical', '57110273b654c5140001fab3': 'the number of educated consumers who could afford such texts began to multiply', '57110273b654c5140001fab4': '63', '57110273b654c5140001fab5': '148', '572817474b864d1900164458': 'eclecticist', '572817474b864d1900164459': 'World War II', '572817474b864d190016445a': 'Palais du Rhin', '572817474b864d190016445b': 'Höhere Mädchenschule', '5acd6c4a07355d001abf4182': '', '5acd6c4a07355d001abf4183': '', '5acd6c4a07355d001abf4184': '', '5acd6c4a07355d001abf4186': '', '572f820a04bcaa1900d76a38': 'Fort Duquesne and Fort Frontenac', '5728f64eaf94a219006a9e7b': 'samurai', '5728f64eaf94a219006a9e7c': 'monochrome ink', '5728e8b43acd2414000e01ab': 'Malcolm Turnbull', '5728e8b43acd2414000e01ac': 'Julie Bishop', '5728e8b43acd2414000e01ad': 'Tony Abbott', '5728e8b43acd2414000e01ae': 'the Abbott Government', '5a6a14595ce1a5001a9696db': '', '5a6a14595ce1a5001a9696dd': '', '570d2b46b3d812140066d4db': 'the \"National Army\"', '570d2b46b3d812140066d4dc': 'end of World War I', '5acecfac32bba1001ae4b4f9': '', '5acecfac32bba1001ae4b4fc': '', '5acecfac32bba1001ae4b4fd': '', '57261b6dec44d21400f3d8f5': 'undocumented', '5a0cfe3af5590b0018dab6fa': '', '5a0cfe3af5590b0018dab6fb': '', '5a0cfe3af5590b0018dab6fd': '', '5a0cfe3af5590b0018dab6fe': '', '57268816dd62a815002e885e': '1976', '5727ba9f4b864d1900163ba2': 'Nimbarka', '5727ba9f4b864d1900163ba4': 'three', '5727ba9f4b864d1900163ba5': 'Brahman, soul, and matter', '5a5e608c5bc9f4001a75af89': '', '5a5e608c5bc9f4001a75af8a': '', '5a5e608c5bc9f4001a75af8b': '', '572629d9ec44d21400f3db2a': 'Anglo-Dutch Wars of the 17th and 18th centuries', '572629d9ec44d21400f3db2c': '1717', '572629d9ec44d21400f3db2d': 'English traders', '5726572af1498d1400e8dc7c': 'Bengal', '5726572af1498d1400e8dc7d': 'waived customs duties', '5726572af1498d1400e8dc7f': 'The Dutch', '5726572af1498d1400e8dc80': 'Anglo-Dutch Wars', '5a8456db7cf838001a46a75e': '', '5a8456db7cf838001a46a761': '', '570cbdcfb3d812140066d263': 'biological mother', '570cbdcfb3d812140066d264': 'Emanuel Swedenborg', '5ad186c5645df0001a2d1e96': '', '56e0fd33231d4119001ac54e': 'three', '5acd345807355d001abf3946': '', '5acd345807355d001abf3947': '', '5acd345807355d001abf3949': '', '5acd345807355d001abf394a': '', '572ea349cb0c0d14000f13b8': 'conceptual art', '572ea349cb0c0d14000f13ba': '2006', '57271234f1498d1400e8f31e': 'Pakistan', '57271234f1498d1400e8f31f': 'proto-dentistry', '57271234f1498d1400e8f320': 'Ayurveda', '56d97d8ddc89441400fdb4e4': 'spirit of Olympics', '56d97d8ddc89441400fdb4e5': 'sports and politics', '56db76fde7c41114004b515a': 'run for spirit of Olympics', '56db76fde7c41114004b515b': 'sports and politics', '56f7e518aef2371900625c50': 'opole', '57268e54dd62a815002e8966': '1967', '57268e54dd62a815002e8967': '1971', '57268e54dd62a815002e8968': '1976', '57268e54dd62a815002e8969': 'Denis Healey', '57268e54dd62a815002e896a': 'Kuwait', '5ad13aca645df0001a2d12d4': '', '5ad13aca645df0001a2d12d7': '', '5ad13aca645df0001a2d12d8': '', '572ff0a2947a6a140053ce36': 'Tehran', '572ff0a2947a6a140053ce38': 'proven oil reserves', '572ff0a2947a6a140053ce39': '19', '572ff0a2947a6a140053ce3a': 'through its large reserves of fossil fuels', '572690d45951b619008f76d0': '28.8 km (17.9 mi)', '572690d45951b619008f76d1': 'Avenida de los Insurgentes', '56df226bc65bf219000b3f8d': '1986', '56df226bc65bf219000b3f8e': '$125 million', '56df226bc65bf219000b3f90': 'Stephen Swid, Martin Bandier, and Charles Koppelman', '5ace33db32bba1001ae49df7': '', '5ace33db32bba1001ae49df8': '', '5ace33db32bba1001ae49df9': '', '5ace33db32bba1001ae49dfa': '', '5acf76ad77cf76001a684e79': '', '5acf76ad77cf76001a684e7a': '', '5acf76ad77cf76001a684e7b': '', '5acf76ad77cf76001a684e7c': '', '5707070890286e26004fc805': 'Jornado', '5707070890286e26004fc806': 'Rio Grande', '5707070890286e26004fc807': 'hunting, gathering, and farming', '5707070890286e26004fc808': 'Tepehuan', '5731d56fe17f3d1400422475': 'urban', '56df785856340a1900b29bee': '1823', '56df785856340a1900b29bef': 'September 2011', '56df785856340a1900b29bf0': 'nine', '5728b2813acd2414000dfcf8': '19th century', '5728b2813acd2414000dfcfa': 'Madeira Island', '5ad22fced7d075001a42869b': '', '5ad22fced7d075001a42869c': '', '5ad22fced7d075001a42869d': '', '5ad22fced7d075001a42869e': '', '572ec004c246551400ce45fa': 'Sweden', '5727721add62a815002e9d05': '1444', '5727721add62a815002e9d08': 'Pope Pius II', '5ad02eec77cf76001a686d5f': '', '5ad02eec77cf76001a686d61': '', '5ad02eec77cf76001a686d62': '', '572782b6dd62a815002e9f32': 'instant-runoff', '5ace451232bba1001ae4a143': '', '5ace451232bba1001ae4a144': '', '5ace451232bba1001ae4a145': '', '5ace451232bba1001ae4a146': '', '57313b16e6313a140071cd51': 'Christianity', '5a81ff2b31013a001a335085': '', '5a81ff2b31013a001a335087': '', '5a81ff2b31013a001a335088': '', '5726d3475951b619008f7f25': 'Wake Island', '57318a1305b4da19006bd25e': 'tile', '5acf9e8d77cf76001a68557c': '', '572816302ca10214002d9d98': 'Ukrainian Independence Day', '572816302ca10214002d9d99': 'religious service', '5726a792dd62a815002e8c1a': 'February 1985', '5726a792dd62a815002e8c1b': 'Vision Quest', '5726a792dd62a815002e8c1c': 'March 1985', '5726fd0f5951b619008f8423': '25 September', '5726fd0f5951b619008f8425': 'Sir John Burgoyne', '5726fd0f5951b619008f8426': 'Raglan and St Arnaud', '570d3ebefed7b91900d45d8e': 'Mediterranean western coast', '570d3ebefed7b91900d45d8f': 'second', '570d3ebefed7b91900d45d90': 'oranges', '572f02fbcb0c0d14000f1709': 'Thailand', '572f02fbcb0c0d14000f170a': 'colonial powers', '5732488d0fdd8d15006c68eb': '15th Infantry', '5732488d0fdd8d15006c68ef': 'Louisiana Maneuvers', '57266708708984140094c4e6': 'Karstadt', '57266708708984140094c4e7': 'KaDeWe', '57266708708984140094c4e8': 'GALERIA Kaufhof', '5ad0b1c4645df0001a2d007a': '', '5ad0b1c4645df0001a2d007b': '', '5ad0b1c4645df0001a2d007c': '', '5ad0b1c4645df0001a2d007d': '', '5ad0b1c4645df0001a2d007e': '', '56dedfc3c65bf219000b3d9e': '2012', '56dedfc3c65bf219000b3d9f': 'chairman', '56f8cc9a9b226e1400dd102f': 'Farming', '56f8cc9a9b226e1400dd1032': 'mid-June', '572ff633a23a5019007fcbbc': 'New York Times', '572ff633a23a5019007fcbbd': 'Professor Ehsan Yarshater', '572ff633a23a5019007fcbbe': 'Iran', '573192cde6313a140071d0c4': 'Sean Connery', '573192cde6313a140071d0c5': 'Tim Burton', '573192cde6313a140071d0c6': 'a daredevil pilot who extinguishes forest fires', '5ad4c0485b96ef001a109f31': '', '5ad4c0485b96ef001a109f32': '', '570a6f236d058f1900182e56': 'Paul Ekman', '570a6f236d058f1900182e57': 'six', '570a6f236d058f1900182e58': 'surprise', '5ad243b2d7d075001a428a1b': '', '5ad243b2d7d075001a428a1c': '', '5726a88e5951b619008f7950': 'The resurgent republican movement', '5a21dc2f8a6e4f001aa08fae': '', '5a21dc2f8a6e4f001aa08fb1': '', '5a21dc2f8a6e4f001aa08fb2': '', '5acf8ff377cf76001a685276': '', '5acf8ff377cf76001a685277': '', '5acf8ff377cf76001a685278': '', '572f78f5b2c2fd1400568161': 'structured document-oriented database', '572f78f5b2c2fd1400568162': 'enterprise database management', '5a8c840bfd22b3001a8d89de': '', '5a8c840bfd22b3001a8d89df': '', '5a8c840bfd22b3001a8d89e0': '', '5a8c840bfd22b3001a8d89e1': '', '57313c97e6313a140071cd6c': 'Qianlong', '57313c97e6313a140071cd6d': '53', '56f754a3a6d7ea1400e171bf': '2010', '56f754a3a6d7ea1400e171c0': 'Peter Underhill', '5ad4c6d55b96ef001a10a01a': '', '5ad4c6d55b96ef001a10a01b': '', '5ad4c6d55b96ef001a10a01c': '', '56cecf68aab44d1400b88ab8': 'tofu-dregs schoolhouses', '56d64a821c85041400947076': 'over 7,000', '56d64a821c85041400947077': 'tofu-dregs schoolhouses', '5727c9722ca10214002d9632': 'external portable USB hard disk drives', '5727c9722ca10214002d9634': 'Thunderbolt', '5727c9722ca10214002d9635': 'a \"translating device\"', '56e0f68f7aa994140058e833': 'Valentina Tereshkova', '56e0f68f7aa994140058e834': 'Vostok 6', '570a88724103511400d59818': '$1.1 billion', '570a88724103511400d59819': '24,000', '570a88724103511400d5981c': '80.5%', '5ad42096604f3c001a400701': '', '5ad42096604f3c001a400702': '', '5ad42096604f3c001a400703': '', '5ad42096604f3c001a400705': '', '57282c833acd2414000df63f': 'Eunicidae and Phyllodocidae', '57282c833acd2414000df641': 'palps covered in cilia', '5ace874132bba1001ae4a97f': '', '5ace874132bba1001ae4a980': '', '5ace874132bba1001ae4a981': '', '5ace874132bba1001ae4a982': '', '573238630fdd8d15006c685f': 'pontifex maximus', '573238630fdd8d15006c6861': 'Theodosius I', '573238630fdd8d15006c6862': 'Sacred fire', '573238630fdd8d15006c6863': 'East and West', '572618d0ec44d21400f3d8c5': 'Al-Biruni', '572618d0ec44d21400f3d8c6': '150–100 BC', '56f7fd15a6d7ea1400e1735f': 'Varius', '56f7fd15a6d7ea1400e17360': 'Servius and Donatus', '56f7fd15a6d7ea1400e17361': 'inferences made from his poetry and allegorizing', '56f7fd15a6d7ea1400e17362': 'problematic', '5a7e3aec70df9f001a8755c5': '', '5a7e3aec70df9f001a8755c6': '', '5a7e3aec70df9f001a8755c7': '', '5a7e3aec70df9f001a8755c8': '', '5709720ded30961900e84165': 'Thompson Seedless', '5709720ded30961900e84166': 'Niagara grapes', '5727aa682ca10214002d9339': '548,000', '5a6fe7e38abb0b001a676037': '', '5a6fe7e38abb0b001a676038': '', '5a6fe7e38abb0b001a676039': '', '5a6fe7e38abb0b001a67603b': '', '57269b6b708984140094cb75': 'impure', '57269b6b708984140094cb78': 'wrought iron', '5a2052ef54a786001a36b301': '', '5a2052ef54a786001a36b302': '', '5a2052ef54a786001a36b303': '', '57105362b654c5140001f8cb': 'France', '57105362b654c5140001f8cc': 'Isaac Newton', '57105362b654c5140001f8cd': 'France', '572818a9ff5b5019007d9d20': '25,000', '572818a9ff5b5019007d9d21': 'one-hour', '572818a9ff5b5019007d9d22': 'eight local factories and institutions', '572818a9ff5b5019007d9d23': '1944', '57096228ed30961900e8403e': 'handicrafts', '57096228ed30961900e8403f': 'woolen and pashmina shawls', '57096228ed30961900e84040': 'increased', '57096228ed30961900e84041': 'aesthetic and tasteful handicrafts', '5a362317788daf001a5f8746': '', '5a362317788daf001a5f8747': '', '5a362317788daf001a5f8749': '', '57265630708984140094c2d1': 'Lawrence Lessig', '5a15b101a54d4200185294ad': '', '5a15b101a54d4200185294ae': '', '5a15b101a54d4200185294af': '', '5a15b101a54d4200185294b0': '', '572675a3dd62a815002e85b2': '10,000,000', '572675a3dd62a815002e85b4': 'Russia', '572675a3dd62a815002e85b6': 'Informal Empire', '5728f26aaf94a219006a9e27': 'Romanticism', '5728f26aaf94a219006a9e28': 'Montmartre', '5728f26aaf94a219006a9e29': 'Montmartre and Montparnasse', '5728f26aaf94a219006a9e2a': '1905 and 1907', '5ad28a70d7d075001a4299b8': '', '5ad28a70d7d075001a4299b9': '', '5ad28a70d7d075001a4299bb': '', '5ad28a70d7d075001a4299bc': '', '5728a75e4b864d1900164b92': 'Bartolomeo Bortolazzi', '5ad211e7d7d075001a4282de': '', '5ad211e7d7d075001a4282df': '', '5ad211e7d7d075001a4282e0': '', '5ad211e7d7d075001a4282e1': '', '5726e2bf708984140094d4c3': 'Muslim', '5726e2bf708984140094d4c4': 'rational philosophy', '5726e2bf708984140094d4c6': '19th century', '5ace618b32bba1001ae4a4cd': '', '5ace618b32bba1001ae4a4ce': '', '5ace618b32bba1001ae4a4cf': '', '5ace618b32bba1001ae4a4d0': '', '5ace618b32bba1001ae4a4d1': '', '5acea21632bba1001ae4ae19': '', '5acea21632bba1001ae4ae1a': '', '5acea21632bba1001ae4ae1b': '', '5acea21632bba1001ae4ae1d': '', '572ac9cbbe1ee31400cb8255': '$750 million', '572ac9cbbe1ee31400cb8258': '2002', '572947026aef051400154c4c': 'John Rolfe', '5ad40858604f3c001a3ffee1': '', '5ad40858604f3c001a3ffee2': '', '5ad40858604f3c001a3ffee4': '', '5ad40858604f3c001a3ffee5': '', '5726fec6dd62a815002e973c': 'where the accumulation of snow and ice exceeds ablation', '5726fec6dd62a815002e973e': 'armchair-shaped', '5a358b22788daf001a5f8624': '', '5a358b22788daf001a5f8625': '', '5a358b22788daf001a5f8626': '', '5a358b22788daf001a5f8627': '', '5ad50a975b96ef001a10aa91': '', '5ad50a975b96ef001a10aa92': '', '5ad50a975b96ef001a10aa93': '', '5ad50a975b96ef001a10aa94': '', '5ad690ae191832001aa7b21f': '', '5ad690ae191832001aa7b220': '', '572f8a4904bcaa1900d76a6a': 'thalamus', '572f8a4904bcaa1900d76a6b': 'spinal cord fibers', '572f8a4904bcaa1900d76a6c': 'the insular cortex', '5acd38ac07355d001abf3982': '', '5acd38ac07355d001abf3984': '', '572688895951b619008f7601': 'Jack Brickhouse', '572688895951b619008f7602': 'Hey Hey!', '572745d8f1498d1400e8f590': 'Washington D.C.', '5ad4171c604f3c001a400377': '', '5ad4171c604f3c001a400378': '', '5ad4171c604f3c001a400379': '', '5ad4171c604f3c001a40037a': '', '570d2d61b3d812140066d4ed': '2002', '570d2d61b3d812140066d4ef': '2008', '570d2d61b3d812140066d4f1': 'May 2010', '59d291ec2763a600182840cf': '', '59d291ec2763a600182840d1': '', '59d291ec2763a600182840d3': '', '572a54e07a1753140016aeb3': 'August 31, 2011', '5a5535e0134fea001a0e19d6': '', '5a5535e0134fea001a0e19d7': '', '5a5535e0134fea001a0e19d8': '', '5a5535e0134fea001a0e19d9': '', '5731dd32e17f3d14004224ba': 'pervasive', '5731dd32e17f3d14004224bc': 'anti-Catholic sentiment', '5731dd32e17f3d14004224bd': 'strict separationism', '5ad14b17645df0001a2d15e0': '', '5ad14b17645df0001a2d15e1': '', '5ad14b17645df0001a2d15e3': '', '5ad14b17645df0001a2d15e4': '', '5726bbc15951b619008f7c55': 'Wake County Public School System', '5726bbc15951b619008f7c56': 'innovative efforts to maintain a socially, economically and racial balanced system', '5726bbc15951b619008f7c57': 'three', '5726bbc15951b619008f7c58': 'International Baccalaureate', '5acd648707355d001abf407c': '', '5acd648707355d001abf407d': '', '5acd648707355d001abf407e': '', '5acd648707355d001abf407f': '', '5acd648707355d001abf4080': '', '571008d5a58dae1900cd67ec': \"relation of heterosexuality to homosexuality in one's history\", '5a86079eb4e223001a8e73cb': '', '5a86079eb4e223001a8e73cc': '', '5a86079eb4e223001a8e73cd': '', '572b65e4be1ee31400cb8359': \"Guam rail (or ko'ko' bird in Chamorro) and the Guam flycatcher\", '5ace640e32bba1001ae4a553': '', '56e8daed0b45c0140094cd17': 'coronation site', '56e8daed0b45c0140094cd18': 'Henry Yevele', '56e8daed0b45c0140094cd19': 'Richard II', '56e8daed0b45c0140094cd1a': 'Cosmati', '56e8daed0b45c0140094cd1b': \"The Confessor's shrine\", '5ad3f42d604f3c001a3ff92c': '', '5ad3f42d604f3c001a3ff92e': '', '5ad3f42d604f3c001a3ff92f': '', '5728ee664b864d19001650b5': '40,000', '5728ee664b864d19001650b6': '900', '5728ee664b864d19001650b7': 'northern Kyūshū', '5728ee664b864d19001650b8': '10,000', '572670f9dd62a815002e84fa': 'Don Baylor', '572670f9dd62a815002e84fb': 'Mack Newton', '572670f9dd62a815002e84fc': 'Preston Wilson', '5728b1163acd2414000dfcd1': 'Andy Irvine', '5728b1163acd2414000dfcd2': 'John Sheahan and the late Barney McKenna', '5728b1163acd2414000dfcd5': 'Rory Gallagher', '5ad22b4bd7d075001a4285ec': '', '5ad22b4bd7d075001a4285ed': '', '5ad22b4bd7d075001a4285ef': '', '570a6e2f4103511400d596f8': 'Michael C. Graham', '570a6e2f4103511400d596f9': 'Psychotherapist', '570a6e2f4103511400d596fc': 'Moods', '5ad24351d7d075001a4289e0': '', '5ad24351d7d075001a4289e1': '', '5ad24351d7d075001a4289e2': '', '5ad24351d7d075001a4289e3': '', '570f40f65ab6b81900390eb3': 'Melatonin', '5a223ceb819328001af38a47': '', '5a223ceb819328001af38a48': '', '5a223cec819328001af38a49': '', '5a223cec819328001af38a4a': '', '572a630e7a1753140016aefb': 'George H. W. Bush', '572a630e7a1753140016aefc': 'Freiburg, Germany', '572a630e7a1753140016aefe': 'American Economic Review', '5726a4465951b619008f78bf': '2007', '5726a4465951b619008f78c1': '11', '5726a4465951b619008f78c2': 'Pachuca', '5726f3eef1498d1400e8f0c6': 'Sumer', '5726f3eef1498d1400e8f0c8': 'numerical data', '5726f3eef1498d1400e8f0c9': '18th century BC', '5726f3eef1498d1400e8f0ca': 'Plimpton 322', '572fa91e04bcaa1900d76b69': 'light-gathering membrane', '5731e810e17f3d1400422535': 'indigenous, Mediterranean, African and Western', '5731e810e17f3d1400422536': 'Hathor', '5731e810e17f3d1400422538': 'Amr Diab and Mohamed Mounir', '56e7b14c37bdd419002c4371': '2014', '56e7b14c37bdd419002c4372': 'six', '570e58ee0dc6ce1900204f75': 'William Pitt', '570e58ee0dc6ce1900204f76': \"Charles D'Ebro and Richard Speight\", '570e58ee0dc6ce1900204f77': 'Eureka Tower', '570e58ee0dc6ce1900204f78': '2006', '570e58ee0dc6ce1900204f79': 'second largest', '5727e243ff5b5019007d977a': 'New Orleans Hornets', '5727e243ff5b5019007d977c': 'Hurricane Katrina', '5727e243ff5b5019007d977e': 'Professional Basketball Club LLC', '56cff6f3234ae51400d9c191': 'Karol Szymanowski', '56cff6f3234ae51400d9c193': 'Nikolai Zverev', '56d3a9282ccc5a1400d82dc8': 'Karol Szymanowski', '56d3a9282ccc5a1400d82dc9': 'Alexander Scriabin', '56d3a9282ccc5a1400d82dca': 'Nikolai Zverev', '57296ab01d046914007793e7': 'W. Klement', '57296ab01d046914007793eb': 'Caltech', '5a67151af038b7001ab0c1c6': '', '5a67151af038b7001ab0c1c7': '', '5a67151af038b7001ab0c1c9': '', '5726994b708984140094cb4b': 'Paul Gauguin and Paul Cézanne', '5726994b708984140094cb4c': 'The Demoiselles', '5acfc92477cf76001a685f86': '', '5acfc92477cf76001a685f87': '', '5acfc92477cf76001a685f89': '', '56dcf8689a695914005b94a0': 'Sassou', '5ad00a9d77cf76001a6867e2': '', '5ad00a9d77cf76001a6867e4': '', '56f78981aef2371900625ba9': '1874', '56f78981aef2371900625bad': '1986', '56f952ba9b226e1400dd1312': 'Spain', '56f952ba9b226e1400dd1315': '1919', '56f952ba9b226e1400dd1316': '1986', '56cfebbd234ae51400d9c0c9': 'Solar heating, cooling and ventilation technologies', '5728bf3a2ca10214002da6d3': 'Maurice de Sully', '5728bf3a2ca10214002da6d4': 'The Left Bank', '5728bf3a2ca10214002da6d5': 'Louis VII', '5a864143b4e223001a8e750a': '', '5a864143b4e223001a8e750b': '', '5a864143b4e223001a8e750c': '', '5a864143b4e223001a8e750d': '', '572808cd2ca10214002d9c1e': '3 June 2013', '572808cd2ca10214002d9c1f': 'Pope Francis', '572808cd2ca10214002d9c20': 'Bergamo', '572808cd2ca10214002d9c21': '5 July 2013', '5a61447ae9e1cc001a33d031': '', '5a61447ae9e1cc001a33d032': '', '56dfbe7c231d4119001abd70': 'medium-to-large businesses, or other ISPs', '56dfbe7c231d4119001abd73': 'synchronous optical networking', '5a10d9ce06e79900185c341f': '', '572e8519c246551400ce42b0': 'Sir William Armstrong', '572e8519c246551400ce42b2': 'they provided a much greater force', '572e8519c246551400ce42b4': 'Counterweights and balances', '572fa79aa23a5019007fc839': '1992, 2000, 2004, and 2016', '572fa79aa23a5019007fc83a': 'Washington University Athletic Complex', '572fa79aa23a5019007fc83b': 'scheduling difficulties between the candidates', '5ace1b9532bba1001ae49adb': '', '5ace1b9532bba1001ae49adc': '', '5ace1b9532bba1001ae49add': '', '5ace1b9532bba1001ae49ade': '', '5ace1b9532bba1001ae49adf': '', '5725eaf7271a42140099d319': 'radiant luminous efficacy', '5725eaf7271a42140099d31a': '683 lm/W', '5725eaf7271a42140099d31c': 'the ratio of the visible light flux emitted (the luminous flux) to the total power radiated over all wavelengths', '5725eaf7271a42140099d31d': 'source luminous efficacy', '5ad18d4e645df0001a2d1f08': '', '5ad18d4e645df0001a2d1f09': '', '5ad18d4e645df0001a2d1f0a': '', '5ad18d4e645df0001a2d1f0b': '', '5ad18d4e645df0001a2d1f0c': '', '5a0cc44cf5590b0018dab54c': '', '5a0cc44cf5590b0018dab54d': '', '5a0cc44cf5590b0018dab54e': '', '5a0cc44cf5590b0018dab54f': '', '5a0cc44cf5590b0018dab550': '', '572734eb5951b619008f86b8': 'Empire Stadium', '5a8c802dfd22b3001a8d8955': '', '5a8c802dfd22b3001a8d8956': '', '5a8c802dfd22b3001a8d8957': '', '5a8c802dfd22b3001a8d8958': '', '5735c0d8e853931400426b4b': 'three', '572830622ca10214002da033': 'The Formation of Vegetable Mould through the Action of Worms', '572830622ca10214002da035': 'oxygen and water can penetrate it', '5ace8a7232bba1001ae4a9d9': '', '5ace8a7232bba1001ae4a9da': '', '5ace8a7232bba1001ae4a9dd': '', '56f8d4209b226e1400dd10ad': 'France', '56f8d4209b226e1400dd10ae': 'Switzerland', '56e79c1c00c9c71400d773a3': 'Chongqing and Wuhan', '570ac16f4103511400d5998c': 'the penalty it exacts on aircraft size, payload, and fuel load', '570ac16f4103511400d5998e': 'a catapult or JATO rocket', '570ac16f4103511400d5998f': 'would eliminate one or more helicopter landing areas', '570ac16f4103511400d59990': 'with a minimal armament and fuel load', '5acd84c307355d001abf453b': '', '5acd84c307355d001abf453c': '', '5acd84c307355d001abf453d': '', '5acd84c307355d001abf453e': '', '572901d03f37b31900477f74': '1960s', '572901d03f37b31900477f75': 'Hector Guimard', '572901d03f37b31900477f76': '1913', '56cdd83862d2951400fa68e0': 'MGM', '56cdd83862d2951400fa68e1': 'SPECTRE', '56cdd83862d2951400fa68e2': 'Eon Productions', '56cf39c4aab44d1400b88ebc': 'Danjaq', '5ad22870d7d075001a428558': '', '5ad22870d7d075001a428559': '', '5ad22870d7d075001a42855a': '', '57267c12dd62a815002e86c2': 'William Gladstone', '57267c12dd62a815002e86c4': '1916', '5731edafe17f3d1400422559': 'Amsterdam', '5731edafe17f3d140042255a': 'John Smyth', '5731edafe17f3d140042255c': 'Roger Williams', '5731edafe17f3d140042255d': 'Baptist missionaries', '57321a39e99e3014001e651d': 'Pouākai', '57321a39e99e3014001e651e': 'National Audubon Society', '57321a39e99e3014001e651f': 'nightingales', '5731f0ffe99e3014001e63ec': 'devotio', '57324b39b9d445190005e9d5': 'a rice-stuffed pumpkin dish', '5a53f1c6bdaabd001a386815': '', '5a53f1c6bdaabd001a386816': '', '5727f9a14b864d190016410f': '33 1⁄3 rpm', '572a31481d04691400779830': 'seven', '5a110ff906e79900185c3516': '', '5a110ff906e79900185c3517': '', '5a110ff906e79900185c3518': '', '56e4731e8c00841900fbaf93': 'the aesthetic', '56e4731e8c00841900fbaf95': 'string courses or rustication', '5acf9cb577cf76001a6854dc': '', '5acf9cb577cf76001a6854dd': '', '5acf9cb577cf76001a6854de': '', '572714d2708984140094d975': 'Cancer', '572714d2708984140094d976': 'International Agency for Research on Cancer', '572714d2708984140094d978': 'tobacco', '572714d2708984140094d979': 'Western', '5705e3dd75f01819005e76fa': 'George V', '5705e3dd75f01819005e76fb': '12 December 1911', '5705e3dd75f01819005e76fc': 'King George V and Queen Mary', '5705e3dd75f01819005e76fd': 'Edwin Lutyens', '5705e3dd75f01819005e76fe': '10 February 1931', '5725ee0a38643c19005acea9': '6,300 °C', '5725ee0a38643c19005aceaa': '52 lumens per watt', '5725ee0a38643c19005acead': '52 lumens per watt', '5ad18e11645df0001a2d1f12': '', '5ad18e11645df0001a2d1f13': '', '5ad18e11645df0001a2d1f14': '', '5ad18e11645df0001a2d1f15': '', '5ad18e11645df0001a2d1f16': '', '5a0cc174f5590b0018dab543': '', '5a0cc174f5590b0018dab546': '', '56e6f6e0de9d371400068101': 'Christian AC', '56e6f6e0de9d371400068102': 'Radio & Records', '56e6f6e0de9d371400068103': 'hot AC', '5731c5ba0fdd8d15006c651d': '2005', '5731c5ba0fdd8d15006c651e': 'two', '5731c5ba0fdd8d15006c651f': 'Geoff Zanelli', '5731c5ba0fdd8d15006c6521': '$250 million', '5ad4ccbb5b96ef001a10a106': '', '5ad4ccbb5b96ef001a10a107': '', '5ad4ccbb5b96ef001a10a108': '', '5ad4ccbb5b96ef001a10a109': '', '5ad4ccbb5b96ef001a10a10a': '', '57293d583f37b3190047816d': '1,360 GW', '5ad1248d645df0001a2d0f1e': '', '5ad1248d645df0001a2d0f1f': '', '5ad1248d645df0001a2d0f20': '', '5706aab252bb891400689b3a': 'Detroit techno music', '5706aab252bb891400689b3b': 'the exclusive association of particular tracks with particular clubs and DJs', '5729a3d56aef051400155074': 'delicacies', '5729a3d56aef051400155075': 'cicadas', '5729a3d56aef051400155076': 'high', '5729a3d56aef051400155077': 'entomophagy', '5ad1171b645df0001a2d0d28': '', '5ad1171b645df0001a2d0d29': '', '5ad1171b645df0001a2d0d2a': '', '5ad1171b645df0001a2d0d2c': '', '56e6d988de9d371400068084': 'the 2005-2007 economic downturn', '56e6d988de9d371400068087': 'physical record', '56d2124ce7d4791d0090263d': 'the yogis', '56d2124ce7d4791d0090263e': 'mindfulness and clear awareness', '5729667c3f37b3190047833d': 'Art Nouveau', '5729667c3f37b3190047833e': 'Teatro Massimo', '5a3ea39b5a76c5001a3a83f1': '', '5a3ea39b5a76c5001a3a83f2': '', '5a3ea39b5a76c5001a3a83f4': '', '572e7aa8dfa6aa1500f8d00e': '10', '572e7aa8dfa6aa1500f8d010': 'three', '572e7aa8dfa6aa1500f8d011': '1 yard', '5a0dcbf06e16420018587b3e': '', '5a0f39cfdecec900184754eb': '', '5a0f39cfdecec900184754ec': '', '5a0f39cfdecec900184754ee': '', '5a0f39cfdecec900184754ef': '', '572f39d804bcaa1900d7679e': 'Fruit', '572f39d804bcaa1900d767a1': 'flowering', '5a3ad7933ff257001ab842b3': '', '5a3ad7933ff257001ab842b5': '', '5a3ad7933ff257001ab842b6': '', '56df84d756340a1900b29cd2': 'Sarah Fuller', '56d3805859d6e41400146581': 'Randy Jackson', '56d3805859d6e41400146582': 'one', '56db2e2de7c41114004b4eeb': 'Randy Jackson', '56db2e2de7c41114004b4eec': 'Mariah Carey and Nicki Minaj', '572a3fa3af94a219006aa90d': 'University of Vienna', '572a3fa3af94a219006aa90e': \"Constantin von Monakow's Institute of Brain Anatomy\", '572a3fa3af94a219006aa90f': 'The Sensory Order', '572a3fa3af94a219006aa910': 'the Geistkreis', '572a3fa3af94a219006aa911': 'Herbert Furth', '5726c1825951b619008f7d51': 'marine organisms', '572931113f37b319004780cc': 'the floor of the Atlantic, and the Mid-Atlantic Ridge', '5ad3e9c6604f3c001a3ff6a5': '', '5ad3e9c6604f3c001a3ff6a6': '', '57279543f1498d1400e8fcc6': 'three', '57279543f1498d1400e8fcc9': 'a constitutional initiative and a referendum', '57279543f1498d1400e8fcca': 'Direct democracy and federalism', '5730187d947a6a140053d0d3': 'Catalina Foothills', '573428154776f419006619b5': 'Catalina Foothills', '573428154776f419006619b8': 'La Encantada', '573428154776f419006619b9': 'Hacienda Del Sol, Westin La Paloma Resort, Loews Ventana Canyon Resort and Canyon Ranch Resort', '56dde98d9a695914005b96aa': 'French kings', '5ad2e74a604f3c001a3fd95e': '', '5ad2e74a604f3c001a3fd95f': '', '5ad2e74a604f3c001a3fd960': '', '572a18b96aef051400155267': 'Osiris', '572a18b96aef051400155268': 'to protect them from evil', '5a74e30342eae6001a389b0c': '', '5a74e30342eae6001a389b0e': '', '5a74e30342eae6001a389b0f': '', '56d0875b234ae51400d9c348': 'convert solar light to heat', '56d0875b234ae51400d9c34b': 'Europe', '572eb9a703f98919007569a5': 'Abdus Salam', '572eb9a703f98919007569a6': '67:3-4', '572eb9a703f98919007569a7': 'Ibn al-Haytham and Al-Biruni', '5ad2292fd7d075001a42857c': '', '5ad2292fd7d075001a42857d': '', '5ad2292fd7d075001a42857e': '', '5727c25f2ca10214002d9592': 'formalist', '5a3b0aab3ff257001ab843b7': '', '5a3b0aab3ff257001ab843b8': '', '5a3b0aab3ff257001ab843b9': '', '5a3b0aab3ff257001ab843ba': '', '5a3b0aab3ff257001ab843bb': '', '56e10245e3433e1400422a96': '2 November 2006', '56e10245e3433e1400422a97': '10 meters', '5acd35ea07355d001abf3964': '', '5acd35ea07355d001abf3965': '', '5acd35ea07355d001abf3966': '', '5acd35ea07355d001abf3967': '', '5acd35ea07355d001abf3968': '', '5728d9e23acd2414000e002f': 'March 10, 2004', '5728d9e23acd2414000e0030': 'September 2, 2004', '5728d9e23acd2414000e0031': 'Dick Cheney', '5728d9e23acd2414000e0033': 'Conservative Party', '5a7224040efcfe001a8afe47': '', '5a7224040efcfe001a8afe49': '', '5732a7f71d5d2e14009ff87b': 'Eocene', '5732a7f71d5d2e14009ff87c': 'huge lakes', '5732a7f71d5d2e14009ff87d': 'Tethys Sea', '5a4ebb46af0d07001ae8cc0a': '', '5a4ebb46af0d07001ae8cc0b': '', '5a4ebb46af0d07001ae8cc0c': '', '5a4ebb46af0d07001ae8cc0d': '', '5a4ebb46af0d07001ae8cc0e': '', '56de94c84396321400ee2a41': 'Conan the Republican', '56e8648f37bdd419002c44ea': 'River Aare', '5728ddb1ff5b5019007da884': 'Anti-communism', '5a6a5f32a9e0c9001a4e9db6': '', '5a6a5f32a9e0c9001a4e9db8': '', '5a6a5f32a9e0c9001a4e9db9': '', '5a6a5f32a9e0c9001a4e9dba': '', '572b85aef75d5e190021fe21': 'phytates', '5acfcdfc77cf76001a6860cc': '', '5acfcdfc77cf76001a6860cd': '', '5acfcdfc77cf76001a6860ce': '', '5acfcdfc77cf76001a6860cf': '', '570a5afd6d058f1900182d96': 'Richard Lazarus', '570a5afd6d058f1900182d97': 'judgments', '570a5afd6d058f1900182d98': 'unconscious', '5ad24963d7d075001a428b9e': '', '5ad24963d7d075001a428b9f': '', '5ad24963d7d075001a428ba0': '', '5733a3cbd058e614000b5f3f': 'The College of Arts and Letters', '5733a3cbd058e614000b5f40': '1842', '5733a3cbd058e614000b5f41': '1849', '5733a3cbd058e614000b5f42': 'Saint Louis University', '5733a3cbd058e614000b5f43': '33', '572acbfe111d821400f38d7e': 'Toruń', '572acbfe111d821400f38d80': '1976', '572acbfe111d821400f38d82': 'ten', '5726ba315951b619008f7c03': 'tin', '5726ba315951b619008f7c05': 'tin', '5726ba315951b619008f7c06': 'alloyed with other metals', '5726ba315951b619008f7c07': 'lead, antimony, bismuth or copper', '5a20a62e8a6e4f001aa08e02': '', '5a20a62e8a6e4f001aa08e03': '', '5a20a62e8a6e4f001aa08e04': '', '5a20a62e8a6e4f001aa08e05': '', '57277d20f1498d1400e8f990': 'manufacturing', '57277d20f1498d1400e8f992': '56%', '5729778f6aef051400154f5a': 'manufacturing', '573040dd947a6a140053d33d': 'British war economy', '573040dd947a6a140053d33e': 'British coastal centres and shipping at sea west of Ireland', '56cee43eaab44d1400b88c05': '1799', '56cee43eaab44d1400b88c06': 'Manhattan', '56cee43eaab44d1400b88c07': 'Alexander Hamilton', '56cee43eaab44d1400b88c08': '1827', '56cee43eaab44d1400b88c09': '16,000', '56cfbb5f234ae51400d9bf2b': '1799', '56cfbb5f234ae51400d9bf2c': '1827', '57315327e6313a140071ce21': 'Alexandria', '57315327e6313a140071ce23': 'Egyptian', '5729e5501d0469140077965b': 'Elastic energy in materials', '5729e5501d0469140077965d': 'kinetic', '5acd166907355d001abf3418': '', '5acd166907355d001abf3419': '', '5acd166907355d001abf341b': '', '5acd166907355d001abf341c': '', '56e1688fe3433e1400422ebc': 'Marty Walsh', '56e1688fe3433e1400422ebe': 'The School Committee', '5732696fe17f3d140042295f': 'refusal of the Russians to permit any sort of inspections', '5732696fe17f3d1400422960': 'London', '5732696fe17f3d1400422962': 'Austria', '5732696fe17f3d1400422963': 'Open Skies', '56f81fe6aef2371900625df5': 'The altitude and size of the range', '56f81fe6aef2371900625df6': 'ibex', '56f81fe6aef2371900625df7': 'Edelweiss', '5acf5b6977cf76001a684c1a': '', '5acf5b6977cf76001a684c1b': '', '5acf5b6977cf76001a684c1c': '', '5ad418a2604f3c001a400415': '', '5ad418a2604f3c001a400416': '', '5ad418a2604f3c001a400417': '', '5ad418a2604f3c001a400418': '', '573149e7e6313a140071cdcf': 'Pulse oximeters', '573149e7e6313a140071cdd1': 'warm-up', '5ad1981f645df0001a2d20e0': '', '5ad1981f645df0001a2d20e1': '', '5a70dbb38abb0b001a6761c5': '', '5a70dbb38abb0b001a6761c6': '', '5a70dbb38abb0b001a6761c7': '', '5a70dbb38abb0b001a6761c8': '', '5a70dbb38abb0b001a6761c9': '', '572781a5f1498d1400e8fa1c': 'council-manager', '572781a5f1498d1400e8fa1d': '11', '572781a5f1498d1400e8fa1e': 'two-year', '5ace433e32bba1001ae4a115': '', '5ace433e32bba1001ae4a116': '', '5ace433e32bba1001ae4a117': '', '5727d0914b864d1900163dc0': 'humid subtropical', '5727d0914b864d1900163dc1': 'Tornado Alley', '5727d0914b864d1900163dc4': 'Mexico', '570b4c3c6b8089140040f860': 'The War on Terrorism', '570b4c3c6b8089140040f864': 'Arkansas and Texas', '5ad1787c645df0001a2d1d70': '', '5ad1787c645df0001a2d1d71': '', '5ad1787c645df0001a2d1d73': '', '5726ec71708984140094d635': 'December 2011', '5726ec71708984140094d636': 'Kim Jong Un', '5726ec71708984140094d637': 'Kim Jong Un', '57303157b2c2fd1400568a37': 'Maximum power transfer', '57303157b2c2fd1400568a38': '50 ohms', '57303157b2c2fd1400568a3a': 'standing wave ratio', '572810aa3acd2414000df393': 'Sodor and Man Diocesan Synod', '572810aa3acd2414000df394': 'Church of England in the Island', '572810aa3acd2414000df395': 'Order in Council', '572810aa3acd2414000df396': 'lieutenant governor', '572a9e1bf75d5e190021fba6': '1994', '572a9e1bf75d5e190021fba7': 'the lieutenant governor', '5acfb9fb77cf76001a685ab2': '', '5acfb9fb77cf76001a685ab4': '', '5acfb9fb77cf76001a685ab5': '', '56ddc1d966d3e219004dacd4': 'Sun Open Storage', '56ddc1d966d3e219004dacd5': \"Sun Microsystems' California campus\", '5a6b0974a9e0c9001a4e9e88': '', '5a6b0974a9e0c9001a4e9e89': '', '5a6b0974a9e0c9001a4e9e8a': '', '5a6b0974a9e0c9001a4e9e8b': '', '5a82062531013a001a335101': '', '5a82062531013a001a335105': '', '5731f24bb9d445190005e6d3': 'Publius Claudius Pulcher', '570e70c30b85d914000d7eff': 'Melbourne', '570e70c30b85d914000d7f00': 'Victoria', '5725c61c38643c19005acc9b': 'Energy', '5725c61c38643c19005acc9c': 'telecommunications', '570c9048b3d812140066d227': '196', '570c9048b3d812140066d228': 'La Liga', '570c9048b3d812140066d229': 'June 1950', '570c9048b3d812140066d22a': '1954', '570c9048b3d812140066d22b': 'Archbishop of Barcelona Gregorio Modrego', '572600a9ec44d21400f3d7f7': 'Athens', '572600a9ec44d21400f3d7f8': 'Library of Alexandria', '572600a9ec44d21400f3d7f9': 'Pergamon', '572600a9ec44d21400f3d7fa': 'Rhodes', '572600a9ec44d21400f3d7fb': 'Seleucia', '56d4647c2ccc5a1400d83130': '2007', '56d4647c2ccc5a1400d83131': 'Chicago', '56d4647c2ccc5a1400d83134': 'August', '56de9a164396321400ee2a45': '55.4%', '56de9a164396321400ee2a47': '1.3 million', '56de9a164396321400ee2a48': '31%', '56de9a164396321400ee2a49': 'John G. Downey', '57279d0aff5b5019007d9107': 'Jacques Peirotes', '57279d0aff5b5019007d9109': 'Treaty of Versailles', '5a6fdd4c8abb0b001a675fd4': '', '5a6fdd4c8abb0b001a675fd5': '', '5a6fdd4c8abb0b001a675fd6': '', '570b421bec8fbc190045b925': '1860', '570b421bec8fbc190045b927': 'South Carolina', '570b421bec8fbc190045b928': 'April 12, 1861', '5ad17172645df0001a2d1bc8': '', '5ad17172645df0001a2d1bc9': '', '5ad17172645df0001a2d1bca': '', '570ba2b3ec8fbc190045baa1': 'protostars', '5a07f8453fc87400182070cb': '', '5a07f8453fc87400182070cd': '', '5a07f8453fc87400182070ce': '', '5a07f8453fc87400182070cf': '', '570b2dd76b8089140040f7d5': '1750', '570b2dd76b8089140040f7d6': 'Gregorian calendar', '570b2dd76b8089140040f7d8': 'Calendar Act of 1750', '5a37143595360f001af1b3eb': '', '5a37143595360f001af1b3ec': '', '5a37143595360f001af1b3ed': '', '5a37143595360f001af1b3ee': '', '5726b597f1498d1400e8e84f': 'belles-lettres', '5a7a34d717ab25001a8a03a3': '', '5a7a34d717ab25001a8a03a6': '', '5a7cfda9e8bc7e001a9e2105': '', '5a7cfda9e8bc7e001a9e2106': '', '5a7cfda9e8bc7e001a9e2107': '', '5a7d25d970df9f001a874fe2': '', '57268e59708984140094c9f5': 'Homo erectus', '57268e59708984140094c9f6': '400,000 years ago', '57268e59708984140094c9f7': 'Anyathian', '5726f6d3708984140094d731': '$250 million', '5726f6d3708984140094d732': '$70 million', '5726f6d3708984140094d733': 'Richard Gilder', '5ad3ece8604f3c001a3ff78b': '', '5ad3ece8604f3c001a3ff78c': '', '5ad3ece8604f3c001a3ff78d': '', '5ad3ece8604f3c001a3ff78f': '', '57262d20271a42140099d703': 'carbon', '5ad19bb3645df0001a2d2112': '', '5ad19bb3645df0001a2d2113': '', '5ad19bb3645df0001a2d2114': '', '5ad19bb3645df0001a2d2115': '', '5ad19bb3645df0001a2d2116': '', '572efc3503f9891900756b13': 'two', '572efc3503f9891900756b15': 'whole genome duplication', '572efc3503f9891900756b17': 'Darwin', '5a3acb0f3ff257001ab8428c': '', '5a3acb0f3ff257001ab8428e': '', '5a3acb0f3ff257001ab8428f': '', '56dedc703277331400b4d775': '1992', '570888bf9928a814004714da': 'blue', '570888bf9928a814004714db': 'red', '570888bf9928a814004714dd': 'navy blue', '570888bf9928a814004714de': 'home matches', '59fb256bee36d60018400d47': '', '59fb256bee36d60018400d48': '', '59fb256bee36d60018400d4a': '', '59fb256bee36d60018400d4b': '', '57324714b9d445190005e98f': 'executive officer', '57324714b9d445190005e990': 'Panama Canal Zone', '57324714b9d445190005e991': 'On War', '57324714b9d445190005e992': '1925–26', '57324714b9d445190005e993': '245', '572931841d04691400779145': 'uncertain', '572931841d04691400779147': '20,000', '572931841d04691400779149': 'undescribed', '572a016b3f37b3190047863d': 'Energy transfer', '572a016b3f37b3190047863e': 'heat', '5acd5d9e07355d001abf3f00': '', '5acd5d9e07355d001abf3f01': '', '5acd5d9e07355d001abf3f02': '', '5706be1e0eeca41400aa0de7': '$26,969', '5706be1e0eeca41400aa0de9': '$15,402', '5706be1e0eeca41400aa0dea': '19.1%', '5727bb132ca10214002d94f4': 'Santa Hermandad', '5727bb132ca10214002d94f5': 'Real Audiencia del Reino de Galicia', '5727bb132ca10214002d94f6': '10%', '5727bb2a3acd2414000deacf': 'sardines, wood, and some cattle and wine', '572ffe9ea23a5019007fcc1f': 'irrigation and farming', '572ffe9ea23a5019007fcc20': 'almonds and citrus fruit', '572ffe9ea23a5019007fcc21': 'Arab merchants', '572ffe9ea23a5019007fcc22': '16th century', '572ffe9ea23a5019007fcc23': 'Hormuz', '5a5447e0134fea001a0e16ee': '', '56cf68594df3c31400b0d73d': 'Graduation', '56cf68594df3c31400b0d73e': '50 Cent', '56cf68594df3c31400b0d73f': '957,000', '56cf68594df3c31400b0d740': 'Daft Punk', '56d1091117492d1400aab7bc': 'Graduation', '57303b58947a6a140053d2e0': 'Mediterranean Sea', '57303b58947a6a140053d2e2': 'Caesar', '57303b58947a6a140053d2e3': 'to meet several new demands', '56f865baaef237190062603f': 'Victorian era', '56f865baaef2371900626040': '1835', '56f865baaef2371900626041': 'October', '56f865baaef2371900626042': '1840', '56f865baaef2371900626043': 'The Gateway to the Empire', '5719c6a94faf5e1900b8a7eb': 'New York', '5719c6a94faf5e1900b8a7ee': 'April 1853', '5725f7a389a1e219009ac114': '15 September 1940', '5725f7a389a1e219009ac115': 'the Palace', '5725f7a389a1e219009ac117': \"King's Messenger\", '5725f7a389a1e219009ac118': '2005', '5726245b89a1e219009ac2f2': '2005', '5a7a513117ab25001a8a04eb': '', '5a7a513117ab25001a8a04ec': '', '5a7a513117ab25001a8a04ee': '', '56d09a0e234ae51400d9c3c2': '2060', '56d8e18ddc89441400fdb38a': 'Reporters Without Borders', '56db1ba8e7c41114004b4d3a': 'Reporters Without Borders', '57305ed58ab72b1400f9c4a8': 'around 500 BC', '57305ed58ab72b1400f9c4a9': 'Greek', '57305ed58ab72b1400f9c4ac': 'each household', '5a43023e4a4859001aac73e5': '', '5a43023e4a4859001aac73e6': '', '5a43023e4a4859001aac73e7': '', '5a43023e4a4859001aac73e8': '', '5a43023e4a4859001aac73e9': '', '57304e618ab72b1400f9c412': 'two', '57304e618ab72b1400f9c414': 'halves', '57304e618ab72b1400f9c416': '1891', '5a79e4bf17ab25001a8a0182': '', '5a79e4bf17ab25001a8a0183': '', '5a79e4bf17ab25001a8a0184': '', '5a79e4bf17ab25001a8a0185': '', '5a79e4bf17ab25001a8a0186': '', '56f9ebe18f12f3190062fff9': \"International Telecommunication Union's radio telecommunications sector\", '56f9ebe18f12f3190062fffa': 'Digital Video Broadcasting', '56f9ebe18f12f3190062fffd': 'ETSI', '5ad3b2f6604f3c001a3fed2f': '', '5ad3b2f6604f3c001a3fed30': '', '5ad3b2f6604f3c001a3fed31': '', '572ea993cb0c0d14000f1418': \"Sana'a, Yemen\", '5ad2183dd7d075001a4283f4': '', '5ad2183dd7d075001a4283f5': '', '5ad2183dd7d075001a4283f6': '', '5ad2183dd7d075001a4283f7': '', '5ad2183dd7d075001a4283f8': '', '5729058baf94a219006a9f69': 'Many religions are practised in Myanmar', '571a8f6a4faf5e1900b8aa7a': '40%', '571a8f6a4faf5e1900b8aa7c': 'founder lineages', '571a8f6a4faf5e1900b8aa7d': 'Hebrew/Levantine', '56de2fa0cffd8e1900b4b63e': 'July 24, 2014', '56de2fa0cffd8e1900b4b63f': '2013', '5ad0ccaa645df0001a2d03ac': '', '5ad0ccaa645df0001a2d03ad': '', '5ad0ccaa645df0001a2d03ae': '', '56e7a21637bdd419002c42a3': 'Jeffrey Vinik', '56e7a21637bdd419002c42a4': 'Hurricane Sandy', '572fe399a23a5019007fcae2': 'earthed', '572fe399a23a5019007fcae3': 'static charge', '572fe399a23a5019007fcae5': 'MCMs', '5ace846a32bba1001ae4a942': '', '5ace846a32bba1001ae4a943': '', '5ace846a32bba1001ae4a944': '', '57318a0ae6313a140071d06c': 'UN Human Rights Council', '57318a0ae6313a140071d06d': 'March', '57318a0ae6313a140071d06e': 'hundreds', '57318a0ae6313a140071d06f': 'United Arab Emirates', '56e039947aa994140058e3d5': 'manhua', '56e039947aa994140058e3d6': 'manhwa', '5acf95a777cf76001a685380': '', '5acf95a777cf76001a685381': '', '5acf95a777cf76001a685382': '', '5acf95a777cf76001a685383': '', '5acf95a777cf76001a685384': '', '572644761125e71900ae191e': 'Corsica', '572644761125e71900ae191f': '1793', '572644761125e71900ae1920': '26', '572644761125e71900ae1921': 'the Austrians and their Italian allies', '572644761125e71900ae1922': '1798', '572817584b864d1900164463': 'Inner London and Outer London', '572817584b864d1900164465': 'North and South', '56e14c63e3433e1400422d74': 'Tourism', '56e14c63e3433e1400422d75': '21.2 million', '56e14c63e3433e1400422d76': '$8.3 billion', '56e14c63e3433e1400422d77': '2014', '5727b1f42ca10214002d941c': '34.7%', '5727b1f42ca10214002d941d': 'one fifth', '5727b1f42ca10214002d941e': 'increase in xenophobia', '572fce07b2c2fd140056848b': 'Sebastián Vizcaíno', '572fce07b2c2fd140056848c': 'San Salvador', '572fce07b2c2fd140056848e': 'Friar Antonio de la Ascensión', '572fce07b2c2fd140056848f': 'Navidad, New Spain', '5ad4a7fc5b96ef001a109d14': '', '5ad4a7fc5b96ef001a109d15': '', '5ad4a7fc5b96ef001a109d16': '', '5ad4a7fc5b96ef001a109d17': '', '5ad4a7fc5b96ef001a109d18': '', '5727b2e53acd2414000dea15': 'reactivation', '5727b2e53acd2414000dea16': 'HIV', '5727b2e53acd2414000dea17': '8%', '5a871df11d3cee001a6a10e3': '', '5a871df11d3cee001a6a10e5': '', '572a2d163f37b3190047876d': '1,996,626 people', '572a2d163f37b3190047876e': 'Protestants', '572a2d163f37b3190047876f': 'Masurians, Kursenieki and Prussian Lithuanians', '5a3bf619cc5d22001a521c5a': '', '5a3bf619cc5d22001a521c5b': '', '5a3bf619cc5d22001a521c5c': '', '5a3bf619cc5d22001a521c5d': '', '5a3bf619cc5d22001a521c5e': '', '56e11d89e3433e1400422c22': '16', '56e11d89e3433e1400422c23': '14', '5acd4eab07355d001abf3cac': '', '5acd4eab07355d001abf3cad': '', '5acd4eab07355d001abf3cae': '', '5acd4eab07355d001abf3caf': '', '5acd4eab07355d001abf3cb0': '', '5722d357f6b826140030fc66': '50', '5722d357f6b826140030fc69': 'his parentage', '5723e1e80dadf01500fa1f6d': '1887', '5723e1e80dadf01500fa1f6f': 'Sir Henry', '5724e8960ba9f01400d97bbd': '1887', '5724e8960ba9f01400d97bbe': 'Hindustani', '5724e8960ba9f01400d97bc1': '20 June', '572635ccec44d21400f3dc3f': '1887', '572635ccec44d21400f3dc41': 'Abdul Karim', '5ad17cf2645df0001a2d1e12': '', '5ad17cf2645df0001a2d1e13': '', '5ad17cf2645df0001a2d1e14': '', '5ad17cf2645df0001a2d1e15': '', '57277778708984140094de56': 'Bible', '57277778708984140094de57': 'Mesopotamia and Egypt', '5a8c775dfd22b3001a8d8857': '', '5a8c775dfd22b3001a8d8858': '', '5a8c775dfd22b3001a8d8859': '', '56ce6629aab44d1400b8874d': '2050', '56d00697234ae51400d9c297': '2050', '56f7f695aef2371900625cfc': 'the highest members of the nobility', '56f7f695aef2371900625cfd': 'Russian Empire', '56f7f695aef2371900625cff': '\"Lithuanians are Russians seduced by Poles and Catholicism\"', '572802332ca10214002d9b51': 'slip-cueing, beatmatching, and scratching', '572f7b31b2c2fd140056817e': 'North America', '572f7b31b2c2fd140056817f': 'Europe', '572f7b31b2c2fd1400568180': 'The Jazz Age', '56e7921a37bdd419002c4171': 'the opening of the Suez Canal', '5ad3c98e604f3c001a3ff0a1': '', '5ad3c98e604f3c001a3ff0a2': '', '5ad3c98e604f3c001a3ff0a3': '', '56f9608b9b226e1400dd13df': '18', '56f9608b9b226e1400dd13e0': 'every four years', '56f9608b9b226e1400dd13e1': 'Nitijela', '56f9608b9b226e1400dd13e2': '1979', '570bd2ec6b8089140040fa6a': 'handshaking', '570bd2ec6b8089140040fa6b': 'Control-R (DC2) and Control-T (DC4)', '5a6513c9c2b11c001a425be1': '', '5a6513c9c2b11c001a425be2': '', '5a6513c9c2b11c001a425be3': '', '5a6513c9c2b11c001a425be4': '', '5a6513c9c2b11c001a425be5': '', '573039c004bcaa1900d773c7': 'cavalry troops', '5ad3d84a604f3c001a3ff377': '', '5ad3d84a604f3c001a3ff378': '', '572785c8dd62a815002e9f6d': 'sell and carry drugs, guns, and other illegal substances', '572842a0ff5b5019007da030': 'Zakaria Mohieddin', '572842a0ff5b5019007da033': 'diabetes', '5734465d879d6814001ca463': 'Hunter-gathering', '5734465d879d6814001ca465': 'paleolithic hunting-gathering', '5ace562032bba1001ae4a2ff': '', '5ace562032bba1001ae4a300': '', '5ace562032bba1001ae4a301': '', '5ace562032bba1001ae4a302': '', '5ace562032bba1001ae4a303': '', '5726275589a1e219009ac3ec': 'George Gaylord Simpson', '5726275589a1e219009ac3ee': 'cladistics', '5a39eb432f14dd001ac72644': '', '5a39eb432f14dd001ac72645': '', '5a39eb432f14dd001ac72646': '', '573192bca5e9cc1400cdc0e7': 'high cost of labor in developed countries', '573192bca5e9cc1400cdc0e9': '10 times faster', '56fc975cb53dbe1900755133': 'prosody', '5a82188631013a001a3351e6': '', '5a82188631013a001a3351e8': '', '5a82188631013a001a3351e9': '', '56fb91df8ddada1400cd64f8': 'customary law', '56fb91df8ddada1400cd64fa': 'Lithuania', '572eb63603f989190075697d': 'six', '5a848a6d7cf838001a46a8ea': '', '5a848a6d7cf838001a46a8eb': '', '5a848a6d7cf838001a46a8ec': '', '5a848a6d7cf838001a46a8ee': '', '5728cc17ff5b5019007da6e2': 'Bal Gangadhar Tilak', '5728cc17ff5b5019007da6e4': 'two factions', '572f7d6f04bcaa1900d76a1b': 'March–June', '572f7d6f04bcaa1900d76a1d': 'May', '56e0ccb3231d4119001ac3b6': 'September 2008', '56e0ccb3231d4119001ac3b7': 'Internet Explorer', '56e0ccb3231d4119001ac3b8': 'May 2012', '5a4d38437a6c4c001a2bbc5c': '', '5a4d38437a6c4c001a2bbc5d': '', '5a4d38437a6c4c001a2bbc5f': '', '5a4d38437a6c4c001a2bbc60': '', '5726e8eb708984140094d589': 'Aposematism', '5a6bdaf94eec6b001a80a5e8': '', '5a6bdaf94eec6b001a80a5ea': '', '5a6bdaf94eec6b001a80a5ec': '', '572f5d5eb2c2fd1400568081': '1930s', '572f5d5eb2c2fd1400568082': 'Adolf Hitler', '56e8dca40b45c0140094cd21': '1535', '56e8dca40b45c0140094cd23': 'Glastonbury Abbey', '5ad3f4c7604f3c001a3ff95a': '', '5ad3f4c7604f3c001a3ff95d': '', '573029cd947a6a140053d1f0': 'indicating the level of resource usage', '5ad496b5ba00c4001a268cb8': '', '5ad496b5ba00c4001a268cb9': '', '5ad496b5ba00c4001a268cba': '', '5ad496b5ba00c4001a268cbb': '', '5ad496b5ba00c4001a268cbc': '', '570e25b30dc6ce1900204dfc': 'pasta', '570e25b30dc6ce1900204dfd': 'honey', '570e25b30dc6ce1900204dff': 'pasta', '5ad0d8bf645df0001a2d06a6': '', '5ad0d8bf645df0001a2d06a8': '', '5ad0d8bf645df0001a2d06a9': '', '5ad0d8bf645df0001a2d06aa': '', '570624f252bb8914006898f6': 'ASPEC', '570624f252bb8914006898f8': 'Layer 2', '570624f252bb8914006898f9': 'MP3', '5727ff0c2ca10214002d9ae4': 'Central Asian Turks', '5727ff0c2ca10214002d9ae5': 'late 14th century', '5727ff0c2ca10214002d9ae6': 'Sikhism', '572820842ca10214002d9e7f': '6 September 2012', '5a7a7cfa21c2de001afe9c7e': '', '5a7a7cfa21c2de001afe9c7f': '', '5a7a7cfa21c2de001afe9c80': '', '5a7a7cfa21c2de001afe9c81': '', '5a7a7cfa21c2de001afe9c82': '', '56cfe7f4234ae51400d9c05f': '16 November 1848', '56d375b859d6e4140014646b': \"London's Guildhall\", '5731225ca5e9cc1400cdbc76': 'Beringia', '5731225ca5e9cc1400cdbc78': '13,500 years ago', '5731bfaae17f3d140042238e': 'controversy over the Eucharist', '5731bfaae17f3d1400422390': 'bread and wine', '5727ff7c2ca10214002d9b00': '30 November 1934', '5727ff7c2ca10214002d9b01': 'Mesembria, Bulgaria', '5727ff7c2ca10214002d9b03': '1935', '5727ff7c2ca10214002d9b04': 'Pope Pius XI', '5a60fc3ae9e1cc001a33cdfd': '', '5a60fc3ae9e1cc001a33ce00': '', '5726909f5951b619008f76c4': '53%', '5726909f5951b619008f76c5': '2013', '5726909f5951b619008f76c7': 'six years', '56f89a0c9e9bad19000a01af': '31 BC', '56f89a0c9e9bad19000a01b0': 'Aeneas', '56f89a0c9e9bad19000a01b1': 'Turnus', '5a7e576e70df9f001a87575d': '', '5a7e576e70df9f001a87575e': '', '5a7e576e70df9f001a875761': '', '57305b0b069b5314008320a8': 'reproducing the original order of sememes', '57305b0b069b5314008320ab': 'syntactic requirements', '5a7e138a70df9f001a87547f': '', '5a7e138a70df9f001a875480': '', '5a7e138a70df9f001a875481': '', '5a7e138a70df9f001a875482': '', '5ad3586b604f3c001a3fde26': '', '5ad3586b604f3c001a3fde27': '', '5ad3586b604f3c001a3fde28': '', '5ad3586b604f3c001a3fde29': '', '5a6bb3444eec6b001a80a4e8': '', '5a6bb3444eec6b001a80a4e9': '', '5a6bb3444eec6b001a80a4eb': '', '5726f9d3f1498d1400e8f192': 'the Soviet Union', '5726f9d3f1498d1400e8f194': 'atomic warfare', '5726f9d3f1498d1400e8f195': 'dropping their code signals and speaking over the wireless in Russian', '5726f9d3f1498d1400e8f196': 'Soviet Union', '572792f3f1498d1400e8fc90': 'Steve Earle', '5a826083e60761001a2eb21f': '', '5a826083e60761001a2eb220': '', '5a826083e60761001a2eb221': '', '5acf5f2877cf76001a684ca4': '', '5acf5f2877cf76001a684ca5': '', '5acf5f2877cf76001a684ca7': '', '5acf5f2877cf76001a684ca8': '', '56ce4100aab44d1400b88612': 'Tümen Khan', '56ce4100aab44d1400b88613': 'the great-grandson of Altan Khan', '56ce4100aab44d1400b88614': 'the 5th Dalai Lama', '57268ac8dd62a815002e88d9': '$390 billion', '57268ac8dd62a815002e88dc': '2020', '5734009a4776f41900661691': 'Punjabi', '5734009a4776f41900661693': 'Punjabis', '5734009a4776f41900661694': '89%', '5734009a4776f41900661695': 'south Punjab', '5a68fabf8476ee001a58a964': '', '5a68fabf8476ee001a58a966': '', '5a68fabf8476ee001a58a967': '', '5a68fabf8476ee001a58a968': '', '5727d9b5ff5b5019007d96d5': 'his study of Dionysus the Areopagite', '5727d9b5ff5b5019007d96d7': 'Mysticism', '5727d9b5ff5b5019007d96d8': 'German mysticism', '5ad25af2d7d075001a428e5c': '', '5ad25af2d7d075001a428e5d': '', '5ad25af2d7d075001a428e5e': '', '5ad25af2d7d075001a428e5f': '', '5ad25af2d7d075001a428e60': '', '5728c7763acd2414000dfe35': 'cattle', '5728c7763acd2414000dfe36': '6000 BCE', '5728c7763acd2414000dfe37': 'Metal objects', '5728c7763acd2414000dfe38': 'due west', '5a611ac3e9e1cc001a33cf1d': '', '5a611ac3e9e1cc001a33cf1e': '', '5a611ac3e9e1cc001a33cf1f': '', '57344599acc1501500babd66': 'hunting', '57344599acc1501500babd67': 'dance and animal sacrifice', '5735e8246c16ec1900b92883': 'stories and myths', '5735e8246c16ec1900b92884': 'hunting hypothesis', '5ace521a32bba1001ae4a29f': '', '5ace521a32bba1001ae4a2a0': '', '5ace521a32bba1001ae4a2a1': '', '572b7afb34ae481900deae3d': 'apeirotheism', '572b7afb34ae481900deae3e': 'pluralistic', '5a7ca6d9e8bc7e001a9e1f47': '', '5a7ca6d9e8bc7e001a9e1f48': '', '5a7ca6d9e8bc7e001a9e1f4a': '', '5a7ca6d9e8bc7e001a9e1f4b': '', '56e1586ee3433e1400422de8': 'Big Five', '56e1586ee3433e1400422de9': 'Gramophone', '56e1586ee3433e1400422dea': 'Symphony Hall', '56e1586ee3433e1400422deb': 'west of Back Bay', '56e1586ee3433e1400422dec': 'south of Boston Common', '572fa925947a6a140053cb1e': 'Tollywood', '572fa925947a6a140053cb1f': 'second largest', '5706a52352bb891400689b10': 'National Rail Museum', '5706a52352bb891400689b11': \"Shankar's International Dolls Museum\", '5706a52352bb891400689b12': 'New Delhi', '5706a52352bb891400689b14': 'New Delhi', '572a34b83f37b319004787a9': 'sickle blades and grinding stones', '5a7d3e5c70df9f001a87503b': '', '5a7d3e5c70df9f001a87503c': '', '5727c7ad3acd2414000dec37': '$9 billion', '5727c7ad3acd2414000dec39': '$5.2 billion', '5727c7ad3acd2414000dec3a': '13', '5731c971e17f3d14004223e1': 'Huguenots', '5731c971e17f3d14004223e2': 'the obduracy and the complacency of the Catholic establishment', '5731c971e17f3d14004223e5': 'August 1572', '57329efbcc179a14009dab7c': '15,000', '57329efbcc179a14009dab7d': '1942', '57329efbcc179a14009dab7e': '19 February', '57329efbcc179a14009dab7f': 'Darwin', '57264e34f1498d1400e8db97': 'Tungsten', '5ad1acf3645df0001a2d21b4': '', '5ad1acf3645df0001a2d21b5': '', '5ad1acf3645df0001a2d21b6': '', '5ad1acf3645df0001a2d21b7': '', '570acf964103511400d59a26': 'Ichthyosaurs', '570acf964103511400d59a27': 'Mosasaurs', '570acf964103511400d59a28': 'Iguanodon', '5a2f4b02a83784001a7d26d9': '', '5a2f4b02a83784001a7d26da': '', '5a2f4b02a83784001a7d26db': '', '5a2f4b02a83784001a7d26dc': '', '5a2f4b02a83784001a7d26dd': '', '572c99202babe914003c299c': 'Republican and the Democratic', '572c99202babe914003c299e': 'yeoman', '572c99202babe914003c299f': 'tobacco and cotton', '56f7d6d8aef2371900625c2b': 'obscure', '56f7d6d8aef2371900625c2c': 'odwieczna', '56f7d6d8aef2371900625c2d': 'descent from the ancient Iranian tribes known as Sarmatians', '56e788ab37bdd419002c40d3': 'February 25, 2010', '56e788ab37bdd419002c40d4': 'over 30 years', '5a7b6b7321c2de001afe9ffa': '', '5a7b6b7321c2de001afe9ffb': '', '5a7b6b7321c2de001afe9ffc': '', '5a7b6b7321c2de001afe9ffd': '', '5ad3fd89604f3c001a3ffbeb': '', '5ad3fd89604f3c001a3ffbec': '', '5ad3fd89604f3c001a3ffbed': '', '5ad3fd89604f3c001a3ffbee': '', '5ad3fd89604f3c001a3ffbef': '', '57304d34069b531400832023': '150 years', '57304e4b069b531400832038': 'Bishop England High School', '57304e4b069b531400832039': 'The Roman Catholic Diocese of Charleston Office of Education', '57304e4b069b53140083203a': 'Ashley Hall', '5ad42525604f3c001a4008d6': '', '5ad42525604f3c001a4008d7': '', '5ad42525604f3c001a4008d8': '', '57293b691d0469140077918d': '$244 billion', '5ad123aa645df0001a2d0ee8': '', '5ad123aa645df0001a2d0eea': '', '5ad123aa645df0001a2d0eeb': '', '5ad123aa645df0001a2d0eec': '', '56e1223ecd28a01900c67631': 'Boston Brahmins', '56e1223ecd28a01900c67632': 'Boston Brahmins', '56e1223ecd28a01900c67634': 'artistic patronage', '57266080f1498d1400e8ddac': 'the European continent', '57266080f1498d1400e8ddad': '12.7 million', '57266080f1498d1400e8ddae': 'United Kingdom', '57266080f1498d1400e8ddaf': 'Central Macedonia', '57266080f1498d1400e8ddb0': '6.5 million', '56de24b24396321400ee25fe': 'Thomas Hobbes', '56de24b24396321400ee25ff': 'Montesquieu', '56de24b24396321400ee2600': 'the framers of the United States Constitution', '56de33fc4396321400ee2695': 'Thomas Hobbes', '56de33fc4396321400ee2696': 'Montesquieu', '5ad37aaa604f3c001a3fe3ce': '', '5ad37aaa604f3c001a3fe3d0': '', '56ddd3909a695914005b95fc': 'The Geophysics Institute', '56cd7d3262d2951400fa6633': 'refurbished replacement iPod', '56cd7d3262d2951400fa6634': 'lithium-ion', '56d131c817492d1400aabbe1': 'lithium-ion', '56d2706859d6e41400145fdd': 'Lokakṣema', '56d2706859d6e41400145fde': 'Prajñāpāramitā', '5a45597219a820001a1eda2b': '', '5a45597219a820001a1eda2c': '', '5a45597219a820001a1eda2d': '', '5a45597219a820001a1eda2e': '', '573056f1069b531400832085': 'China Central Television', '573056f1069b531400832086': 'Sina Weibo', '573056f1069b531400832087': 'Ni Guangnan', '573056f1069b531400832088': 'Fudan University', '5ad4a204ba00c4001a268e88': '', '5ad4a204ba00c4001a268e89': '', '5ad4a204ba00c4001a268e8a': '', '5ad4a204ba00c4001a268e8b': '', '5ad4a204ba00c4001a268e8c': '', '5acfc11177cf76001a685cda': '', '5acfc11177cf76001a685cdb': '', '5acfc11177cf76001a685cdc': '', '56e02cb57aa994140058e2fb': 'Portuguese India carracks', '56e02cb57aa994140058e2fc': 'the Dutch', '56e02cb57aa994140058e2fd': 'The Portuguese and Spanish', '572a7a17be1ee31400cb802b': '2003', '572a7a17be1ee31400cb802c': 'Brickell Avenue', '572a7a17be1ee31400cb802d': 'South America', '5ad3903d604f3c001a3fe64c': '', '5ad3903d604f3c001a3fe64d': '', '572fe936b2c2fd14005685bf': 'resonance principle', '5727b0563acd2414000de9cb': 'Tang dynasty', '5727b0563acd2414000de9cc': 'Du Fu', '5727b0563acd2414000de9cd': 'armies', '5727b0563acd2414000de9ce': '755-763', '5a512d7dce860b001aa3fbed': '', '5a512d7dce860b001aa3fbee': '', '5a512d7dce860b001aa3fbef': '', '5a512d7dce860b001aa3fbf1': '', '5a68e9e48476ee001a58a880': '', '5a68e9e48476ee001a58a881': '', '57317177a5e9cc1400cdbf6d': 'Senussi', '57317177a5e9cc1400cdbf6f': 'none', '57317177a5e9cc1400cdbf70': 'execution', '57264c62dd62a815002e80d2': 'fish', '57264c62dd62a815002e80d3': 'Rochester, England', '5728b6bbff5b5019007da53e': 'northern part', '5728b6bbff5b5019007da53f': 'near-equal length', '5728b6bbff5b5019007da541': 'July', '56cf6057aab44d1400b89177': 'local artists', '56cf6057aab44d1400b8917a': 'Go-Getters', '56d0fe7217492d1400aab705': 'mid-1990s', '56d45f882ccc5a1400d830ef': 'mid-1990s', '56d45f882ccc5a1400d830f0': 'Deric \"D-Dot\" Angelettie', '56d45f882ccc5a1400d830f1': 'Go-Getters', '56d45f882ccc5a1400d830f2': 'Hustle Period', '56d45f882ccc5a1400d830f3': '1999', '56dd37fe66d3e219004dac79': \"Her Majesty's Most Honourable Privy Council\", '5acfc9ba77cf76001a685fb6': '', '5acfc9ba77cf76001a685fb8': '', '5728171b4b864d1900164452': '128', '5728171b4b864d1900164453': 'conjuncts', '5acd579107355d001abf3dec': '', '5acd579107355d001abf3ded': '', '5acd579107355d001abf3dee': '', '5acd579107355d001abf3df0': '', '56fdee67761e401900d28c59': 'machine language', '57288d3b3acd2414000dfae7': 'Magnetic Lasso', '5acea46832bba1001ae4aead': '', '56fdfa03761e401900d28c83': 'registers', '5731320605b4da19006bce89': 'Vladimir the Great', '5731320605b4da19006bce8a': '1019', '5ad0124077cf76001a6868ae': '', '5ad0124077cf76001a6868af': '', '5ad0124077cf76001a6868b1': '', '5727fb4eff5b5019007d99e2': 'communist', '5727fb4eff5b5019007d99e3': 'nine', '5727fb4eff5b5019007d99e4': 'Muslim Brotherhood', '5a770f312d6d7f001a4a9f1d': '', '5a770f312d6d7f001a4a9f1e': '', '5a770f312d6d7f001a4a9f1f': '', '5a770f312d6d7f001a4a9f20': '', '5a770f312d6d7f001a4a9f21': '', '570c5037b3d812140066d0bd': 'Isabella of Angoulême', '570c5037b3d812140066d0be': 'downright mean', '570c5037b3d812140066d0bf': '1207 and 1215', '5acd14cb07355d001abf33c0': '', '5acd14cb07355d001abf33c1': '', '5acd14cb07355d001abf33c2': '', '5acd14cb07355d001abf33c3': '', '5acd14cb07355d001abf33c4': '', '5727011b708984140094d844': 'local commanders', '5727011b708984140094d845': 'Lord Raglan', '5727011b708984140094d846': 'The local commanders', '56e4cfd839bdeb14003479de': 'New Urbanism, Metaphoric architecture and New Classical Architecture', '5acfb2be77cf76001a68595c': '', '5acfb2be77cf76001a68595d': '', '5acfb2be77cf76001a68595e': '', '5acfb2be77cf76001a68595f': '', '57301a30b2c2fd140056886d': 'thousands', '57301a30b2c2fd140056886f': 'Oro Valley', '57301a30b2c2fd1400568870': 'Marana', '573428b44776f419006619cd': 'northwest', '572823804b864d190016454c': \"St. John's\", '572823804b864d190016454d': 'Signal Hill', '572823804b864d190016454e': '1897', '572823804b864d1900164550': 'Guglielmo Marconi', '5a6280f6f8d794001af1c070': '', '5a6280f6f8d794001af1c071': '', '5a6280f6f8d794001af1c072': '', '5a6280f6f8d794001af1c073': '', '56dd260266d3e219004dac12': 'Glorious Revolution', '56dd260266d3e219004dac13': 'Bill of Rights', '56dd260266d3e219004dac14': 'House of Commons', '5acfa94777cf76001a685794': '', '5acfa94777cf76001a685796': '', '5acfa94777cf76001a685797': '', '56f72fe03d8e2e1400e37402': 'complexity', '572b6f49111d821400f38e9c': 'subjectivists', '572b6f49111d821400f38e9e': '1954', '572b6f49111d821400f38e9f': 'Foster', '5a7c7516e8bc7e001a9e1e23': '', '5a7c7516e8bc7e001a9e1e24': '', '5a7c7516e8bc7e001a9e1e26': '', '5a7c7516e8bc7e001a9e1e27': '', '57268c78708984140094c9bd': 'Max Clifford', '57268c78708984140094c9bf': 'Unwrapped', '56e0b2127aa994140058e6ad': 'Hydrogen', '56e0b2127aa994140058e6ae': 'deuterium and tritium', '56e0b2127aa994140058e6af': 'D and T', '56e0b2127aa994140058e6b1': '2H and 3H', '56dfc0ae231d4119001abd95': 'upstream ISPs', '5a10dee906e79900185c343c': '', '572ed3f503f9891900756a68': 'Sahih al-Bukhari', '572ed3f503f9891900756a6a': 'Mary', '572ed3f503f9891900756a6b': 'common divine source', '5ad23431d7d075001a4287c4': '', '5ad23431d7d075001a4287c5': '', '5ad23431d7d075001a4287c6': '', '5ad23431d7d075001a4287c7': '', '5ad23431d7d075001a4287c8': '', '56f739203d8e2e1400e3749a': 'international law', '56f739203d8e2e1400e3749b': 'North Korea and the United States', '56f739203d8e2e1400e3749c': 'security guarantees and nuclear proliferation', '56d632371c85041400946fe0': 'less severe', '56d632371c85041400946fe1': '12.9', '56d632371c85041400946fe2': '60.7', '56d9e7e4dc89441400fdb902': 'Colorado', '56d9e7e4dc89441400fdb903': '60.7', '572f068ccb0c0d14000f1719': 'Safety Code for Elevators and Escalators', '572f068ccb0c0d14000f171a': 'CAN/CSA B44 Safety Standard', '572f068ccb0c0d14000f171b': 'ASME A17.2 Standard', '56ce6382aab44d1400b88732': '1872', '56d0007f234ae51400d9c243': 'Solar distillation', '56d0007f234ae51400d9c244': '16th-century Arab alchemists', '56d0007f234ae51400d9c245': '1872', '5acda46b07355d001abf48a3': '', '5acda46b07355d001abf48a4': '', '5acda46b07355d001abf48a5': '', '5acda46b07355d001abf48a6': '', '5733b2e14776f41900661085': 'Archaeology', '5733b2e14776f41900661086': 'cultural and material lives of past societies', '5ad2d6a8d7d075001a42a420': '', '5ad2d6a8d7d075001a42a421': '', '5ad2d6a8d7d075001a42a422': '', '57318ae9e6313a140071d07e': 'head of state security', '57318ae9e6313a140071d080': 'Istanbul', '57318ae9e6313a140071d081': 'Saif al-Islam', '57318ae9e6313a140071d082': 'Abdullah Senussi', '572fb5b0b2c2fd1400568395': 'endospores', '572fb5b0b2c2fd1400568398': 'Dipicolinic acid', '57278133dd62a815002e9ee8': 'late 19th century', '572a7b02111d821400f38b54': 'Miami', '572a7b02111d821400f38b55': '2001', '5ad39216604f3c001a3fe68f': '', '5ad39216604f3c001a3fe690': '', '5ad39216604f3c001a3fe691': '', '5ad39216604f3c001a3fe692': '', '572f8622947a6a140053ca18': '1941', '572f8622947a6a140053ca1a': 'Adolf Hitler', '570c27686b8089140040fb9a': '1950s and 1960s', '570c27686b8089140040fb9c': 'Dr. T.R.M. Howard', '570c27686b8089140040fb9e': 'COINTELPRO', '5ad37d89604f3c001a3fe417': '', '5ad37d89604f3c001a3fe41a': '', '5ad37d89604f3c001a3fe41b': '', '57096fa9ed30961900e84120': 'any behavior of one animal that affects the current or future behavior of another animal', '57096fa9ed30961900e84121': 'zoo semiotics', '57096fa9ed30961900e84124': 'vibrational communication', '59fc23cca9fb160018f10da5': '', '5ad3e20e604f3c001a3ff521': '', '5ad3e20e604f3c001a3ff522': '', '5ad3e20e604f3c001a3ff525': '', '5706158575f01819005e7960': 'The Wombles', '5706158575f01819005e7961': 'Very Behind the Times', '5706158575f01819005e7962': 'Uncle Bulgaria', '57324359b9d445190005e945': 'Bible', '57324359b9d445190005e946': 'River Brethren', '57324359b9d445190005e947': \"Jehovah's Witnesses\", '57324359b9d445190005e948': 'West Point', '573106b7497a881900248b08': 'France', '573106b7497a881900248b09': 'British Empire', '573106b7497a881900248b0b': 'Otto von Bismarck', '5a149f88a54d4200185292b4': '', '5731785b497a881900248f39': '62%', '5731785b497a881900248f3a': '3.7%', '5731785b497a881900248f3b': '66.4%', '5731785b497a881900248f3c': '36', '5731785b497a881900248f3d': 'fifty', '572a23643f37b31900478728': 'Formative stage', '5a7d1cfd70df9f001a874fa9': '', '5a7d1cfd70df9f001a874faa': '', '5a7d1cfd70df9f001a874fab': '', '5a7d1cfd70df9f001a874fac': '', '570b6593ec8fbc190045b9d2': '1991', '570b6593ec8fbc190045b9d3': 'The Black Crowes', '570b6593ec8fbc190045b9d5': 'Hysteria', '570b6593ec8fbc190045b9d6': 'five weeks', '5a5a45ab9c0277001abe70e8': '', '5a5a45ab9c0277001abe70ea': '', '5a5a45ab9c0277001abe70eb': '', '5a5a45ab9c0277001abe70ec': '', '572849ebff5b5019007da0fe': 'to avoid breaching sovereignty through military invasion', '572849ebff5b5019007da0ff': 'Global War on Terror', '5a85e80ab4e223001a8e72c1': '', '5a85e80ab4e223001a8e72c3': '', '5a85e80ab4e223001a8e72c4': '', '5a85e80ab4e223001a8e72c5': '', '56cd8ffa62d2951400fa6720': 'nods and facial expressions', '56cd8ffa62d2951400fa6721': 'Midna', '56cd8ffa62d2951400fa6722': 'Akiko Kōmoto', '56d128ed17492d1400aabad9': 'grunts', '56d128ed17492d1400aabada': 'nods and facial expressions', '56d128ed17492d1400aabadb': 'Midna', '56d128ed17492d1400aabadc': 'Akiko Kōmoto', '5a8d914edf8bba001a0f9b0c': '', '5a8d914edf8bba001a0f9b0d': '', '5a8d914edf8bba001a0f9b0e': '', '57096446ed30961900e8406a': '21st', '57096446ed30961900e8406b': 'Tripura', '57096446ed30961900e8406c': 'Kangra district', '5a3625b0788daf001a5f875c': '', '5a3625b0788daf001a5f875d': '', '5727ae3e2ca10214002d9380': 'Upanishads and Brahma', '5727ae3e2ca10214002d9381': 'first millennium BCE', '5a5e5a285bc9f4001a75af1d': '', '5a5e5a285bc9f4001a75af1e': '', '5a5e5a285bc9f4001a75af1f': '', '5a5e5a285bc9f4001a75af20': '', '5a5e5a285bc9f4001a75af21': '', '57315bfaa5e9cc1400cdbf02': 'October 20, 1789', '57315bfaa5e9cc1400cdbf03': 'July 17, 1791', '57315bfaa5e9cc1400cdbf04': 'Rouget de Lisle', '57315bfaa5e9cc1400cdbf05': '1790', '5ad4fc2f5b96ef001a10a88c': '', '5ad4fc2f5b96ef001a10a88d': '', '5ad4fc2f5b96ef001a10a88e': '', '5ad4fc2f5b96ef001a10a890': '', '572fffeb947a6a140053cf38': 'Teflon', '572fffeb947a6a140053cf39': 'phenolic cotton paper', '572fffeb947a6a140053cf3b': 'glass fiber', '5ace942432bba1001ae4aacb': '', '5ace942432bba1001ae4aacc': '', '5ace942432bba1001ae4aacd': '', '573035c8b2c2fd1400568a7c': 'San Diego Regional Airport Authority', '573035c8b2c2fd1400568a7e': '17 million', '573035c8b2c2fd1400568a7f': 'Montgomery Field (MYF) and Brown Field (SDM)', '5ad4db545b96ef001a10a420': '', '5ad4db545b96ef001a10a421': '', '5ad4db545b96ef001a10a422': '', '5ad4db545b96ef001a10a423': '', '5ad4db545b96ef001a10a424': '', '572f83ae947a6a140053c9fb': '1929', '572f83ae947a6a140053c9fc': 'a worldwide economic downturn', '572efd66cb0c0d14000f16cd': 'Berlin', '572efd66cb0c0d14000f16ce': 'press the buttons', '572efd66cb0c0d14000f16cf': 'Lifted: A Cultural History of the Elevator', '57109aada58dae1900cd6ac0': '1730s', '57109aada58dae1900cd6ac1': 'British', '57109aada58dae1900cd6ac4': 'Grand Architect', '5730c888aca1c71400fe5ab8': 'US$200 per unit', '5ad18fa2645df0001a2d1f62': '', '5ad18fa2645df0001a2d1f63': '', '5ad18fa2645df0001a2d1f64': '', '5ad18fa2645df0001a2d1f65': '', '5ad18fa2645df0001a2d1f66': '', '57282ec23acd2414000df67c': 'Rukh', '57282ec23acd2414000df67e': '1946', '5acd1ab607355d001abf3538': '', '5acd1ab607355d001abf3539': '', '5acd1ab607355d001abf353a': '', '5acd1ab607355d001abf353b': '', '5acd1ab607355d001abf353c': '', '572848e94b864d19001648c5': 'DVDs', '572a3b486aef0514001553b9': 'Gustav Edler von Hayek', '56cf694f4df3c31400b0d750': 'Alexis Phifer', '56cf694f4df3c31400b0d751': 'Auto-Tune', '56d109f317492d1400aab7ca': '2007', '56d109f317492d1400aab7cb': 'Alexis Phifer', '56d109f317492d1400aab7cc': 'Glow in the Dark Tour', '5725be0738643c19005acc3f': 'Flemish', '5725be0738643c19005acc40': 'Nederlands', '5725be0738643c19005acc43': 'Western Flemish', '572f8892b2c2fd14005681c5': 'London', '572f8892b2c2fd14005681c6': 'Battle of Britain Day', '572f8892b2c2fd14005681c7': 'bomb-load limitations', '572f8892b2c2fd14005681c8': 'four-engined bombers', '56e08070231d4119001ac1ff': '+290', '56e08070231d4119001ac200': 'Tristan da Cunha', '56e08070231d4119001ac202': '1 October 2013', '56e08070231d4119001ac203': '2', '56fe01af19033b140034ce31': 'a computer', '5730878f2461fd1900a9ce91': 'Ladoga and Karelia regions', '5acff7c077cf76001a68669f': '', '5acff7c077cf76001a6866a1': '', '5acff7c077cf76001a6866a2': '', '56cf4b72aab44d1400b88f75': 'Bob Ewell', '572fc229b2c2fd1400568406': 'attack him and his abilities', '571de3ebb64a571400c71dd8': 'slaves', '571de3ebb64a571400c71dd9': 'Sally Hemings', '571de3ebb64a571400c71dda': 'paternity', '5ad2b36bd7d075001a429f90': '', '5ad2b36bd7d075001a429f93': '', '5ad2b36bd7d075001a429f94': '', '572759cb5951b619008f888e': '1890s', '572759cb5951b619008f888f': 'chemicals industry', '572759cb5951b619008f8891': 'DuPont', '5a8192e331013a001a334cce': '', '5a8192e331013a001a334ccf': '', '5a8192e331013a001a334cd0': '', '5a8192e331013a001a334cd1': '', '5732a3dfcc179a14009dabc5': 'Mesozoic', '5a4eb85eaf0d07001ae8cbf0': '', '5a4eb85eaf0d07001ae8cbf1': '', '5a4eb85eaf0d07001ae8cbf2': '', '5a4eb85eaf0d07001ae8cbf3': '', '56d8dc9cdc89441400fdb350': 'April 3', '56d8dc9cdc89441400fdb352': 'Sultanahmet Square', '56db0a87e7c41114004b4cb1': 'Istanbul', '56db0a87e7c41114004b4cb2': 'Sultanahmet Square', '56db0a87e7c41114004b4cb4': 'Uyghurs', '56db0a87e7c41114004b4cb5': 'arrested', '571a993510f8ca140030518d': 'Middle Eastern Jews and European/Syrian Jews', '56e168ebe3433e1400422ec4': '80%', '56e168ebe3433e1400422ec5': 'Seagram', '56e168ebe3433e1400422ec6': '$5.7 billion', '56e168ebe3433e1400422ec8': 'PolyGram', '5ad15e76645df0001a2d18e4': '', '56d0051e234ae51400d9c26f': 'to treat waste water without chemicals or electricity', '56d0051e234ae51400d9c270': 'algae may produce toxic chemicals', '56ce451caab44d1400b8863c': 'Master of Vajradhara', '56ce451caab44d1400b8863d': 'Yonten Gyatso', '56ce451caab44d1400b8863e': '1616', '56ce451caab44d1400b8863f': 'Yonten Gyatso', '5acec97b32bba1001ae4b407': '', '5acec97b32bba1001ae4b408': '', '5acec97b32bba1001ae4b409': '', '56defb12c65bf219000b3e75': 'eleven', '5ad3eb5a604f3c001a3ff713': '', '5ad3eb5a604f3c001a3ff714': '', '5ad3eb5a604f3c001a3ff715': '', '5ad3eb5a604f3c001a3ff716': '', '5ad28515d7d075001a429888': '', '5ad28515d7d075001a429889': '', '5ad28515d7d075001a42988a': '', '5ad28515d7d075001a42988b': '', '5ad28515d7d075001a42988c': '', '56cbd2356d243a140015ed68': 'solo piano', '56cbd2356d243a140015ed6a': '20', '56ce0a3762d2951400fa69d6': '1810', '56ce0a3762d2951400fa69d8': 'Warsaw', '56ce0a3762d2951400fa69d9': 'solo piano', '56ce0a3762d2951400fa69da': '20', '56cf54a2aab44d1400b89006': '17 October 1849', '56cf54a2aab44d1400b89009': 'solo piano', '56d1ca30e7d4791d009021a7': '1810', '56d1ca30e7d4791d009021a9': '20', '56d1ca30e7d4791d009021aa': 'Romantic', '56d1ca30e7d4791d009021ab': '1849', '5727a8874b864d19001639b8': 'seven', '5727a8874b864d19001639b9': 'free movement of persons', '5727a8874b864d19001639ba': '2004', '5733fa9a4776f41900661626': '2011', '5733fa9a4776f41900661627': '€78 billion', '5733fa9a4776f41900661628': 'May 2014', '5733fa9a4776f41900661629': '15.3 percent', '570a5e0e6d058f1900182dbd': 'Faculty of Medicine', '570a5e0e6d058f1900182dbe': 'Alice Gast', '5a4875a284b8a4001a7e788e': '', '5a4875a284b8a4001a7e788f': '', '5a4875a284b8a4001a7e7891': '', '56df631296943c1400a5d4a9': 'Atlantic depressions', '56df631296943c1400a5d4aa': 'autumn', '56df631296943c1400a5d4ac': 'November', '56df631296943c1400a5d4ad': 'south-west', '572e7f0adfa6aa1500f8d048': 'Mycenaean Greek traders', '572e7f0adfa6aa1500f8d04a': 'Aphrodite and Adonis', '57285eda3acd2414000df970': 'it makes those aspects more familiar', '57285eda3acd2414000df972': 'The construction of gods and spirits like persons', '572c0480dfb02c14005c6b5f': 'Pascal Boyer', '572c0480dfb02c14005c6b61': 'Greek mythology', '572c0480dfb02c14005c6b62': 'Sigmund Freud', '5a3c823dcc5d22001a521dec': '', '5a3c823dcc5d22001a521ded': '', '5a3c823dcc5d22001a521dee': '', '5a3c823dcc5d22001a521def': '', '5a3c823dcc5d22001a521df0': '', '5728e29f3acd2414000e0117': 'Dionysus', '5728e29f3acd2414000e0118': 'Apollo', '5728e29f3acd2414000e0119': 'Hyperborea', '570b2117ec8fbc190045b842': 'Sweden', '570b2117ec8fbc190045b845': 'United States', '5a1f6f6654a786001a36b2bd': '', '5a1f6f6654a786001a36b2be': '', '5a1f6f6654a786001a36b2bf': '', '5a1f6f6654a786001a36b2c0': '', '5a1f6f6654a786001a36b2c1': '', '5705fcd775f01819005e783a': 'Australian', '5705fcd775f01819005e783b': 'Rupert Murdoch', '56e180f5e3433e1400422f99': '90% to 95%', '56e180f5e3433e1400422f9a': 'Alguerese', '5a4d40ce7a6c4c001a2bbc85': '', '5a4d40ce7a6c4c001a2bbc86': '', '5a4d40ce7a6c4c001a2bbc87': '', '5a4d40ce7a6c4c001a2bbc88': '', '5726ee155951b619008f82ad': 'prey', '5726ee155951b619008f82af': 'survival and fecundity', '5a6be2214eec6b001a80a5f2': '', '5a6be2214eec6b001a80a5f3': '', '5a6be2214eec6b001a80a5f6': '', '5728d8e3ff5b5019007da80e': 'Wayne State University', '5728d8e3ff5b5019007da810': 'Downtown', '56f8360ca6d7ea1400e174ab': 'Karađorđevo', '56f8360ca6d7ea1400e174ac': 'Galeb', '56f8360ca6d7ea1400e174ad': 'Aviogenex', '56f8360ca6d7ea1400e174ae': 'Montenegro', '56f8360ca6d7ea1400e174af': 'seagull', '5706969952bb891400689ab4': 'Synthesizing: Ten Ragas to a Disco Beat', '5706969952bb891400689ab6': '1982', '5ad269fad7d075001a4292fe': '', '5ad269fad7d075001a429300': '', '572b8fff111d821400f38f1a': '1914', '572b8fff111d821400f38f1c': 'Texas, Nebraska and Wisconsin', '572b8fff111d821400f38f1d': 'Spanish', '572b8fff111d821400f38f1e': '70,500', '5a7a198217ab25001a8a032a': '', '5a7a198217ab25001a8a032b': '', '5a7a198217ab25001a8a032c': '', '5a7a198217ab25001a8a032d': '', '5a7a198217ab25001a8a032e': '', '572649865951b619008f6f1d': 'Andreas Papandreou', '572649865951b619008f6f1e': 'Panhellenic Socialist Movement', '572649865951b619008f6f1f': '1980', '572649865951b619008f6f21': '1999', '5ad13a0b645df0001a2d12a8': '', '5ad13a0b645df0001a2d12a9': '', '57266a17708984140094c532': 'the Constitution', '57266a17708984140094c533': 'federal constitutional rights', '57266a17708984140094c534': 'state law', '5726d83d708984140094d332': 'universal', '5726d83d708984140094d333': 'dual-sovereign system', '5726d83d708984140094d334': 'Indian reservations', '5726d83d708984140094d335': 'the federal Constitution', '572c9ab3dfb02c14005c6bac': '50', '572c9ab3dfb02c14005c6baf': 'state law', '5a79baa717ab25001a8a0002': '', '5a79baa717ab25001a8a0004': '', '5a79baa717ab25001a8a0005': '', '5728b8862ca10214002da65c': 'monopolized', '57283abb4b864d19001647ae': 'Leonid Kravchuk', '57283abb4b864d19001647af': 'Chernobyl Nuclear Power Plant', '57283abb4b864d19001647b2': '40,000', '57296cccaf94a219006aa3df': 'Germany', '57296cccaf94a219006aa3e0': 'United States', '57296cccaf94a219006aa3e1': 'coal', '5ad13603645df0001a2d1188': '', '5ad13603645df0001a2d1189': '', '5ad13603645df0001a2d118a': '', '5ad13603645df0001a2d118b': '', '57342bbc4776f419006619f2': 'Carlos Damas, Gerardo Ribeiro', '57342bbc4776f419006619f4': \"Nuno Malo and Miguel d'Oliveira\", '5726d331f1498d1400e8ec6f': 'Royal Niger Company', '5726d331f1498d1400e8ec70': '1900', '5726d331f1498d1400e8ec71': '1 January 1901', '5726d331f1498d1400e8ec72': 'Benin', '570b09a96b8089140040f703': \"carrying a heavy cruiser's complement of defensive weapons and large P-700 Granit offensive missiles\", '5acd884007355d001abf4605': '', '5acd884007355d001abf4606': '', '5acd884007355d001abf4607': '', '5709828fed30961900e84244': '3rd–2nd century BC', '5709828fed30961900e84245': 'Native Americans', '5709828fed30961900e84246': 'Isle Royale', '5709828fed30961900e84247': 'Peru', '5709828fed30961900e84248': 'early 20th century', '5a836aeee60761001a2eb68b': '', '5a836aeee60761001a2eb68c': '', '5a836aeee60761001a2eb68d': '', '5a836aeee60761001a2eb68e': '', '56e163afe3433e1400422e64': 'Boston Patriots', '570d410afed7b91900d45db3': 'solid fuel rockets', '570d410afed7b91900d45db5': 'unrotated projectiles', '570d410afed7b91900d45db6': '2-inch', '570d410afed7b91900d45db7': '3-inch', '56cf6aa44df3c31400b0d75f': 'Matthew Trammell', '56d10ab617492d1400aab7ee': 'Matthew Trammell', '56f8d3179b226e1400dd1097': 'France', '57303e5ea23a5019007fcffd': 'Backup and Restore', '5ad49c1fba00c4001a268d90': '', '5ad49c1fba00c4001a268d91': '', '5ad49c1fba00c4001a268d92': '', '5ad49c1fba00c4001a268d93': '', '5ad49c1fba00c4001a268d94': '', '5726adf0dd62a815002e8cc4': 'cathedrals and great churches', '5ad0ddaa645df0001a2d0716': '', '5ad0ddaa645df0001a2d0717': '', '5ad0ddaa645df0001a2d0718': '', '5ad0ddaa645df0001a2d0719': '', '56df778a5ca0a614008f9adb': '33', '56df778a5ca0a614008f9adc': 'National Geographic Society', '56df778a5ca0a614008f9ade': 'hydrofoils', '5731dd950fdd8d15006c65b0': 'English and Spanish', '5731dd950fdd8d15006c65b1': 'six', '5731dd950fdd8d15006c65b2': 'August 2016', '5731dd950fdd8d15006c65b3': 'British', '5a2eeb0aa83784001a7d2569': '', '5a2eeb0aa83784001a7d256a': '', '5a2eeb0aa83784001a7d256c': '', '5a2eeb0aa83784001a7d256d': '', '56f8cf869b226e1400dd1051': 'two', '56f8cf869b226e1400dd1052': 'Southampton City College', '56f8cf869b226e1400dd1054': 'Barton Peveril College', '572846473acd2414000df84f': 'Manhattan Project', '56dfb89e7aa994140058e071': 'The George', '56dfb89e7aa994140058e072': 'Scotland', '5731eaa7e17f3d1400422547': 'Industrial Revolution', '5731eaa7e17f3d1400422549': 'slightly lower', '5a7b2ba921c2de001afe9d80': '', '5a7b2ba921c2de001afe9d81': '', '5a7b2ba921c2de001afe9d83': '', '5a7b2ba921c2de001afe9d84': '', '572786b5dd62a815002e9f8a': 'five years', '572786b5dd62a815002e9f8b': 'John van Wyhe', '56e837f437bdd419002c44b2': 'Latin', '56e837f437bdd419002c44b3': 'Ancient Greek', '56e837f437bdd419002c44b4': 'Old Norse', '56e837f437bdd419002c44b5': 'English', '5ad27318d7d075001a4294ae': '', '5ad27318d7d075001a4294af': '', '5ad27318d7d075001a4294b0': '', '5ad27318d7d075001a4294b2': '', '56e09c507aa994140058e64e': 'H+', '56e790e237bdd419002c414e': '90th', '5acf5c9477cf76001a684c36': '', '5acf5c9477cf76001a684c37': '', '5acf5c9477cf76001a684c38': '', '5acf5c9477cf76001a684c3a': '', '57284dd0ff5b5019007da13e': '30 July 2003', '57284dd0ff5b5019007da140': \"a person's business, bookstore, and library records\", '57284dd0ff5b5019007da141': 'governing bodies in a number of communities', '5a86027fb4e223001a8e737f': '', '5a86027fb4e223001a8e7380': '', '5a86027fb4e223001a8e7381': '', '5a86027fb4e223001a8e7382': '', '57266970f1498d1400e8dede': 'turn', '57266970f1498d1400e8dee0': 'Vince McMahon', '5a7a553c21c2de001afe9b72': '', '5a7a553c21c2de001afe9b75': '', '5a7e59cc48f7d9001a063516': '', '5acf588577cf76001a684bd2': '', '5acf588577cf76001a684bd3': '', '5acf588577cf76001a684bd4': '', '5acf588577cf76001a684bd5': '', '5730c30bf6cb411900e24460': 'mid-19th', '5730c30bf6cb411900e24461': 'palagi traders', '5730c30bf6cb411900e24462': \"John (also known as Jack) O'Brien\", '5730c30bf6cb411900e24463': 'Salai', '5730c30bf6cb411900e24464': 'Louis Becke', '57304c5e8ab72b1400f9c401': 'aerial mines', '5727b735ff5b5019007d933e': 'ISO-8859-1', '5727b735ff5b5019007d933f': 'to make it trivial to convert existing western text', '5acd10bc07355d001abf32f4': '', '5acd10bc07355d001abf32f5': '', '5acd10bc07355d001abf32f7': '', '57322fcce17f3d14004226ed': 'Policy Committee', '57322fcce17f3d14004226ee': 'Steering Committee', '5a84a6117cf838001a46aa08': '', '5a84a6117cf838001a46aa09': '', '5a84a6117cf838001a46aa0a': '', '5a84a6117cf838001a46aa0b': '', '5a84a6117cf838001a46aa0c': '', '57317c50a5e9cc1400cdbfc0': '1743', '57317c50a5e9cc1400cdbfc1': 'papal gifts', '57317c50a5e9cc1400cdbfc3': 'designs or canvases', '5726e7d3f1498d1400e8ef85': 'The Corps of Royal Engineers', '5726e7d3f1498d1400e8ef86': 'Paris', '56fad599f34c681400b0c149': 'Asia', '56fad599f34c681400b0c14a': 'Holden', '56fad599f34c681400b0c14b': 'Afro-Asiatic', '5733e8ccd058e614000b656e': 'active in the allied war effort', '5733e8ccd058e614000b656f': 'Nazi Germany, Fascist Italy, and Imperial Japan', '5733e8ccd058e614000b6570': 'the armed forces', '5733e8ccd058e614000b6571': 'intelligence', '572a3a0b6aef0514001553a2': 'J. M. E. McTaggart', '572a3a0b6aef0514001553a3': 'two', '5a42d2aa4a4859001aac734d': '', '56e79b2d00c9c71400d77383': 'Yangtze River', '56e79b2d00c9c71400d77386': 'Stone Mountain', '56e79b2d00c9c71400d77387': 'tiger', '5a273563c93d92001a400425': '', '5726c3b1f1498d1400e8ea96': 'Pope John Paul II', '5726c3b1f1498d1400e8ea98': 'Pontifical Council for Interreligious Dialogue', '5726c3b1f1498d1400e8ea99': 'Andrei Gromyko', '5726c3b1f1498d1400e8ea9a': 'peace day', '5a6bb40d4eec6b001a80a4fd': '', '5a6bb40d4eec6b001a80a4fe': '', '5a6bb40d4eec6b001a80a4ff': '', '5a6bb40d4eec6b001a80a500': '', '56cc643d6d243a140015ef88': 'weak bass response', '56cc643d6d243a140015ef8a': 'high-impedance', '56cc643d6d243a140015ef8b': 'external headphone amplifier', '571a10584faf5e1900b8a881': '5th Avenue Theatre', '571a10584faf5e1900b8a882': 'fringe theatre', '571a10584faf5e1900b8a883': 'equity theaters', '571a10584faf5e1900b8a884': '28', '5726638e708984140094c492': 'the salination of the Werra river', '5a7cc42be8bc7e001a9e1fda': '', '5a7cc42be8bc7e001a9e1fdb': '', '5a7cc42be8bc7e001a9e1fdc': '', '5a7cc42be8bc7e001a9e1fdd': '', '570aaf434103511400d59928': 'top ranking', '570aaf434103511400d59929': '2008', '570aaf434103511400d5992a': 'fourth', '570aaf434103511400d5992b': 'second', '570aaf434103511400d5992c': '2010', '5ad420ea604f3c001a400729': '', '5ad420ea604f3c001a40072a': '', '5ad420ea604f3c001a40072b': '', '5ad420ea604f3c001a40072c': '', '5ad420ea604f3c001a40072d': '', '56e19df2e3433e1400423034': 'Eastern Catalan', '5726086889a1e219009ac16e': 'higher', '5ad18fb3645df0001a2d1f6c': '', '5ad18fb3645df0001a2d1f6d': '', '5ad18fb3645df0001a2d1f6e': '', '5ad18fb3645df0001a2d1f6f': '', '5acfbe5777cf76001a685bfc': '', '5acfbe5777cf76001a685bfd': '', '5acfbe5777cf76001a685bfe': '', '5acfbe5777cf76001a685bff': '', '572ea043c246551400ce4422': 'two', '572ea043c246551400ce4423': 'Greek and Turkish', '572ea043c246551400ce4424': 'Armenian and Cypriot Maronite Arabic', '5725bdfa271a42140099d10f': '1882', '5725bdfa271a42140099d110': '1886', '5725bdfa271a42140099d111': '1888', '5725bdfa271a42140099d112': '1897', '56cfef3c234ae51400d9c10e': 'Funeral March', '56d3913859d6e41400146795': 'Revolutionary Étude', '56d3913859d6e41400146796': 'Minute Waltz', '5731185a05b4da19006bcd90': 'Aves', '5731185a05b4da19006bcd91': 'Neornithes', '5731185a05b4da19006bcd92': 'Aves', '5731185a05b4da19006bcd93': '9,800 to 10,050', '570d6de5fed7b91900d460b8': '1519–1523', '570d6de5fed7b91900d460ba': 'Italian republics', '56f8def59e9bad19000a063e': 'three', '56f8def59e9bad19000a063f': 'Town Quay', '56f8def59e9bad19000a0640': 'Two', '56f8def59e9bad19000a0641': 'Red Funnel', '56f8def59e9bad19000a0642': 'Southampton Water', '56de80edcffd8e1900b4b98e': 'Munich stone-lifting contest', '5728a7b83acd2414000dfc03': 'Domestic Organization', '5728a7b83acd2414000dfc04': 'about 1940', '5728a7b83acd2414000dfc05': 'cases of cheating and academic dishonesty', '5728a7b83acd2414000dfc06': '1957', '5728a7b83acd2414000dfc07': 'all students, faculty, and staff', '5acec3f332bba1001ae4b33a': '', '5acec3f332bba1001ae4b33b': '', '5acec3f332bba1001ae4b33c': '', '5acec3f332bba1001ae4b33d': '', '57291a3e1d04691400779035': 'eleven', '57291a3e1d04691400779036': 'Western Allies', '57291a3e1d04691400779038': '1957', '5a4745ff5fd40d001a27dd9e': '', '5a4745ff5fd40d001a27dd9f': '', '5a4745ff5fd40d001a27dda0': '', '5a514b63ce860b001aa3fcbe': '', '5a514b63ce860b001aa3fcc0': '', '5728f761af94a219006a9e87': 'many wealthy merchants', '5730ed3ea5e9cc1400cdbaf2': '11 years', '5730ed3ea5e9cc1400cdbaf3': '99.0%', '5730ed3ea5e9cc1400cdbaf4': '1,918', '57060fde52bb891400689836': '2012', '57060fde52bb891400689837': 'Barack Obama', '57060fde52bb891400689838': 'foreign policy', '572a518ab8ce0319002e2a93': 'commercial centres and routes', '56df7b3d56340a1900b29c0c': 'Plymouth City Council', '56df7b3d56340a1900b29c0d': 'South West Water', '56df7b3d56340a1900b29c0e': 'Western Power Distribution', '56df7b3d56340a1900b29c0f': '2009', '56df7b3d56340a1900b29c10': 'Plympton', '56f8c9d29e9bad19000a04f0': 'cell division', '56f8c9d29e9bad19000a04f2': 'DNA polymerases', '56f8c9d29e9bad19000a04f3': 'Because the DNA double helix is held together by base pairing', '56f8c9d29e9bad19000a04f4': 'semiconservative', '5726e07a5951b619008f8106': 'the PVA', '5726e07a5951b619008f8108': \"broke the attack's momentum\", '5acd969f07355d001abf47ae': '', '5acd969f07355d001abf47af': '', '5acd969f07355d001abf47b0': '', '5acd969f07355d001abf47b1': '', '56e0cdc37aa994140058e723': 'Mac', '5a4d39a27a6c4c001a2bbc66': '', '5a4d39a27a6c4c001a2bbc67': '', '5a4d39a27a6c4c001a2bbc68': '', '5a4d39a27a6c4c001a2bbc69': '', '5a4d39a27a6c4c001a2bbc6a': '', '57302bbda23a5019007fcee3': 'Liberian timber exports', '57302bbda23a5019007fcee6': '60%', '57302bbda23a5019007fcee7': '2010', '5a62be8af8d794001af1c1fd': '', '5a62be8af8d794001af1c200': '', '5733b195d058e614000b6086': '2015', '5aced84b32bba1001ae4b713': '', '5aced84b32bba1001ae4b714': '', '5aced84b32bba1001ae4b715': '', '5aced84b32bba1001ae4b716': '', '56e7b1e900c9c71400d77503': 'an extra hour', '56e7b1e900c9c71400d77505': 'three', '56e7b1e900c9c71400d77506': 'Double Summer Time', '56e7b1e900c9c71400d77507': 'Central European Midsummer Time', '5a7fb0f18f0597001ac0007d': '', '56d638e71c8504140094700c': 'immune-stimulating microorganisms', '56d638e71c8504140094700d': 'social interactions', '56d638e71c8504140094700f': '2015', '56d9ddf4dc89441400fdb871': 'strangers', '5731450de6313a140071cda1': 'The Eye in the Sky', '5731450de6313a140071cda3': 'Electronic Warfare/Jamming', '5726f50add62a815002e9630': 'scribes', '5726f50add62a815002e9633': 'A. Aaboe', '5726f50add62a815002e9634': \"today's calendars\", '5725d375ec44d21400f3d643': 'Fortaleza del Cerro', '5725d375ec44d21400f3d644': 'a beacon', '5725d375ec44d21400f3d645': '1809', '571021d7a58dae1900cd68ce': 'James W. Rodgers', '571021d7a58dae1900cd68d0': 'Utah', '571021d7a58dae1900cd68d1': 'June 22, 1965', '571021d7a58dae1900cd68d2': 'Oklahoma', '5ad3f93e604f3c001a3ffaa5': '', '5ad3f93e604f3c001a3ffaa6': '', '5ad3f93e604f3c001a3ffaa8': '', '5733e5704776f41900661451': 'human–animal studies', '5733e5704776f41900661452': 'Anthrozoology', '5733e5704776f41900661454': 'positive', '5733e5704776f41900661455': 'anthropology, sociology, biology, and philosophy', '5ad2f672604f3c001a3fda6c': '', '57302c9aa23a5019007fceff': 'elliptical', '57302c9aa23a5019007fcf00': 'one direction', '572845f7ff5b5019007da09e': 'two', '572845f7ff5b5019007da09f': 'near the Somali village of Baarawe', '572845f7ff5b5019007da0a1': 'Al-Shabaab', '572845f7ff5b5019007da0a2': 'Kenyan', '5a839cf3e60761001a2eb829': '', '5a85b7e3b4e223001a8e71ba': '', '5a85b7e3b4e223001a8e71bb': '', '5a85b7e3b4e223001a8e71bc': '', '57266a2c5951b619008f7203': 'saltpetre', '57266a2c5951b619008f7204': '1673', '57266a2c5951b619008f7205': 'armed forces', '57266a2c5951b619008f7207': '£37,000', '5a8464dd7cf838001a46a7db': '', '5a8464dd7cf838001a46a7dc': '', '5a8464dd7cf838001a46a7dd': '', '570b00026b8089140040f69f': 'AT&T Corporation', '570b00026b8089140040f6a2': '1 MHz', '5a1f22043de3f40018b26511': '', '5a1f22043de3f40018b26512': '', '572a8012be1ee31400cb8043': 'music, dancing and poetry', '572a8012be1ee31400cb8044': 'Shuhda', '572a8012be1ee31400cb8046': '1258', '572a8012be1ee31400cb8047': 'Abbasid', '5ace804f32bba1001ae4a879': '', '5ace804f32bba1001ae4a87a': '', '5ace804f32bba1001ae4a87d': '', '572b883cf75d5e190021fe33': 'several colleges', '5acd76f907355d001abf437a': '', '5acd76f907355d001abf437b': '', '5acd76f907355d001abf437c': '', '5acd76f907355d001abf437d': '', '5acd76f907355d001abf437e': '', '5731a21de17f3d1400422295': 'limited resources', '5731a21de17f3d1400422296': 'free', '5a77ae98b73996001af5a4f9': '', '5a77ae98b73996001af5a4fb': '', '56dde0ea66d3e219004dad7e': 'four', '56dde0ea66d3e219004dad7f': 'the National Defence Act', '5ad37275604f3c001a3fe2b3': '', '5ad37275604f3c001a3fe2b4': '', '5ad3e04d604f3c001a3ff4b3': '', '5ad3e04d604f3c001a3ff4b4': '', '5709b165ed30961900e84427': 'European monarchs', '5a8cd753fd22b3001a8d8f2e': '', '5a8cd753fd22b3001a8d8f2f': '', '5a8cd753fd22b3001a8d8f30': '', '5a8cd753fd22b3001a8d8f32': '', '56e14538e3433e1400422d1e': 'French', '56e14538e3433e1400422d22': 'Catalan', '56d296f259d6e414001460f8': 'text', '56d296f259d6e414001460fa': 'āgamas', '56d296f259d6e414001460fc': 'size and complexity of the Buddhist canons', '56dfdbee7aa994140058e1c8': 'April 2007', '56dfdbee7aa994140058e1c9': 'July 2007', '56dfdbee7aa994140058e1ca': 'Carlsberg and Heineken', '572827843acd2414000df5af': 'microvilli', '5ace7ee632bba1001ae4a82b': '', '5ace7ee632bba1001ae4a82c': '', '5ace7ee632bba1001ae4a82d': '', '5ace7ee632bba1001ae4a82e': '', '5728b201ff5b5019007da4a2': 'Japan', '5ad22e93d7d075001a42866a': '', '5ad22e93d7d075001a42866b': '', '5ad22e93d7d075001a42866c': '', '5ad22e93d7d075001a42866d': '', '5728dbd84b864d1900164faa': 'Commune of Paris', '5728dbd84b864d1900164fab': '21,616', '5728dbd84b864d1900164fac': 'Hauts-de-Seine, Seine-Saint-Denis and Val-de-Marne', '573015da04bcaa1900d77155': 'the Bears', '573015da04bcaa1900d77156': 'Division III', '573015da04bcaa1900d77157': '19', '573015da04bcaa1900d77158': '2008, 2009', '573015da04bcaa1900d77159': 'John Schael', '5ace3ada32bba1001ae49f8b': '', '5ace3ada32bba1001ae49f8c': '', '5ace3ada32bba1001ae49f8e': '', '5ace3ada32bba1001ae49f8f': '', '56e7ac6c37bdd419002c430e': 'Lukou International Airport', '56e7ac6c37bdd419002c4311': '28 June 1997', '57325124e17f3d140042285b': '1759', '57325124e17f3d140042285e': '1693', '57325124e17f3d140042285f': 'Frederick Philipse', '5727a1eeff5b5019007d9161': 'Fiesta de las Flores y las Frutas', '5727a1eeff5b5019007d9164': 'bomba del chota', '57276683dd62a815002e9c34': 'early 20th century', '56e032247aa994140058e34c': '1959', '5acf864b77cf76001a6850d0': '', '5acf864b77cf76001a6850d1': '', '5acf864b77cf76001a6850d2': '', '5acf864b77cf76001a6850d4': '', '57321963e99e3014001e650c': 'Birds', '5a7704962d6d7f001a4a9f13': '', '5a7704962d6d7f001a4a9f14': '', '5a7704962d6d7f001a4a9f15': '', '5a7704962d6d7f001a4a9f16': '', '5a7704962d6d7f001a4a9f17': '', '5730a9732461fd1900a9cf64': 'Mesopotamia and the Persian Gulf', '5730a9732461fd1900a9cf65': 'Eridu', '5730a9732461fd1900a9cf66': 'irrigation', '5730a9732461fd1900a9cf67': 'Enki', '5a650f65c2b11c001a425bc5': '', '5a650f65c2b11c001a425bc6': '', '57310f4ae6313a140071cbcc': '19 March 1920', '57310f4ae6313a140071cbcd': 'League of Nations Council', '5a14a7c7a54d4200185292fe': '', '5a14a7c7a54d4200185292ff': '', '5a14a7c7a54d420018529301': '', '56de7b394396321400ee295a': 'Devonport', '56de7a53cffd8e1900b4b960': 'William Cookworthy', '56de7a53cffd8e1900b4b961': '1768', '56de7a53cffd8e1900b4b962': 'chemist', '56de7a53cffd8e1900b4b963': 'John Smeaton', '5732ae23cc179a14009dabfb': 'Pliocene', '5a4ebec8af0d07001ae8cc26': '', '5a4ebec8af0d07001ae8cc28': '', '5a4ebec8af0d07001ae8cc29': '', '572bc28a111d821400f38f77': 'essential for countries to be able to achieve high levels of economic growth', '5acd857607355d001abf4560': '', '5acd857607355d001abf4561': '', '5acd857607355d001abf4562': '', '5acd857607355d001abf4563': '', '57337cc94776f41900660bab': 'Bruno Latour', '5ad3d82d604f3c001a3ff370': '', '5ad3d82d604f3c001a3ff371': '', '5ad3d82d604f3c001a3ff372': '', '56e10bf4cd28a01900c674c3': 'April 19, 1971', '56e10bf4cd28a01900c674c4': 'Vladislav Volkov, Georgi Dobrovolski and Viktor Patsayev', '56fae1d7f34c681400b0c16d': 'Afro-Asiatic', '56fae1d7f34c681400b0c16e': 'Cushitic', '56fae1d7f34c681400b0c16f': 'Afar', '56fae1d7f34c681400b0c170': 'Somali', '56fae1d7f34c681400b0c171': '1900', '5723fc250dadf01500fa1fe1': 'Prince Frederick William of Prussia', '5723fc250dadf01500fa1fe2': 'London', '5723fc250dadf01500fa1fe3': '14', '572501720ba9f01400d97c25': 'Prince Frederick William of Prussia', '572501720ba9f01400d97c26': '14', '572501720ba9f01400d97c27': 'Germany', '572501720ba9f01400d97c28': 'Wilhelm', '57266b70708984140094c56a': 'Wilhelm', '5ad17607645df0001a2d1cf2': '', '571a2b2410f8ca1400304f29': '7th congressional district', '571a2b2410f8ca1400304f2a': 'Jim McDermott', '571a2b2410f8ca1400304f2b': '1988', '571a2b2410f8ca1400304f2c': 'Ed Murray', '56e7b1d437bdd419002c437b': 'Sunday', '56e7b1d437bdd419002c437c': 'June 30, 2006', '572fd6a6b2c2fd14005684f5': 'Business and Law', '573271ece17f3d140042297f': 'Kaesong sanctuary', '573271ece17f3d1400422980': 'use nuclear force', '573271ece17f3d1400422982': 'death of Stalin', '573271ece17f3d1400422983': 'Strategic Air Command', '5706b5fa0eeca41400aa0d70': \"Jewel's Catch One\", '5706b5fa0eeca41400aa0d71': 'One Voice Records', '5706b5fa0eeca41400aa0d73': 'Ozn', '5ad2932ad7d075001a429adf': '', '56f8ba089e9bad19000a03c9': 'Junk', '56f8ba089e9bad19000a03ca': 'dance', '56f96a3c9e9bad19000a08ee': 'US$62.7 million', '56f96a3c9e9bad19000a08f0': 'a trust fund', '572966e11d046914007793a7': 'additive', '5a74beb142eae6001a389a58': '', '5a74beb142eae6001a389a59': '', '5a74beb142eae6001a389a5a': '', '5a74beb142eae6001a389a5b': '', '5a74beb142eae6001a389a5c': '', '56e0c956231d4119001ac392': 'private networks', '56e0c956231d4119001ac393': 'file systems', '5a4d2f747a6c4c001a2bbc16': '', '5a4d2f747a6c4c001a2bbc17': '', '5a4d2f747a6c4c001a2bbc1a': '', '5ad417d2604f3c001a4003b7': '', '5ad417d2604f3c001a4003b8': '', '5ad417d2604f3c001a4003ba': '', '5726b8addd62a815002e8e27': 'Six', '5726b8addd62a815002e8e29': 'Vatican II', '5726b8addd62a815002e8e2a': 'Canon Law', '5729145a3f37b31900478001': 'several', '5a47387c5fd40d001a27dd7f': '', '5a47387c5fd40d001a27dd80': '', '5a47387c5fd40d001a27dd81': '', '5a47387c5fd40d001a27dd82': '', '5a47387c5fd40d001a27dd83': '', '5a5140ccce860b001aa3fc87': '', '5a5140ccce860b001aa3fc88': '', '5a5140ccce860b001aa3fc89': '', '5a5140ccce860b001aa3fc8a': '', '5a5140ccce860b001aa3fc8b': '', '56dfe86b7aa994140058e252': 'John Manners, 3rd Duke of Rutland', '56dfe86b7aa994140058e253': 'general', '572fa6bca23a5019007fc831': 'World War I', '572fa6bca23a5019007fc832': '29 May 1915', '572fa6bca23a5019007fc833': 'Anatolia', '57290895af94a219006a9fad': 'Le Monde and Le Figaro', '57290895af94a219006a9faf': '1835', '57290895af94a219006a9fb0': 'France 24', '56e162a3e3433e1400422e45': 'Boston Garden', '56e162a3e3433e1400422e46': 'two', '56e162a3e3433e1400422e47': '18,624', '56e162a3e3433e1400422e48': '17,565', '5726b8be708984140094cf16': 'religion', '5726b8be708984140094cf17': 'Catholic', '5726b8be708984140094cf18': 'Protestant', '5ad11411645df0001a2d0c95': '', '5ad11411645df0001a2d0c96': '', '5ad11411645df0001a2d0c97': '', '5ad11411645df0001a2d0c98': '', '5731bcdc0fdd8d15006c64c3': 'legal scholars', '5731bcdc0fdd8d15006c64c4': 'to appease the Anti-Federalists', '5731bcdc0fdd8d15006c64c7': 'Massachusetts', '5ad1423f645df0001a2d1421': '', '5ad1423f645df0001a2d1423': '', '5ad1423f645df0001a2d1424': '', '56f8d9db9b226e1400dd10e0': 'Senegal', '56f8d9db9b226e1400dd10e2': 'Atlantic Ocean', '56f8d9db9b226e1400dd10e3': '11° and 13°N', '5728116c3acd2414000df3a3': 'independence', '5a7a6eb521c2de001afe9c3b': '', '5a7a6eb521c2de001afe9c3c': '', '5a7a6eb521c2de001afe9c3d': '', '5726134f89a1e219009ac1fc': 'better', '5726134f89a1e219009ac1fd': 'LED lamps', '5726134f89a1e219009ac1fe': 'higher initial cost of alternatives and lower quality of light of fluorescent lamps', '5726134f89a1e219009ac1ff': 'mercury', '5ad192db645df0001a2d2008': '', '5ad192db645df0001a2d2009': '', '5ad192db645df0001a2d200a': '', '5ad192db645df0001a2d200b': '', '5ad29f83d7d075001a429c7a': '', '5ad29f83d7d075001a429c7b': '', '5ad29f83d7d075001a429c7c': '', '5ad29f83d7d075001a429c7d': '', '56e6df336fe0821900b8ec10': 'dance-pop', '56e6df336fe0821900b8ec12': 'power pops', '56e6df336fe0821900b8ec14': '18-54', '5710f431b654c5140001fa41': 'Isaac Newton', '57294c7f3f37b31900478213': '1 GW', '57294c7f3f37b31900478214': '10 GW', '57294c7f3f37b31900478215': '17 percent', '5ad12914645df0001a2d0ff6': '', '5ad12914645df0001a2d0ff7': '', '5ad12914645df0001a2d0ff8': '', '570a5d534103511400d59675': 'five', '570a5d534103511400d59677': '£800 million', '570a5d534103511400d59678': 'more than a million', '5a48747484b8a4001a7e7889': '', '5a48747484b8a4001a7e788a': '', '56defd9bc65bf219000b3e9d': 'CJOC', '56defd9bc65bf219000b3e9e': 'CFB Trenton', '56defd9bc65bf219000b3e9f': '427', '5ad3ec3b604f3c001a3ff75a': '', '5ad3ec3b604f3c001a3ff75b': '', '5ad3ec3b604f3c001a3ff75c': '', '5ad3ec3b604f3c001a3ff75d': '', '570b28ab6b8089140040f7a5': 'irregularities', '570b28ab6b8089140040f7a6': 'uninterrupted', '570b28ab6b8089140040f7a7': 'Friday, 15 October 1582', '570b28ab6b8089140040f7a8': 'Thursday, 4 October 1582', '5a3717ea95360f001af1b3ff': '', '5a3717ea95360f001af1b400': '', '5a3717ea95360f001af1b401': '', '5a3717ea95360f001af1b402': '', '57301c4fb2c2fd1400568893': 'corruption and political repression', '5a62a976f8d794001af1c199': '', '5a62a976f8d794001af1c19c': '', '56beb67d3aeaaa14008c929b': 'Cater 2 U', '56d4dd502ccc5a1400d832b0': 'co-producing', '56d4dd502ccc5a1400d832b1': 'melodies and ideas', '56ddd49566d3e219004dad08': 'National Polytechnic School', '56ddd49566d3e219004dad09': '1873', '572a982b34ae481900deaba3': 'political science', '572a982b34ae481900deaba4': 'Yale University', '572a982b34ae481900deaba7': 'Vietnam Veterans Against the War', '572956496aef051400154d12': 'World War I and World War II', '572956496aef051400154d13': 'Major-General Glyn Charles Anglim Gilbert', '5ad423a1604f3c001a40084b': '', '5ad423a1604f3c001a40084d': '', '572b7eb8be1ee31400cb83ed': '64Zn', '572b7eb8be1ee31400cb83ee': 'intense gamma radiation', '572b7eb8be1ee31400cb83f0': '65Zn', '5acfc84c77cf76001a685f42': '', '5acfc84c77cf76001a685f44': '', '5726a5daf1498d1400e8e608': 'John Williams', '5726a5daf1498d1400e8e609': 'London Missionary Society', '5726a5daf1498d1400e8e60a': 'headhunting', '5726a5daf1498d1400e8e60b': 'Robert Louis Stevenson', '5726a5daf1498d1400e8e60c': '1894', '5a6246a3f8d794001af1bf2a': '', '5a6246a3f8d794001af1bf2b': '', '5a6246a3f8d794001af1bf2e': '', '570e38eb0dc6ce1900204ea5': '79 CE', '570e38eb0dc6ce1900204ea6': 'yellow', '5ad1167a645df0001a2d0d15': '', '5ad1167a645df0001a2d0d16': '', '5ad1167a645df0001a2d0d17': '', '56ddde4a9a695914005b9626': 'Instituts de technologie', '56cd59a162d2951400fa652b': 'third-party', '570b26c7ec8fbc190045b886': 'Xbox 360 Dashboard', '570b26c7ec8fbc190045b887': 'AKQA and Audiobrain', '5a70c2358abb0b001a676178': '', '5a70c2358abb0b001a676179': '', '5a70c2358abb0b001a67617a': '', '5a70c2358abb0b001a67617b': '', '56dfa2414a1a83140091ebe0': '85%', '56dfa2414a1a83140091ebe1': '31%', '5acd54e907355d001abf3d75': '', '57264dcd708984140094c1d7': 'Athens', '57264dcd708984140094c1db': 'off the coast of northeast Euboea', '571ae53e9499d21900609b97': 'murder', '571ae53e9499d21900609b9a': 'Eusebius of Vercelli', '5acec23a32bba1001ae4b2f7': '', '5acec23a32bba1001ae4b2f8': '', '5acec23a32bba1001ae4b2f9': '', '5acec23a32bba1001ae4b2fa': '', '56dfc2b77aa994140058e153': 'the landscape', '56dfc2b77aa994140058e155': 'slash-and-burn', '56dfc2b77aa994140058e156': 'agriculture', '5acd607f07355d001abf3fbd': '', '5acd607f07355d001abf3fc0': '', '572fffb8a23a5019007fcc2a': 'Octavian', '572fffb8a23a5019007fcc2b': '27 BC', '572fffb8a23a5019007fcc2d': 'Octavian', '573367eed058e614000b5a5f': 'a marine reserve', '5a39963a2f14dd001ac72433': '', '5a39963a2f14dd001ac72434': '', '5ad504cb5b96ef001a10a9d3': '', '5ad504cb5b96ef001a10a9d4': '', '5ad504cb5b96ef001a10a9d5': '', '5ad504cb5b96ef001a10a9d6': '', '5ad68fcb191832001aa7b1f3': '', '5ad68fcb191832001aa7b1f4': '', '5ad68fcb191832001aa7b1f5': '', '5ad68fcb191832001aa7b1f6': '', '5731c1260fdd8d15006c6504': 'penicillins and cephalosporins', '5731c1260fdd8d15006c6505': 'polymyxins', '5731c1260fdd8d15006c6506': 'four', '5733b4cf4776f419006610ca': 'bacterial functions or growth processes', '5733b4cf4776f419006610cb': 'penicillins and cephalosporins', '5733b4cf4776f419006610cc': 'polymyxins', '5a65cfcfc2b11c001a425d58': '', '5a65cfcfc2b11c001a425d59': '', '5a65cfcfc2b11c001a425d5a': '', '5a65cfcfc2b11c001a425d5b': '', '56e79ed037bdd419002c4267': 'specifying the name of a location', '56e79ed037bdd419002c4269': 'two', '5725f1dc271a42140099d356': '1762', '57261ed8ec44d21400f3d921': 'Nash', '57261ed8ec44d21400f3d922': 'the Royal Mews', '57261ed8ec44d21400f3d923': 'Sir William Chambers', '57261ed8ec44d21400f3d924': '1762', '5a7a4c3d17ab25001a8a0490': '', '5a7a4c3d17ab25001a8a0491': '', '5a7a4c3d17ab25001a8a0492': '', '5a7a4c3d17ab25001a8a0494': '', '572d3b0d8351f81400e9d380': 'between 1978 and 1984', '572e6bacc246551400ce422f': 'Reynolds', '572e6bacc246551400ce4230': '1978 and 1984', '5a270daac93d92001a4003a1': '', '5a270daac93d92001a4003a4': '', '5a282e9bd1a287001a6d0ac6': '', '5a282e9bd1a287001a6d0ac9': '', '5730888c069b531400832165': 'near railway trunk routes', '5730888c069b531400832166': 'Massachusetts Bay Transportation Authority', '5730888c069b531400832167': 'rapid transit, light rail lines or other non-road public transport systems', '5a4e8143755ab9001a10f4a2': '', '5a4e8143755ab9001a10f4a3': '', '5a4e8143755ab9001a10f4a4': '', '57271739f1498d1400e8f389': 'reverse insulin resistance', '57301d1da23a5019007fcdab': 'two', '5ad4d50b5b96ef001a10a256': '', '5ad4d50b5b96ef001a10a257': '', '5ad4d50b5b96ef001a10a258': '', '5ad4d50b5b96ef001a10a259': '', '5ad4d50b5b96ef001a10a25a': '', '57295a38af94a219006aa307': 'fourth largest', '57295a38af94a219006aa30b': 'up to 30,000', '5a63e5537f3c80001a150b85': '', '5a63e5537f3c80001a150b88': '', '57317628e6313a140071cf60': 'pentatonic', '57317628e6313a140071cf64': \"a jaguar's growl\", '56d660e91c850414009470d3': 'condolences and assistance', '56d660e91c850414009470d4': 'May 14', '56d660e91c850414009470d5': 'UNICEF', '5727865cf1498d1400e8fad0': '20%', '5727865cf1498d1400e8fad3': 'choral and English language courses', '5ad2043fd7d075001a4281fa': '', '5ad2043fd7d075001a4281fc': '', '5ad2043fd7d075001a4281fd': '', '57338653d058e614000b5c81': '1879', '57338653d058e614000b5c82': 'Rev. William Corby', '57338653d058e614000b5c83': '17th of May', '57338653d058e614000b5c84': 'Washington Hall', '57338653d058e614000b5c85': 'LaFortune Student Center', '5ad3d325604f3c001a3ff26b': '', '5ad3d325604f3c001a3ff26d': '', '5ad3d325604f3c001a3ff26e': '', '56e0a2a3231d4119001ac2f4': 'Stalin', '56e0a2a3231d4119001ac2f5': 'accusations of collaboration with the invaders and separatism', '56e0a2a3231d4119001ac2f6': 'Georgian SSR', '5ace04ac32bba1001ae4996d': '', '5ace04ac32bba1001ae4996e': '', '5ace04ac32bba1001ae4996f': '', '5ace04ac32bba1001ae49970': '', '5ace04ac32bba1001ae49971': '', '5727657f708984140094dcf7': 'Dante Alighieri', '5727657f708984140094dcf8': 'Latin as well as Italian', '5727657f708984140094dcfa': 'Decameron', '5727657f708984140094dcfb': 'Petrarch', '5ad02a8077cf76001a686c54': '', '5ad02a8077cf76001a686c55': '', '5ad02a8077cf76001a686c57': '', '5ad02a8077cf76001a686c58': '', '5726083a89a1e219009ac164': 'Tottenham Hotspur', '5726083a89a1e219009ac165': 'North London derbies', '5726083a89a1e219009ac166': 'Manchester United', '5acd176407355d001abf3440': '', '5acd176407355d001abf3441': '', '5acd176407355d001abf3443': '', '56cd81df62d2951400fa6667': 'Electronic Industry Code of Conduct Implementation Group', '56cd81df62d2951400fa6668': 'Foxconn', '56cd81df62d2951400fa6669': 'Longhua, Shenzhen', '56d134c2e7d4791d00901ff7': 'Foxconn', '56d134c2e7d4791d00901ff8': 'Verité', '56d134c2e7d4791d00901ff9': '2006', '56e10dbdcd28a01900c674e3': 'globally to general users', '56e10dbdcd28a01900c674e4': 'Sun Jiadong', '5acd455207355d001abf3b88': '', '5acd455207355d001abf3b8b': '', '5727b808ff5b5019007d9358': 'Samuel Rutherford', '5727b808ff5b5019007d9359': 'A. V. Dicey', '5a3af1c53ff257001ab84351': '', '5a3af1c53ff257001ab84352': '', '5a3af1c53ff257001ab84353': '', '5a3af1c53ff257001ab84355': '', '5726bd64dd62a815002e8eea': 'Nigeria', '5726bd64dd62a815002e8eeb': '182 million', '5726bd64dd62a815002e8eec': 'seventh', '5726bd64dd62a815002e8eed': 'over 500', '5726bd64dd62a815002e8eee': 'English', '570a60076d058f1900182de2': 'fear', '570a60076d058f1900182de3': 'changes in pulse rate', '570a60076d058f1900182de4': 'baring teeth', '570a60076d058f1900182de5': 'Jonathan Turner', '570a60076d058f1900182de6': 'four', '5ad26126d7d075001a429004': '', '5ad26126d7d075001a429006': '', '5ad26126d7d075001a429007': '', '5ad26126d7d075001a429008': '', '56df0c1a3277331400b4d91b': 'the Pope', '5ad2fda5604f3c001a3fda8f': '', '5ad2fda5604f3c001a3fda90': '', '5ad2fda5604f3c001a3fda91': '', '5ad2fda5604f3c001a3fda92': '', '5ad2fda5604f3c001a3fda93': '', '57293faa6aef051400154be8': 'Genetic studies', '5ad4005e604f3c001a3ffccb': '', '5ad4005e604f3c001a3ffccd': '', '5ad4005e604f3c001a3ffcce': '', '5ad4005e604f3c001a3ffccf': '', '5726cb515951b619008f7e4b': 'The National Autonomous University of Mexico', '5726cb515951b619008f7e4c': '300,000', '5726cb515951b619008f7e4e': '74th', '5726cb515951b619008f7e4f': 'Ciudad Universitaria', '572f9c99a23a5019007fc7d3': '2N', '572f9c99a23a5019007fc7d4': 'a three-terminal device', '572f9c99a23a5019007fc7d7': 'a p–n–p germanium switching transistor', '5a7b83a521c2de001afea0e6': '', '5a7b83a521c2de001afea0e7': '', '56f9409c9b226e1400dd12c6': 'Eighth', '56f9409c9b226e1400dd12c7': '1928', '56f9409c9b226e1400dd12c8': 'Cass Gilbert', '5731628fe6313a140071ceb2': '1 mm or less', '5731628fe6313a140071ceb5': 'private devotion', '5727ba3c2ca10214002d94cc': '1863', '5727ba3c2ca10214002d94cd': 'George Armstrong Custer', '5727ba3c2ca10214002d94ce': 'Iron Brigade', '5727ba3c2ca10214002d94cf': '82%', '56f756c6a6d7ea1400e171d6': 'key elements of the music', '57268a4cdd62a815002e88b0': 'the only edition of the Daily Mirror to ever sell every single copy issued throughout the country', '57109988a58dae1900cd6abb': 'natural history', '57109988a58dae1900cd6abc': 'René-Antoine Ferchault de Réaumur', '570969eaed30961900e840ce': 'Himachal Pradesh', '570969eaed30961900e840d0': 'All major English daily newspapers', '570969eaed30961900e840d1': 'Aapka Faisla, Amar Ujala, Panjab Kesari, Divya Himachal', '570969eaed30961900e840d2': 'Radio and TV', '5a3636d5788daf001a5f878c': '', '5a3636d5788daf001a5f878d': '', '5a3636d5788daf001a5f878e': '', '5a3636d5788daf001a5f878f': '', '5732321ce17f3d140042271a': 'absolute head of state', '5727faefff5b5019007d99d1': 'maintain the list of scripts that are candidates or potential candidates for encoding', '5acd1d2407355d001abf3589': '', '5acd1d2407355d001abf358a': '', '5acd1d2407355d001abf358b': '', '5acd1d2407355d001abf358c': '', '5730eacaaca1c71400fe5b79': '2007', '5730eacaaca1c71400fe5b7a': 'budget constraints', '5730eacaaca1c71400fe5b7b': '330,000', '5730eacaaca1c71400fe5b7c': 'flight hours for crew training', '571aeca132177014007e9fee': '$301 million', '571aeca132177014007e9ff0': 'Geodon', '571d3bc95efbb31900334ed2': 'False Claims Act', '571d3bc95efbb31900334ed3': 'Eli Lilly', '571d3bc95efbb31900334ed4': 'Geodon', '571d3bc95efbb31900334ed5': '$301 million', '5ad3b079604f3c001a3feca7': '', '5ad3b079604f3c001a3feca9': '', '5ad3b079604f3c001a3fecaa': '', '5ad3b079604f3c001a3fecab': '', '570c56adfed7b91900d458d8': '1189', '570c56adfed7b91900d458d9': 'fines, court fees and the sale of charters and other privileges', '5728c567ff5b5019007da662': 'JPMorgan Chase', '5728c567ff5b5019007da663': '$25 million', '5728c567ff5b5019007da664': '$12.5 million', '5728c567ff5b5019007da665': '$32 million', '57300a06b2c2fd1400568789': '548,404', '57300a06b2c2fd140056878b': '9,123', '57300a06b2c2fd140056878d': 'Berkeley County', '5ad4188a604f3c001a400403': '', '5ad4188a604f3c001a400404': '', '5ad4188a604f3c001a400405': '', '5ad4188a604f3c001a400406': '', '5ad4188a604f3c001a400407': '', '570b221b6b8089140040f769': 'LifeSize Communications', '5a1f2bc43de3f40018b26536': '', '5a1f2bc43de3f40018b26538': '', '5a1f2bc43de3f40018b26539': '', '5a1f2bc43de3f40018b2653a': '', '5726ad725951b619008f79ef': 'Nepean Island', '5726ad725951b619008f79f1': 'Phillip Island', '5a81b79c31013a001a334dd2': '', '5a81b79c31013a001a334dd3': '', '5a81b79c31013a001a334dd4': '', '5ad2c97fd7d075001a42a23a': '', '5ad2c97fd7d075001a42a23b': '', '5ad2c97fd7d075001a42a23c': '', '5ad2c97fd7d075001a42a23d': '', '5ad2c97fd7d075001a42a23e': '', '572f90e2a23a5019007fc767': 'downtown St. Louis', '572f90e2a23a5019007fc76a': 'Robert S. Brookings, Henry Ware Eliot, and William Huse', '5ace156432bba1001ae49a6b': '', '5ace156432bba1001ae49a6d': '', '57313831497a881900248c74': 'Niulakita', '56d384e559d6e4140014660b': '19 Entertainment', '5727b5f02ca10214002d9488': 'Beinecke Rare Book and Manuscript Library', '5727b5f02ca10214002d9489': 'Yale University Art Gallery', '5727b5f02ca10214002d948a': 'Yale Center for British Art', '5727b5f02ca10214002d948b': 'Eli Whitney Museum', '5727b5f02ca10214002d948c': 'Yale', '572a470efed8de19000d5b66': 'Yale University Art Gallery', '5730048aa23a5019007fcc4f': 'Iran', '57098392ed30961900e8425e': '2014', '57098392ed30961900e8425f': '1974, 1978 and 1994', '57098392ed30961900e84260': 'Uruguay', '57098392ed30961900e84261': 'sixteen', '57098392ed30961900e84262': '4–1', '59fb34d4ee36d60018400d65': '', '59fb34d4ee36d60018400d66': '', '59fb34d4ee36d60018400d67': '', '59fb34d4ee36d60018400d69': '', '572b749abe1ee31400cb83ab': 'transcendental', '572b749abe1ee31400cb83ac': 'Berkeley', '572b749abe1ee31400cb83ad': 'Absolute', '5a7c8ba9e8bc7e001a9e1eb1': '', '5a7c8ba9e8bc7e001a9e1eb3': '', '5a7c8ba9e8bc7e001a9e1eb4': '', '5a7b9f0621c2de001afea1e0': '', '5a7b9f0621c2de001afea1e1': '', '5a7b9f0621c2de001afea1e2': '', '5a7b9f0621c2de001afea1e3': '', '56cbedde6d243a140015edf2': 'Souvenir de Paganini', '56cbedde6d243a140015edf5': 'two', '56cbedde6d243a140015edf6': '17 March 1830', '56cf6af94df3c31400b0d761': 'Souvenir de Paganini', '56cf6af94df3c31400b0d762': 'Vienna', '56cf6af94df3c31400b0d764': 'September 1829', '56d315d159d6e41400146222': 'Niccolò Paganini', '56d315d159d6e41400146223': 'Vienna', '57279c2edd62a815002ea1ef': 'five', '57279c2edd62a815002ea1f1': 'anumāṇa', '57279c2edd62a815002ea1f2': 'upamāṇa', '5a5e51b25bc9f4001a75aee1': '', '5a5e51b25bc9f4001a75aee2': '', '5a5e51b25bc9f4001a75aee3': '', '5a5e51b25bc9f4001a75aee5': '', '573254ece99e3014001e66c4': 'Secretary of Defense', '573254ece99e3014001e66c6': 'Augusta National Golf Club', '573254ece99e3014001e66c7': 'Columbia Associates', '573254ece99e3014001e66c8': 'July 1949', '572726a2f1498d1400e8f41d': 'General William Tecumseh Sherman', '572726a2f1498d1400e8f41e': 'Forty acres and a mule', '572726a2f1498d1400e8f41f': 'Equal Protection Clause of the 14th Amendment', '572726a2f1498d1400e8f420': 'President John F. Kennedy', '5ad40ef3604f3c001a400143': '', '5ad40ef3604f3c001a400144': '', '5ad40ef3604f3c001a400146': '', '5727d1683acd2414000ded2d': 'Roman artifacts', '5727d1683acd2414000ded2e': 'Grande Île', '5727d1683acd2414000ded30': '1988', '5acd583707355d001abf3e0a': '', '5acd583707355d001abf3e0c': '', '5acd583707355d001abf3e0d': '', '5acd583707355d001abf3e0e': '', '5727e8484b864d1900163fc8': '1228', '5727e8484b864d1900163fc9': 'Middle Ages', '56f75c11a6d7ea1400e17205': 'come scritto', '5734296dd058e614000b6a72': 'Great Falls, Lewistown, Cut Bank and Glasgow', '56d0772c234ae51400d9c2f9': 'Buddhacarita, the Lokottaravādin Mahāvastu, and the Sarvāstivādin Lalitavistara Sūtra', '56d0e42e17492d1400aab689': '5th century CE', '570f887880d9841400ab35a2': 'Queen Victoria', '570f887880d9841400ab35a5': 'world history', '5ad354ab604f3c001a3fdd89': '', '5ad354ab604f3c001a3fdd8a': '', '5ad354ab604f3c001a3fdd8c': '', '5ad354ab604f3c001a3fdd8d': '', '56cda50b62d2951400fa67ac': 'Tantalus Media', '56cda50b62d2951400fa67ad': 'Wii U', '56cda50b62d2951400fa67ae': 'November 12, 2015', '56cda50b62d2951400fa67af': 'March 5, 2016', '56d1335f17492d1400aabc14': 'The Legend of Zelda: Twilight Princess HD', '56d1335f17492d1400aabc15': 'Tantalus Media', '56d1335f17492d1400aabc17': 'March 4, 2016', '5a8db847df8bba001a0f9ba2': '', '5a8db847df8bba001a0f9ba3': '', '5a8db847df8bba001a0f9ba5': '', '572804792ca10214002d9ba1': 'temporal measurements', '5a7e003d70df9f001a8753fd': '', '5a7e003d70df9f001a8753ff': '', '5a7e003d70df9f001a875400': '', '5a7e003d70df9f001a875401': '', '5a8103d68f0597001ac00229': '', '5a8103d68f0597001ac0022a': '', '5a8103d68f0597001ac0022b': '', '5a8103d68f0597001ac0022c': '', '570d5aabfed7b91900d45f06': '1609', '570d5aabfed7b91900d45f08': 'a third', '570d5aabfed7b91900d45f09': '1613', '56f8a3aa9b226e1400dd0d23': 'Scientists', '56e7909700c9c71400d772e1': 'January 1912', '56e7909700c9c71400d772e2': 'Sun Yat-sen', '570b69c1ec8fbc190045ba01': \"High 'n' Dry\", '570b69c1ec8fbc190045ba02': '\"Photograph\", \"Rock of Ages\" and \"Foolin\\'\"', '570b69c1ec8fbc190045ba03': 'Quiet Riot', '570b69c1ec8fbc190045ba04': '1983', '5a5a3dd89c0277001abe70c2': '', '5a5a3dd89c0277001abe70c4': '', '56dd1e8366d3e219004dabd9': 'Catholics', '56dd1e8366d3e219004dabda': '22.3%', '56dd1e8366d3e219004dabdb': '19.9%', '56dd1e8366d3e219004dabdc': '1.6%', '5ad02ad977cf76001a686c73': '', '5ad02ad977cf76001a686c74': '', '5ad02ad977cf76001a686c75': '', '572fc623947a6a140053cc94': 'The Armenian Army, Air Force, Air Defence, and Border Guard', '572fc623947a6a140053cc95': '1991', '572fc623947a6a140053cc96': '1992', '572fc623947a6a140053cc98': 'Colonel General Seyran Ohanyan', '570e6b020b85d914000d7eb7': 'north-west', '570e6b020b85d914000d7eb8': 'India and Pakistan', '570e6b020b85d914000d7eb9': 'Indo-Aryan migration theory', '5a2994e803c0e7001a3e17ec': '', '5a2994e803c0e7001a3e17ee': '', '5a2994e803c0e7001a3e17ef': '', '5a2ab45c5b078a001a2f06c3': '', '5a2ab45c5b078a001a2f06c4': '', '5a2ab45c5b078a001a2f06c5': '', '572ebac003f98919007569ae': 'Mexico', '572ebac003f98919007569b0': 'Colorado', '5a0f25d8d7c85000188645bf': '', '5a0f25d8d7c85000188645c1': '', '5726128a89a1e219009ac1ea': 'senators', '5726128a89a1e219009ac1eb': 'high-ranking chiefs', '5726128a89a1e219009ac1ed': 'China', '5726128a89a1e219009ac1ee': 'clothing choice', '5a0cf7c8f5590b0018dab6b4': '', '5a0cf7c8f5590b0018dab6b5': '', '5a0cf7c8f5590b0018dab6b6': '', '5a0cf7c8f5590b0018dab6b7': '', '572f54dba23a5019007fc551': '1912', '572f54dba23a5019007fc552': 'Sun Yat-sen', '572f54dba23a5019007fc553': 'Yuan Shikai', '572fdf3904bcaa1900d76e20': 'FC Bayern Munich', '572fdf3904bcaa1900d76e21': '44th', '56f74604aef2371900625a89': 'between the 6th and 10th centuries', '56f74604aef2371900625a8a': 'Orthodox Christianity', '56f74604aef2371900625a8b': 'Roman Catholicism', '56f74604aef2371900625a8c': '11th century', '56f74604aef2371900625a8d': 'Orthodox', '5ad4bab45b96ef001a109e62': '', '5ad4bab45b96ef001a109e63': '', '5ad4bab45b96ef001a109e64': '', '5ad4bab45b96ef001a109e65': '', '5ad4bab45b96ef001a109e66': '', '5ad16905645df0001a2d1a24': '', '5ad16905645df0001a2d1a25': '', '5ad16905645df0001a2d1a26': '', '5ad16905645df0001a2d1a27': '', '5ad16905645df0001a2d1a28': '', '56cbd8c66d243a140015ed85': 'indirect', '56cbd8c66d243a140015ed87': 'Romantic era', '56cbd8c66d243a140015ed88': 'films and biographies', '56ce1138aab44d1400b88428': 'France', '56ce1138aab44d1400b88429': 'Poland', '56cf5a5aaab44d1400b890dd': 'France', '56cf5a5aaab44d1400b890de': 'political insurrection', '56cf5a5aaab44d1400b890df': 'Romantic era', '56de708f4396321400ee28df': 'Constantine the Great', '56de708f4396321400ee28e0': 'Edict of Milan', '56de708f4396321400ee28e1': 'Pontifex Maximus', '5a5abce09c0277001abe7162': '', '5a5abce09c0277001abe7163': '', '5a5abce09c0277001abe7164': '', '5a5abce09c0277001abe7165': '', '57325b9fe99e3014001e670b': 'Alan Rogerson', '57325b9fe99e3014001e670d': '\"intellectual dominance\"', '57325b9fe99e3014001e670e': 'mind control', '5ad3e86a604f3c001a3ff64f': '', '5ad3e86a604f3c001a3ff650': '', '5ad3e86a604f3c001a3ff651': '', '5ad3e86a604f3c001a3ff652': '', '5ad3e86a604f3c001a3ff653': '', '5733f165d058e614000b663d': 'a highly specialized criminal investigation police', '5725d36038643c19005acdb2': '25%', '5725d36038643c19005acdb3': 'white meat', '5728162d4b864d1900164440': 'The Best', '5728162d4b864d1900164441': 'Platinum', '5ad2a8afd7d075001a429e20': '', '5ad2a8afd7d075001a429e21': '', '5ad2a8afd7d075001a429e22': '', '5ad2a8afd7d075001a429e23': '', '5ad33f9a604f3c001a3fdbce': '', '5ad33f9a604f3c001a3fdbcf': '', '56f7c779aef2371900625c09': 'magnates', '56f7c779aef2371900625c0a': 'magnat', '56f7c779aef2371900625c0c': 'możni', '56f7c779aef2371900625c0d': 'Lithuania', '5722ccb20dadf01500fa1ef3': '1861', '5722ccb20dadf01500fa1ef5': 'chronic stomach trouble', '5722ccb20dadf01500fa1ef6': 'army manoeuvres', '5723d010f6b826140030fc8a': '1861', '5723d010f6b826140030fc8b': 'Conroy and Lehzen', '5723d010f6b826140030fc8c': 'Albert', '5723d010f6b826140030fc8e': '1861', '5724d5ba0a492a1900435636': 'March 1861', '5724d5ba0a492a1900435637': 'typhoid fever', '5724d5ba0a492a1900435638': '14 December 1861', '5724d5ba0a492a1900435639': 'widow of Windsor', '57257e8fcc50291900b28536': 'Conroy and Lehzen', '5ad176e7645df0001a2d1d20': '', '5ad176e7645df0001a2d1d22': '', '5ad176e7645df0001a2d1d23': '', '5ad176e7645df0001a2d1d24': '', '56f95c439b226e1400dd13a8': 'World War II', '56f95c439b226e1400dd13aa': '1944', '56f95c439b226e1400dd13ac': 'Wotje', '5726ed3ddd62a815002e9573': 'Hanja', '5a3bf219cc5d22001a521c3c': '', '5a3bf219cc5d22001a521c3d': '', '5a3bf219cc5d22001a521c3e': '', '5a3bf219cc5d22001a521c40': '', '56fb85aab28b3419009f1dfa': 'Pope Gregory VII', '56fb85aab28b3419009f1dfb': '1122', '5726bd86f1498d1400e8e9b1': 'semantic indicator', '5726593a5951b619008f7038': 'females are exempted from conscription', '5726593a5951b619008f7039': 'nine months', '5726593a5951b619008f703a': '18 and 60', '57280c3f3acd2414000df311': 'London Fire and Emergency Planning Authority', '572913111d0469140077901b': 'the Weimar Constitution of 1919', '572913111d0469140077901e': 'three', '572913111d0469140077901f': 'Hamburg and Bremen', '5a4736e95fd40d001a27dd75': '', '5a4736e95fd40d001a27dd76': '', '5a4736e95fd40d001a27dd77': '', '5a4736e95fd40d001a27dd79': '', '5a513e8ece860b001aa3fc7d': '', '5a513e8ece860b001aa3fc7f': '', '5a513e8ece860b001aa3fc80': '', '5a513e8ece860b001aa3fc81': '', '5725e48589a1e219009ac05a': 'Antiochus VII Sidetes', '5725e48589a1e219009ac05c': '66 BC – 217 AD', '5725e48589a1e219009ac05e': 'Iranian', '56e0bc7b231d4119001ac363': 'Vanguard', '56e0bc7b231d4119001ac365': 'Florida', '56e0bc7b231d4119001ac366': 'Jupiter-C rocket', '572ac792111d821400f38d5e': 'Bashar al-Assad', '572ac792111d821400f38d62': 'September 28', '57307352069b5314008320ee': 'Sorbonne University', '5ad29b2ed7d075001a429bc2': '', '5ad29b2ed7d075001a429bc3': '', '5ad29b2ed7d075001a429bc4': '', '5ad29b2ed7d075001a429bc5': '', '56df64a68bc80c19004e4bb4': 'teacher training', '572cb837750c471900ed4cf2': 'state courts', '5a79f25b17ab25001a8a01fe': '', '5a79f25b17ab25001a8a01ff': '', '5a79f25b17ab25001a8a0202': '', '56dc544814d3a41400c267bf': 'molecular biology and genetics', '56dc544814d3a41400c267c0': 'DNA', '5a591bbb3e1742001a15cf90': '', '5a591bbb3e1742001a15cf92': '', '5a591bbb3e1742001a15cf94': '', '57283f892ca10214002da182': 'cheaper', '5728ea364b864d1900165086': 'Kokin Wakashū', '5728ea364b864d1900165087': '905–914', '56f8ec5d9e9bad19000a0702': 'India', '56f8ec5d9e9bad19000a0703': 'the Balkans', '5726baf5f1498d1400e8e92d': 'The Guardian', '5726baf5f1498d1400e8e92e': 'Nigel Rumfitt QC', '56fb69e38ddada1400cd63f5': 'Christmas Day 800', '56fb69e38ddada1400cd63f7': '300', '56fb69e38ddada1400cd63f8': 'small farms', '56fb69e38ddada1400cd63f9': 'Scandinavia', '5729fcffaf94a219006aa725': 'when it is trapped in a system with zero momentum', '5729fcffaf94a219006aa727': 'Albert Einstein', '5729fcffaf94a219006aa728': '1905', '5729fcffaf94a219006aa729': 'E = mc²', '5acd46d507355d001abf3bcc': '', '5acd46d507355d001abf3bcd': '', '5acd46d507355d001abf3bcf': '', '5acd46d507355d001abf3bd0': '', '56dcdbe566d3e219004dab36': '1500 BC', '5acff76f77cf76001a686694': '', '5acff76f77cf76001a686695': '', '5acff76f77cf76001a686696': '', '5acff76f77cf76001a686697': '', '5726f52bdd62a815002e9643': 'December 2014', '5726f52bdd62a815002e9644': '2012', '572b72e9be1ee31400cb83a1': 'sixth century AD', '572b72e9be1ee31400cb83a2': 'Čech', '572b72e9be1ee31400cb83a5': 'Glagolitic', '5a7a0eba17ab25001a8a0280': '', '5a7a0eba17ab25001a8a0281': '', '5a7a0eba17ab25001a8a0282': '', '5a7a0eba17ab25001a8a0283': '', '5a7a0eba17ab25001a8a0284': '', '572a2b821d04691400779801': 'two', '572a2b821d04691400779802': 'fundamental', '5a42cd804a4859001aac7328': '', '5a42cd804a4859001aac7329': '', '57277595708984140094de35': 'Kvarner', '57277595708984140094de36': 'a man-like doll', '57277595708984140094de37': 'Jure Piškanac', '57277595708984140094de39': 'fritule', '5a5e5b755bc9f4001a75af39': '', '5a5e5b755bc9f4001a75af3a': '', '5a5e5b755bc9f4001a75af3d': '', '57095defed30961900e84004': 'biostatic', '57095defed30961900e84006': 'Muntz metal', '57095defed30961900e84007': 'netting materials', '5a836f64e60761001a2eb6e1': '', '5a836f64e60761001a2eb6e2': '', '5a836f64e60761001a2eb6e3': '', '5a836f64e60761001a2eb6e4': '', '57279d21ff5b5019007d9112': '1893', '5a8c8ad7fd22b3001a8d8a6e': '', '5a8c8ad7fd22b3001a8d8a6f': '', '5a8c8ad7fd22b3001a8d8a70': '', '5a8c8ad7fd22b3001a8d8a71': '', '5a8c8ad7fd22b3001a8d8a72': '', '570a6c176d058f1900182e4e': '1.2%', '570a6c176d058f1900182e4f': 'five', '5ad255afd7d075001a428d46': '', '5ad255afd7d075001a428d47': '', '5ad255afd7d075001a428d49': '', '5ad255afd7d075001a428d4a': '', '572a22256aef0514001552fb': 'Battle of Vienna', '572a22256aef0514001552fc': 'Europe', '57261a8e38643c19005acff3': 'Praxagoras of Kos', '57261a8e38643c19005acff4': 'Herophilos', '57261a8e38643c19005acff6': 'Herophilos', '570d9a31df2f5219002ed011': 'followed the pointer and loaded the shells', '570d9a31df2f5219002ed012': 'Germany', '57316532a5e9cc1400cdbf17': 'Poetry', '57316532a5e9cc1400cdbf18': 'women', '57316532a5e9cc1400cdbf1a': 'Six Chapters of a Floating Life', '57316532a5e9cc1400cdbf1b': 'Cao Xueqin', '572668e2708984140094c517': 'paternal grandmother', '572668e2708984140094c519': 'Vanity Fair', '57300da0947a6a140053cffc': 'Roman Empire', '57300da0947a6a140053cffd': '9th century', '5acd1c9c07355d001abf357e': '', '5acd1c9c07355d001abf357f': '', '5acd1c9c07355d001abf3580': '', '5acd1c9c07355d001abf3581': '', '56df6b9d56340a1900b29ae4': 'Nasrani', '56df6b9d56340a1900b29ae5': 'Isaai', '56df6b9d56340a1900b29ae6': 'Isa Masih', '5ad2de1cd7d075001a42a57c': '', '5ad2de1cd7d075001a42a57d': '', '5730b1488ab72b1400f9c6a4': 'Super Scope', '5730b1488ab72b1400f9c6a6': 'Mario Paint', '5730b1488ab72b1400f9c6a7': 'light gun', '5730b1488ab72b1400f9c6a8': 'BatterUP', '5a42e7df4a4859001aac7393': '', '5a42e7df4a4859001aac7395': '', '5a42e7df4a4859001aac7396': '', '5a42e7df4a4859001aac7397': '', '5728cbea3acd2414000dfeab': 'Detroit', '5728cbea3acd2414000dfead': 'Movement', '5728cbea3acd2414000dfeaf': 'Hart Plaza', '5726ccbedd62a815002e909a': 'May', '5726ccbedd62a815002e909b': '15 November 2009', '5726ccbedd62a815002e909c': 'Roger', '5726ccbedd62a815002e909d': 'USA', '572a5f33b8ce0319002e2ae7': 'the wide ethnic range of the Ottoman Empire', '572a5f33b8ce0319002e2ae8': 'Eritrea, Tunisia, Algiers, the Balkans and Romania', '56e9644c0b45c0140094cdef': '9,045', '56e9644c0b45c0140094cdf0': '1,185', '570b609e6b8089140040f8ea': 'Canadian', '570b609e6b8089140040f8ec': 'Irish', '570b609e6b8089140040f8ee': 'Germany', '5a5a32199c0277001abe70a2': '', '5a5a32199c0277001abe70a4': '', '5a5a32199c0277001abe70a6': '', '56ddd24566d3e219004dad05': '1997', '572a8990f75d5e190021fb57': 'Jamia Nayeemia Muradabad', '572a8990f75d5e190021fb5a': 'Hindu-Muslim friction', '5ace820b32bba1001ae4a8cb': '', '5ace820b32bba1001ae4a8cc': '', '5ace820b32bba1001ae4a8cd': '', '5ace820b32bba1001ae4a8ce': '', '5ace820b32bba1001ae4a8cf': '', '57277e4edd62a815002e9eb6': 'traditions', '57277e4edd62a815002e9eb8': 'boerenbruiloft', '570e432f0dc6ce1900204ee1': 'high-density penetrators', '570e432f0dc6ce1900204ee4': 'Persian Gulf', '5ad113dc645df0001a2d0c8a': '', '5ad113dc645df0001a2d0c8b': '', '5ad113dc645df0001a2d0c8c': '', '5ad113dc645df0001a2d0c8e': '', '5732549b0fdd8d15006c69c7': 'Council on Foreign Relations', '572ba616111d821400f38f34': 'Nouns', '572ba616111d821400f38f35': 'any case', '572ba616111d821400f38f38': 'a Slavic language', '5a7a316817ab25001a8a038e': '', '5a7a316817ab25001a8a038f': '', '5a7a316817ab25001a8a0390': '', '5a7a316817ab25001a8a0391': '', '5a7a316817ab25001a8a0392': '', '56dcfc7b66d3e219004dab7b': 'Bantus', '56dcfc7b66d3e219004dab7d': 'December 30, 2010', '5ad014c177cf76001a686916': '', '5ad014c177cf76001a686918': '', '5ad014c177cf76001a68691a': '', '5ad0d272645df0001a2d054c': '', '5acd71d807355d001abf428e': '', '5acd71d807355d001abf428f': '', '5acd71d807355d001abf4290': '', '5acd71d807355d001abf4291': '', '56dff3c2231d4119001abeed': 'The Rovers Return', '56dff3c2231d4119001abeee': 'The Queen Vic', '56dff3c2231d4119001abeef': 'BBC One', '56dff3c2231d4119001abef1': 'the Woolpack', '56ddcf2b66d3e219004dacf5': 'Affiliate Schools', '5726a3fc5951b619008f78b7': 'opposed', '5726a3fc5951b619008f78b8': 'le Worm', '5726a3fc5951b619008f78b9': 'Angela Merkel', '5726872c5951b619008f75ca': 'the military junta', '57279c1aff5b5019007d90e8': 'Saint Finbarr', '57279c1aff5b5019007d90e9': '6th century', '5a5d0b835e8782001a9d5e70': '', '5a5d0b835e8782001a9d5e71': '', '5a5d0b835e8782001a9d5e72': '', '5a7e45da70df9f001a87566d': '', '5a7e45da70df9f001a87566e': '', '5a7e45da70df9f001a87566f': '', '5a7e45da70df9f001a875670': '', '5a7e45da70df9f001a875671': '', '56de71b2cffd8e1900b4b8fc': 'European Reformation', '56de71b2cffd8e1900b4b8fe': 'Sir Thomas More and Cardinal John Fisher', '56de71b2cffd8e1900b4b8ff': 'Edward VI', '56de71b2cffd8e1900b4b900': '1612', '5a5ad1eb9c0277001abe71b0': '', '5a5ad1eb9c0277001abe71b2': '', '5a5ad1eb9c0277001abe71b3': '', '5a5ad1eb9c0277001abe71b4': '', '56f8dbf69e9bad19000a0610': 'Cunard Line', '56f8dbf69e9bad19000a0612': 'HRH The Duchess of Cornwall', '56f8dbf69e9bad19000a0613': '2011', '56f8dbf69e9bad19000a0614': 'Royal Princess', '572f2ce3a23a5019007fc4b3': '3 kV DC', '572f2ce3a23a5019007fc4b4': '25 kV 50 Hz AC', '5acd72be07355d001abf42c0': '', '5acd72be07355d001abf42c1': '', '5acd72be07355d001abf42c2': '', '5acd72be07355d001abf42c3': '', '570a55836d058f1900182d66': 'South Kensington', '570a55836d058f1900182d67': 'Albertopolis', '570a55836d058f1900182d69': 'Thomas Collcutt', '570a55836d058f1900182d6a': \"Queen's Tower\", '5a4860a984b8a4001a7e7856': '', '5a4860a984b8a4001a7e7857': '', '5a4860a984b8a4001a7e7859': '', '5723df4df6b826140030fcce': 'Recognition', '5726225d271a42140099d4c4': 'Recognition', '5726225d271a42140099d4c6': 'inter-visitation', '5726225d271a42140099d4c7': 'Exclusive Jurisdiction and Regularity', '5acf801b77cf76001a684fe6': '', '5acf801b77cf76001a684fe7': '', '5acf801b77cf76001a684fe9': '', '56beabab3aeaaa14008c91db': 'Vogue', '56bfafdba10cfb140055123d': 'Vogue', '56bfafdba10cfb140055123e': 'April 2013', '56bfafdba10cfb140055123f': 'Ban Bossy', '56bfafdba10cfb1400551240': 'Flawless', '56bfafdba10cfb1400551241': 'leadership in girls', '56d4d3b12ccc5a1400d83277': 'Chimamanda Ngozi Adichie', '56d4d3b12ccc5a1400d83278': 'Ban Bossy', '56de5ba04396321400ee284d': 'two', '56de5ba04396321400ee284e': 'Loughborough University of Technology', '570a6f996d058f1900182e5d': 'Aristotle', '570a6f996d058f1900182e5e': 'passions', '570a6f996d058f1900182e5f': 'Thomas Aquinas', '5ad24569d7d075001a428a9a': '', '5ad24569d7d075001a428a9b': '', '5ad24569d7d075001a428a9c': '', '5ad24569d7d075001a428a9d': '', '572f6c2cb2c2fd14005680f7': 'Sultan Alauddin Khilji', '572f6c2cb2c2fd14005680fa': '1325', '572f6c2cb2c2fd14005680fb': '1347', '570d6cf1fed7b91900d460a5': 'breaking up', '570d6cf1fed7b91900d460a6': 'General Canrobert', '570d6cf1fed7b91900d460a8': 'Rezonville', '571ae71a32177014007e9fca': 'Line-extensions', '571d1d495efbb31900334ea9': 'biotechnology', '571d1d495efbb31900334eaa': 'major pharmaceutical multinationals', '5ad399f2604f3c001a3fe844': '', '5ad399f2604f3c001a3fe845': '', '5ad399f2604f3c001a3fe846': '', '5726fc5d5951b619008f840f': 'Joséphine de Beauharnais', '5726fc5d5951b619008f8410': '1796', '5726fc5d5951b619008f8411': '26', '5726fc5d5951b619008f8413': 'Rose', '57286d4b4b864d19001649dc': '1979', '5ad13b7e645df0001a2d1300': '', '5ad13b7e645df0001a2d1301': '', '572f6eacb2c2fd140056810b': 'spiral', '572f6eacb2c2fd140056810c': 'photodiode', '572f6eacb2c2fd140056810d': 'change in height between pits and lands', '5a567d306349e2001acdcdc4': '', '5a567d306349e2001acdcdc6': '', '5a567d306349e2001acdcdc7': '', '5a567d306349e2001acdcdc8': '', '5735b062dc94161900571f24': 'four', '5735b062dc94161900571f27': 'casinos', '5732aeedcc179a14009dac04': 'about 10,000 years ago', '5732aeedcc179a14009dac08': 'Tyrrell Sea', '5a4ebffaaf0d07001ae8cc2f': '', '5a4ebffaaf0d07001ae8cc30': '', '5a4ebffaaf0d07001ae8cc31': '', '572f21e6b2c2fd1400567f3f': 'aerial circumnavigation of the world', '572f21e6b2c2fd1400567f40': '1960s', '5a2d5455f28ef0001a52648b': '', '5a2d5455f28ef0001a52648c': '', '5a2d5455f28ef0001a52648f': '', '56de43294396321400ee2736': 'unavailability of certain crucial data', '5ad0cfc9645df0001a2d0484': '', '5731fe53e17f3d14004225bb': '1 April 1942', '5731fe53e17f3d14004225bd': 'Harry Hopkins', '5731fe53e17f3d14004225be': 'Washington', '5731fe53e17f3d14004225bf': 'Portuguese Timor', '56e030597aa994140058e32d': \"St. James' Church\", '56e030597aa994140058e32e': 'Plantation House', '570cf05db3d812140066d34a': 'the small intestine', '570cf05db3d812140066d34d': 'a triglyceride', '573236d2e17f3d1400422736': '361', '573236d2e17f3d1400422738': \"Jerusalem's temple\", '57272cf65951b619008f868d': 'Truman', '57272cf65951b619008f868e': '10,000', '57272cf65951b619008f868f': 'Lincoln Memorial', '57272cf65951b619008f8690': 'equality of opportunity', '5ad4105d604f3c001a4001a3': '', '57277dfddd62a815002e9eac': '1879', '57277dfddd62a815002e9ead': 'over 60', '57277dfddd62a815002e9eae': '1954', '57277dfddd62a815002e9eaf': 'Ann Arbor Civic Ballet', '5ace3ac232bba1001ae49f79': '', '5ace3ac232bba1001ae49f7a': '', '5ace3ac232bba1001ae49f7b': '', '5ace3ac232bba1001ae49f7c': '', '5ace3ac232bba1001ae49f7d': '', '572fb2fb947a6a140053cbaa': 'Nikita Khruschev', '572fb2fb947a6a140053cbab': '1953', '572fb2fb947a6a140053cbac': '1955', '572fb2fb947a6a140053cbae': '1967', '57099d82ed30961900e84382': 'October 3, 2010', '57099d82ed30961900e84383': \"The Action Plan 2010–2015 for Canada's Cyber Security Strategy\", '57099d82ed30961900e84384': 'The Cyber Incident Management Framework for Canada', '5a557754134fea001a0e1ab5': '', '5a557754134fea001a0e1ab6': '', '5a557754134fea001a0e1ab7': '', '5a5cfad65e8782001a9d5e36': '', '5a5cfad65e8782001a9d5e38': '', '5a5cfad65e8782001a9d5e39': '', '5a5cfad65e8782001a9d5e3a': '', '570d7be1b3d812140066d9e0': 'Archbishop of Paris', '570d7be1b3d812140066d9e1': 'government buildings', '570d7be1b3d812140066d9e2': 'Tuileries Palace', '570d7be1b3d812140066d9e3': 'between 6,000 and 10,000', '570629ba52bb891400689914': 'bit rate', '570629ba52bb891400689915': 'input', '570629ba52bb891400689916': 'Compact Disc', '5730b108069b53140083226a': 'Eight', '56cddcae62d2951400fa6915': 'Alessandro Cremona', '56cddcae62d2951400fa6916': 'Stephanie Sigman', '56cf45bcaab44d1400b88ef3': 'Mexico', '56cf45bcaab44d1400b88ef5': 'Alessandro Cremona', '56cf45bcaab44d1400b88ef6': 'Estrella', '5ad22be1d7d075001a42860b': '', '5730e777aca1c71400fe5b48': 'U.S. Army Air Forces', '5730e777aca1c71400fe5b49': 'President Harry S Truman', '5727891b708984140094e032': '3,000', '5727891b708984140094e034': 'blows to the head', '5727891b708984140094e035': 'Mukhtar Shakhanov', '572805f5ff5b5019007d9b1a': 'RFC 3629', '572805f5ff5b5019007d9b1b': 'UTF-8', '5acd211a07355d001abf35fe': '', '5acd211a07355d001abf35ff': '', '5acd211a07355d001abf3600': '', '5acd211a07355d001abf3601': '', '5acd211a07355d001abf3602': '', '56e7860900c9c71400d77231': 'more than two and a half centuries', '56e7860900c9c71400d77232': 'Nanjing', '56fb7c108ddada1400cd6458': 'Genoa', '56fb7c108ddada1400cd645a': 'double-entry bookkeeping', '57313a17497a881900248c91': 'Dzungar–Qing War', '57313a17497a881900248c92': '1683', '570c5a9bfed7b91900d4590f': 'late 12th and early 13th centuries', '570c5a9bfed7b91900d45910': 'Henry II', '570c5a9bfed7b91900d45911': 'Treaty of Norham', '572a3b61af94a219006aa8e5': '1.5 million', '572a3b61af94a219006aa8e7': 'Greek and Assyrian minorities', '572a8395111d821400f38b90': 'Metrorail', '572a8395111d821400f38b91': '24.4', '572a8395111d821400f38b92': '23', '572a8395111d821400f38b93': 'three', '5ad3984c604f3c001a3fe7ed': '', '5ad3984c604f3c001a3fe7ee': '', '5ad3984c604f3c001a3fe7ef': '', '573036ea947a6a140053d2bc': 'The Merge', '573036ea947a6a140053d2bd': 'The South Bay Expressway', '573036ea947a6a140053d2be': 'Terminal 2', '573036ea947a6a140053d2c0': '37 percent', '5ad4dbaa5b96ef001a10a451': '', '5ad4dbaa5b96ef001a10a452': '', '5ad4dbaa5b96ef001a10a453': '', '5ad4dbaa5b96ef001a10a454': '', '56dfb0e97aa994140058dfe5': 'metal detector', '570a5bbf4103511400d59658': '£822.0 million', '570a5bbf4103511400d59659': '£754.9 million', '570a5bbf4103511400d5965c': '£124 million', '5a4869b284b8a4001a7e7868': '', '5a4869b284b8a4001a7e7869': '', '57313c1205b4da19006bcf02': 'Bronze age', '57313c1205b4da19006bcf03': 'Albania', '57313c1205b4da19006bcf04': 'Pergamon', '56e141e2e3433e1400422d05': '15.8%', '56e141e2e3433e1400422d08': 'Over 27,000', '5727d11d3acd2414000ded19': '83 °F', '5727d11d3acd2414000ded1c': 'one tornado per hour', '56e06d44231d4119001ac103': 'Wu Chinese', '56e06d44231d4119001ac104': '/p pʰ b/', '5acd2a8c07355d001abf378c': '', '5acd2a8c07355d001abf378d': '', '5acd2a8c07355d001abf378f': '', '56d08e3e234ae51400d9c384': 'Solar chemical processes', '56d08e3e234ae51400d9c385': 'artificial photosynthesis', '5726bd78dd62a815002e8ef4': '10,000', '5726bd78dd62a815002e8ef5': 'Spanish and English', '5726bd78dd62a815002e8ef6': '2007', '5726bd78dd62a815002e8ef7': 'giant, high definition screens', '5709a2e5200fba1400368201': 'during the manufacturing process', '5709a2e5200fba1400368203': 'insecurity', '5a55606b134fea001a0e1a68': '', '5a55606b134fea001a0e1a69': '', '5a55606b134fea001a0e1a6a': '', '5a5cd75a5e8782001a9d5ddc': '', '5a5cd75a5e8782001a9d5ddd': '', '5a5cd75a5e8782001a9d5ddf': '', '5a5cd75a5e8782001a9d5de0': '', '572713ce708984140094d966': '10', '572713ce708984140094d967': '1st century BC', '572713ce708984140094d968': 'The Nine Chapters on the Mathematical Art', '56fa2008f34c681400b0bfc8': 'Populus', '56fa2008f34c681400b0bfc9': 'cherry', '56fa2008f34c681400b0bfca': 'water conducting', '56fa2008f34c681400b0bfcb': 'diffuse-porous', '56defa45c65bf219000b3e71': 'Eastern Catholic cardinals', '5ad31659604f3c001a3fdb4d': '', '5ad31659604f3c001a3fdb50': '', '572fddea947a6a140053cd7c': 'Lake Sevan', '572fddea947a6a140053cd7d': 'chess, weightlifting and wrestling', '572fddea947a6a140053cd7f': 'Pan-Armenian Games', '570b3f0fec8fbc190045b910': 'January 2002', '570b3f0fec8fbc190045b911': 'al-Qaida', '570b3f0fec8fbc190045b914': 'Abu Sayyaf', '5ad178b7645df0001a2d1d7a': '', '5ad178b7645df0001a2d1d7e': '', '5731417005b4da19006bcf5e': 'Polish, Hungarian and Lithuanian', '5731417005b4da19006bcf5f': 'Prince Roman Mstislavich', '5731417005b4da19006bcf60': '1202', '5731417005b4da19006bcf61': 'Cyprian', '5ad0185377cf76001a6869ae': '', '5ad0185377cf76001a6869af': '', '5ad0185377cf76001a6869b0': '', '5ad0185377cf76001a6869b2': '', '56f8c2519e9bad19000a0447': 'two', '56f8c2519e9bad19000a0449': 'Southampton and District Sunday Football League', '56e136e7cd28a01900c676bb': 'More than two-thirds', '56e136e7cd28a01900c676bc': 'tidal areas', '56e136e7cd28a01900c676bd': 'Trimountain', '56e78bb537bdd419002c410a': 'Borneo', '5a6b5ebba9e0c9001a4e9f4e': '', '5a6b5ebba9e0c9001a4e9f51': '', '56cfb1a2234ae51400d9be86': 'three out of five', '56cfb1a2234ae51400d9be87': 'one out of four', '56cfb1a2234ae51400d9be88': '24', '5731e07b0fdd8d15006c65e5': 'Islam, Christianity, and Judaism', '56fb2c94f34c681400b0c1eb': '1000', '56fb2c94f34c681400b0c1ed': 'Manorialism', '56fb2c94f34c681400b0c1ee': 'feudalism', '56fb2c94f34c681400b0c1ef': '1095', '5728484dff5b5019007da0ce': '2 February 1207', '5728484dff5b5019007da0cf': 'Livonian Brothers of the Sword', '5728484dff5b5019007da0d0': '1237', '5728484dff5b5019007da0d1': '1346', '5728484dff5b5019007da0d2': 'German rule', '570fe7425ab6b819003910b3': 'Wisconsin', '570fe7425ab6b819003910b4': '1957', '570fe7425ab6b819003910b5': '1984', '570fe7425ab6b819003910b6': '1978', '5ad3f120604f3c001a3ff85b': '', '5ad3f120604f3c001a3ff85c': '', '5ad3f120604f3c001a3ff85d': '', '57267ab2dd62a815002e868a': 'New York, Los Angeles and Chicago', '57267ab2dd62a815002e868c': 'the Marvel Universe', '57267ab2dd62a815002e868d': 'Spider-Man and the Fantastic Four', '5a5fa27aeae51e001ab14b67': '', '5a5fa27aeae51e001ab14b69': '', '5a5fa27aeae51e001ab14b6a': '', '5726fd98708984140094d7d1': 'Schmoelders', '5aceaec232bba1001ae4b028': '', '5aceaec232bba1001ae4b029': '', '5aceaec232bba1001ae4b02b': '', '5728f05e3acd2414000e0235': '72.1 million', '5728f05e3acd2414000e0236': 'Notre Dame Cathedral', '5728f05e3acd2414000e0237': '9.2 million', '5728f05e3acd2414000e0238': 'Disneyland Paris', '572819473acd2414000df485': 'wood', '572819473acd2414000df486': '1892', '572819473acd2414000df487': 'downtown core', '5a6271a6f8d794001af1bfda': '', '5a6271a6f8d794001af1bfdb': '', '5a6271a6f8d794001af1bfdc': '', '5726ee62dd62a815002e9582': 'true theories', '5726ee62dd62a815002e9585': 'competing paradigms', '5a21d8f78a6e4f001aa08f6e': '', '5a21d8f78a6e4f001aa08f6f': '', '5a21d8f78a6e4f001aa08f70': '', '5a21d8f78a6e4f001aa08f71': '', '5a21d8f78a6e4f001aa08f72': '', '5728c1414b864d1900164d58': 'Loxias', '5728c1414b864d1900164d59': 'Musagetes', '5728c1414b864d1900164d5a': 'Manticus', '5726a1d5708984140094cc63': 'Annual Register', '5726a1d5708984140094cc66': '1789', '5ad0b591645df0001a2d0106': '', '5ad0b591645df0001a2d0107': '', '5ad0b591645df0001a2d0109': '', '5ad0b591645df0001a2d010a': '', '56f8997b9b226e1400dd0c94': 'Rosalind Franklin', '56f8997b9b226e1400dd0c95': 'James D. Watson and Francis Crick', '56f8997b9b226e1400dd0c96': 'reverse transcription in retroviruses', '56f8997b9b226e1400dd0c97': 'molecular genetics', '57320ba7e99e3014001e647b': 'Etruscan', '57320ba7e99e3014001e647c': 'Jupiter, Juno and Minerva', '56f71740711bf01900a44932': 'two', '57278fc7dd62a815002ea070': '59 Club', '57278fc7dd62a815002ea073': \"Eton Manor Boys' Club\", '5ad20a18d7d075001a42822c': '', '5ad20a18d7d075001a42822d': '', '5ad20a18d7d075001a42822e': '', '5ad20a18d7d075001a42822f': '', '5ad20a18d7d075001a428230': '', '56e02a437aa994140058e2dd': '1980s', '56e02a437aa994140058e2df': \"L'Association\", '56e02a437aa994140058e2e1': 'print market', '5acf875d77cf76001a685102': '', '5acf875d77cf76001a685103': '', '56fb879b8ddada1400cd64cf': '1347', '56fb879b8ddada1400cd64d0': '35', '56fb879b8ddada1400cd64d1': 'the jacquerie', '56fb879b8ddada1400cd64d3': 'Florence', '57326cefe99e3014001e67a6': '1989', '56e166ffcd28a01900c67877': 'Hal B. Wallis', '56e166ffcd28a01900c67878': 'Maxwell Anderson', '56e166ffcd28a01900c67879': '1971', '56e166ffcd28a01900c6787a': 'Richard Burton', '56e166ffcd28a01900c6787b': 'Rooster Cogburn', '5ad155de645df0001a2d17e4': '', '5ad155de645df0001a2d17e5': '', '5726c709f1498d1400e8eb03': '1993', '5726c709f1498d1400e8eb04': '2001', '5726c709f1498d1400e8eb05': 'Tai-lo', '5726c709f1498d1400e8eb06': '2007', '5a1e1aa53de3f40018b264c8': '', '5a1e1aa53de3f40018b264c9': '', '5a1e1aa53de3f40018b264ca': '', '5a1e1aa53de3f40018b264cc': '', '56fc88d898e8fc14001ea7d1': 'sounds', '56fc88d898e8fc14001ea7d2': 'brain', '56fc88d898e8fc14001ea7d3': 'allophones', '572837e7ff5b5019007d9f46': 'changed towards a generally autonomous model', '572837e7ff5b5019007d9f47': 'in the aftermath of the Soviet Union', '572837e7ff5b5019007d9f48': 'Boris Yeltsin', '572837e7ff5b5019007d9f49': 'Vladimir Putin', '5acfafea77cf76001a685870': '', '5acfafea77cf76001a685871': '', '5acfafea77cf76001a685872': '', '5acfafea77cf76001a685873': '', '57240e580a492a1900435609': '1832', '57240e580a492a190043560a': '122', '57240e580a492a190043560b': 'Princess Beatrice', '5725677c69ff041400e58c6b': 'Princess Beatrice', '5725677c69ff041400e58c6d': 'Lord Esher', '5ad17f1f645df0001a2d1e4b': '', '5ad17f1f645df0001a2d1e4c': '', '5ad17f1f645df0001a2d1e4e': '', '5726d4705951b619008f7f55': '1988', '5726d4705951b619008f7f57': '1988 to 1996', '5726d4705951b619008f7f59': '20', '5a284fbcd1a287001a6d0b4a': '', '5a284fbcd1a287001a6d0b4b': '', '5a284fbcd1a287001a6d0b4c': '', '5a284fbcd1a287001a6d0b4d': '', '5a284fbcd1a287001a6d0b4e': '', '5728c7dd2ca10214002da7ac': 'famines', '5728c7dd2ca10214002da7ad': '6.1 million to 10.3 million', '5728c7dd2ca10214002da7ae': '1876–78', '5728c7dd2ca10214002da7af': '1.25 to 10 million', '5a2716bbc93d92001a4003cb': '', '5a2716bbc93d92001a4003cc': '', '5a0dfd6ed7c85000188644b5': '', '5a0dfd6ed7c85000188644b6': '', '5a0dfd6ed7c85000188644b8': '', '5a0dfd6ed7c85000188644b9': '', '570b27b4ec8fbc190045b890': 'Ms. Pac-Man', '570b27b4ec8fbc190045b891': 'Assault Heroes', '570b27b4ec8fbc190045b892': 'November 3, 2004', '570b27b4ec8fbc190045b893': 'November 22, 2005', '5a70d3ef8abb0b001a6761bb': '', '5a70d3ef8abb0b001a6761bc': '', '5a70d3ef8abb0b001a6761bd': '', '5a70d3ef8abb0b001a6761be': '', '5a70d3ef8abb0b001a6761bf': '', '57284af44b864d19001648d1': 'Harkat-ul-Jihad al-Islami', '57284af44b864d19001648d4': 'mobilisation of resources for continuation of jihad in Kashmir', '5a85e99bb4e223001a8e72d5': '', '5a85e99bb4e223001a8e72d6': '', '5a85e99bb4e223001a8e72d7': '', '5a85e99bb4e223001a8e72d8': '', '573387acd058e614000b5cb1': 'Knute Rockne', '573387acd058e614000b5cb2': '105', '573387acd058e614000b5cb3': '13', '573387acd058e614000b5cb4': 'three', '573387acd058e614000b5cb5': '1925', '5728be814b864d1900164d3e': 'Latvia and Lithuania', '5728be814b864d1900164d40': 'Nordic-Baltic Eight', '5728be824b864d1900164d41': '1989', '572805304b864d1900164250': '8 December 1962', '572805304b864d1900164252': '12 November 1962', '5a611100e9e1cc001a33ce88': '', '5a611100e9e1cc001a33ce8c': '', '572fefcb947a6a140053ce2f': '121 BC', '56ce35b2aab44d1400b885b4': 'mental illness', '56ce35b2aab44d1400b885b5': 'lawyer', '57288f642ca10214002da46e': 'Over three quarters', '57288f642ca10214002da470': 'over 60', '57288f642ca10214002da472': 'United States Department of Education', '5acea68932bba1001ae4aeef': '', '5acea68932bba1001ae4aef2': '', '5706b4af0eeca41400aa0d5b': 'Cybotron', '5706b4af0eeca41400aa0d5f': 'Tony Wilson', '5ad28d6fd7d075001a429a2d': '', '57267786dd62a815002e85f2': 'Metrobús', '57267786dd62a815002e85f3': 'Ecobici', '57267786dd62a815002e85f5': 'ozone and nitrogen oxides', '572814c34b864d1900164420': 'TGS 2007', '5ad2a7a2d7d075001a429e13': '', '5ad2a7a2d7d075001a429e14': '', '5ad2a7a2d7d075001a429e15': '', '5ad33f45604f3c001a3fdbc1': '', '5ad33f45604f3c001a3fdbc2': '', '5ad33f45604f3c001a3fdbc3': '', '5ad33f45604f3c001a3fdbc4': '', '57303815947a6a140053d2ca': 'Tehran World Festival', '56d126e117492d1400aaba97': 'December 21, 2013', '570e08690dc6ce1900204d97': 'Antarctic fur seal', '570e08690dc6ce1900204d98': 'Sir James Weddell', '570e08690dc6ce1900204d99': 'British sealing expeditions', '5ad2623cd7d075001a429088': '', '5ad2623cd7d075001a429089': '', '5ad2623cd7d075001a42908a': '', '5ad2623cd7d075001a42908b': '', '5ad2623cd7d075001a42908c': '', '5ad2c367d7d075001a42a132': '', '5ad2c367d7d075001a42a133': '', '5ad2c367d7d075001a42a134': '', '5ad2c367d7d075001a42a135': '', '5725cc5938643c19005acd25': 'The Solís Theatre', '5725cc5938643c19005acd26': '1856', '5725cc5938643c19005acd28': '1998', '5725cc5938643c19005acd29': '2004', '5731bb10b9d445190005e4cf': 'Christians', '5731bb10b9d445190005e4d1': 'Scottish Catholic', '5731bb10b9d445190005e4d2': 'religious test', '5731bb10b9d445190005e4d3': '1799', '5ad141b5645df0001a2d13f8': '', '5ad141b5645df0001a2d13f9': '', '5ad141b5645df0001a2d13fa': '', '572749fdf1498d1400e8f5aa': 'Eli Whitney', '572749fdf1498d1400e8f5ac': 'Whitneyville', '572749fdf1498d1400e8f5ae': 'Samuel Colt', '572948091d04691400779249': 'Eli Whitney', '572948091d0469140077924a': 'Whitney Avenue', '572948091d0469140077924c': 'The Arsenal of America', '572948091d0469140077924d': '1836', '572a329aaf94a219006aa885': 'drought or pests', '572a329aaf94a219006aa887': 'agrarian communities', '5a7d388a70df9f001a875018': '', '5a7d388a70df9f001a87501a': '', '57098140200fba14003680cb': 'suicide attempts', '57098140200fba14003680cd': '3 ppm', '57098140200fba14003680cf': '30 mg/kg', '5a8371c6e60761001a2eb71b': '', '5a8371c6e60761001a2eb71d': '', '5a8371c6e60761001a2eb71e': '', '5a8371c6e60761001a2eb71f': '', '572771b85951b619008f8a09': '2007', '5a81a8b031013a001a334d31': '', '5a81a8b031013a001a334d32': '', '5a81a8b031013a001a334d33': '', '5a81a8b031013a001a334d34': '', '57267c335951b619008f7457': 'November 2, 2014', '57267c335951b619008f7458': 'Jon Lester', '57267c335951b619008f7459': '$155 million', '57267c335951b619008f745a': '97–65', '570d3bd1fed7b91900d45d65': '18th century', '570d3bd1fed7b91900d45d66': 'Gregory Maians and Perez Bayer', '570d3bd1fed7b91900d45d67': '1776', '570d3bd1fed7b91900d45d68': 'woven silk and ceramic tiles', '570d3bd1fed7b91900d45d69': 'Charles III', '57260e5b271a42140099d405': 'traditional', '57260e5b271a42140099d407': 'Euhemerism', '5727ec41ff5b5019007d9890': 'Europe', '5727ec41ff5b5019007d9892': 'US', '5acfb20177cf76001a685909': '', '5acfb20177cf76001a68590a': '', '5acfb20177cf76001a68590b': '', '5725cedc38643c19005acd57': 'Herbert Chapman', '5725cedc38643c19005acd59': '1930s', '5725cedc38643c19005acd5b': '1930 FA Cup Final', '5acd005507355d001abf3159': '', '5acd005507355d001abf315a': '', '5728d7c4ff5b5019007da7f6': 'Medicaid', '5a503d98ce860b001aa3fb1d': '', '5a503d98ce860b001aa3fb1e': '', '5a503d98ce860b001aa3fb1f': '', '5a503d98ce860b001aa3fb20': '', '5a503d98ce860b001aa3fb21': '', '56dcf8b79a695914005b94a7': 'Congolese Labour Party', '56dcf8b79a695914005b94a8': 'Parti Congolais du Travail', '5ad00faf77cf76001a686847': '', '5ad00faf77cf76001a686849': '', '5ad00faf77cf76001a68684a': '', '5acd893b07355d001abf4636': '', '5acd893b07355d001abf4637': '', '5acd893b07355d001abf4638': '', '5acd893b07355d001abf4639': '', '5728283a2ca10214002d9f70': 'fused bundles of cilia', '5728283a2ca10214002d9f72': 'polychaetes', '5ace7f8d32bba1001ae4a859': '', '5ace7f8d32bba1001ae4a85a': '', '5ace7f8d32bba1001ae4a85b': '', '5ace7f8d32bba1001ae4a85c': '', '56f71a5e3d8e2e1400e3735a': '1934', '56f71a5e3d8e2e1400e3735b': 'Milan Gorkić', '56f71a5e3d8e2e1400e3735c': 'Moscow', '56f71a5e3d8e2e1400e3735e': 'Tito', '570cff2cb3d812140066d393': 'Virgin Mary', '5ad18f44645df0001a2d1f4f': '', '5ad18f44645df0001a2d1f51': '', '572ba8ea111d821400f38f3e': 'low cost private schools', '572ba8ea111d821400f38f3f': 'disputes around whether the schools are affordable for the poor', '572ba8ea111d821400f38f40': 'Africa and Asia', '5acd809c07355d001abf449a': '', '5acd809c07355d001abf449b': '', '5acd809c07355d001abf449c': '', '5acd809c07355d001abf449e': '', '57062c2552bb891400689928': '7 July 1994', '57062c2552bb891400689929': 'l3enc', '57062c2552bb89140068992a': '.mp3', '57062c2552bb89140068992b': 'WinPlay3', '56e0a41f7aa994140058e68b': 'April 17, 1946', '56e0a41f7aa994140058e68c': 'East Prussia', '5ace063c32bba1001ae49995': '', '5ace063c32bba1001ae49998': '', '5726cc10f1498d1400e8eb82': 'perpendicular to the magnetic lines of force', '5ad16abe645df0001a2d1a74': '', '5ad16abe645df0001a2d1a75': '', '5ad16abe645df0001a2d1a76': '', '5ad16abe645df0001a2d1a77': '', '56e8379037bdd419002c44a8': 'logograms', '56e8379037bdd419002c44a9': 'Cantonese', '56e8379037bdd419002c44ab': 'Beijing', '56e8379037bdd419002c44ac': 'Standard Mandarin', '5ad27b63d7d075001a429653': '', '5a860699b4e223001a8e73c1': '', '57283e8bff5b5019007d9fe2': 'July 16, 1945', '57283e8bff5b5019007d9fe4': 'between 20 and 22 kilotons', '5726fad05951b619008f8409': 'Allen Metz and Carol Benson', '5726fad05951b619008f840b': 'Madonna', '572e7f8003f98919007566db': '1473', '572e7f8003f98919007566dd': '1489', '572e7f8003f98919007566de': 'Ottoman Empire', '572e7f8003f98919007566df': '1539', '57270821708984140094d8d1': 'state', '57270821708984140094d8d3': 'Louisiana State Agricultural Center', '57270821708984140094d8d4': '2005', '570ff9cda58dae1900cd679c': 'state collateral review', '5ad3f6d9604f3c001a3ffa01': '', '5ad3f6d9604f3c001a3ffa02': '', '5ad3f6d9604f3c001a3ffa03': '', '5ad3f6d9604f3c001a3ffa04': '', '56de0abc4396321400ee2564': 'Dari', '56de0abc4396321400ee2566': '875 CE', '5a18e9499aa02b0018605f26': '', '5a18e9499aa02b0018605f28': '', '5a18e9499aa02b0018605f29': '', '5726c7305951b619008f7dd3': 'Australian federal taxes', '5726c7305951b619008f7dd4': 'David Buffett', '5726c7305951b619008f7dd6': 'July 1, 2016', '5a81c94f31013a001a334eb3': '', '5a81c94f31013a001a334eb4': '', '5a81c94f31013a001a334eb5': '', '5a81c94f31013a001a334eb6': '', '56cd796762d2951400fa65ef': '2009', '56cd796762d2951400fa65f0': '14.21%', '56d12e4917492d1400aabb84': '220 million', '56d12e4917492d1400aabb86': '2013', '56dfb914231d4119001abd07': 'food and drink', '56dfb914231d4119001abd08': 'Holiday Inn', '56dfb914231d4119001abd09': 'innkeepers', '572904223f37b31900477f83': 'Homo sapiens', '572904223f37b31900477f84': '2 million years ago', '572904223f37b31900477f85': '1.5', '572904223f37b31900477f87': 'African Homo erectus', '56dddab666d3e219004dad30': 'Holy Roman Emperor Charles V', '56dddab666d3e219004dad31': '1568', '5a11c08c06e79900185c3548': '', '5a11c08c06e79900185c3549': '', '5a11c08c06e79900185c354b': '', '5726dcbf708984140094d3fd': 'Congress for Progressive Change', '5726dcbf708984140094d3fe': 'Muhammadu Buhari', '5726dcbf708984140094d3ff': '12,214,853', '57277434708984140094de01': 'The Fiscus Judaicus', '57277434708984140094de02': '96 CE', '57277434708984140094de03': 'illegality of male conversion to Judaism', '5ace989e32bba1001ae4abe7': '', '5729f6b13f37b31900478617': 'Earthquakes', '5729f6b13f37b31900478618': 'heat', '5729f6b13f37b31900478619': 'thermal energy', '5729f6b13f37b3190047861a': 'elastic strain', '5acd416807355d001abf3aa4': '', '5acd416807355d001abf3aa5': '', '5acd416807355d001abf3aa6': '', '572a4f507a1753140016ae92': 'ethnic Russians, Belarusians, and Ukrainians', '5a3c02eacc5d22001a521d22': '', '5a3c02eacc5d22001a521d23': '', '5a3c02eacc5d22001a521d24': '', '56d1109d17492d1400aab894': 'two-thirds', '57342937d058e614000b6a64': 'bacalhau', '57342937d058e614000b6a65': 'grilled sardines and caldeirada', '57342937d058e614000b6a67': 'beef, pork, lamb, or chicken', '57301519b2c2fd1400568827': 'three ounces', '57301519b2c2fd1400568828': '0.0042 inches', '57301519b2c2fd140056882a': 'thermal strains', '57301519b2c2fd140056882b': 'external heat sinks', '5ace96bf32bba1001ae4ab4d': '', '5ace96bf32bba1001ae4ab4e': '', '57338497d058e614000b5c4f': 'Paul Volcker', '57338497d058e614000b5c50': 'Paul Volcker', '57277a72708984140094deb2': 'Romania', '57277a72708984140094deb4': \"Congress of People's Deputies\", '5724041d0ba9f01400d97b19': '1870', '5724041d0ba9f01400d97b1a': 'Trafalgar Square', '5724041d0ba9f01400d97b1b': 'Radical MPs', '5724041d0ba9f01400d97b1c': 'arm', '5724041d0ba9f01400d97b1d': 'Joseph Lister', '57255c8a69ff041400e58c39': 'Trafalgar Square', '57255c8a69ff041400e58c3b': 'typhoid fever', '57267189708984140094c641': '1870', '57267f57f1498d1400e8e1cb': 'carbolic acid spray', '57267f57f1498d1400e8e1cc': 'typhoid fever', '57267f57f1498d1400e8e1cd': 'Radical MPs', '5ad17959645df0001a2d1db4': '', '5ad17959645df0001a2d1db7': '', '5ad17959645df0001a2d1db8': '', '5a0cfad6f5590b0018dab6ce': '', '5a0cfad6f5590b0018dab6cf': '', '5a0cfad6f5590b0018dab6d0': '', '5a0cfad6f5590b0018dab6d2': '', '56e83bdf37bdd419002c44bc': 'Italian and Spanish', '56e83bdf37bdd419002c44bd': 'body parts', '56f7165e3d8e2e1400e37336': 'Kumrovec', '56f7165e3d8e2e1400e37337': '1892', '56f7165e3d8e2e1400e37338': 'Croat', '56f7165e3d8e2e1400e3733a': '2nd', '5ad42d05604f3c001a40094e': ''}, 'similar_text': {'5731e624e17f3d140042251a': {'truth': 'Talaat Harb', 'predicted': 'industrialist Talaat Harb', 'question': 'Who financed Studio Misr?'}, '572805f84b864d190016425e': {'truth': '150', 'predicted': '', 'question': 'How many papers did Von Neumann publish?'}, '572805f84b864d1900164260': {'truth': 'Manhattan Project', 'predicted': '', 'question': 'What high profile controversial project was Von Neumann a prinipal of?'}, '572805f84b864d1900164261': {'truth': '60 in pure mathematics, 20 in physics, and 60 in applied mathematics', 'predicted': '150 papers in his life; 60 in pure mathematics, 20 in physics, and 60 in applied mathematics', 'question': 'Of his published works, what topics were they covering?'}, '56de6a2e4396321400ee28ad': {'truth': '40 kilometres', 'predicted': '40 kilometres radius', 'question': \"How far from its studio could the BBC's broadcast originally reach?\"}, '5a83215fe60761001a2eb427': {'truth': '', 'predicted': 'a British television set', 'question': 'What were engineers experimenting with in Alexandra?'}, '5726dad6708984140094d3ab': {'truth': '7.5 million dollars (31.5 million Reichsmark)', 'predicted': '7.5 million dollars', 'question': 'How much did the transfer of the Lithuanian Strip cost the Soviet Union?'}, '5726dad6708984140094d3ae': {'truth': 'until August 1, 1942', 'predicted': '', 'question': 'How long did the amendment extend the trade agreements?'}, '5ad28656d7d075001a4298de': {'truth': '', 'predicted': 'two and a half months', 'question': 'How long did germans have to relocate from the baltic states before the amendment of secret protocols?'}, '57309103069b531400832192': {'truth': 'two', 'predicted': '', 'question': 'ATC responsibilities are usually divided into how many main areas?'}, '5709b308ed30961900e84433': {'truth': '\"not worth a continental\"', 'predicted': 'not worth a continental\"', 'question': 'The quick loss in value of paper money resulted in which phrase being hear?'}, '5a8cdd89fd22b3001a8d8f6a': {'truth': '', 'predicted': '1862', 'question': 'Which year was it when paper money was first issued without the backing of the Articles of Confederation?'}, '5a8cdd89fd22b3001a8d8f6c': {'truth': '', 'predicted': 'War of 1812', 'question': 'Which other war also caused a disconnect between the United States Constitution?'}, '5a8cdd89fd22b3001a8d8f6e': {'truth': '', 'predicted': 'article 1', 'question': 'The loss in value resulted in a clause being written in which article in the Articles of Confederation?'}, '56d1ef6ae7d4791d00902599': {'truth': 'a Buddha', 'predicted': 'Buddha', 'question': 'Who is a fully awakened being who has purified his mind of the three poisons of desire, aversion, and ignorance?'}, '56bf940da10cfb1400551189': {'truth': 'twenty-fifth birthday', 'predicted': 'twenty-fifth', 'question': \"What birthday did Beyonce's album B'Day celebrate?\"}, '56e7305b37bdd419002c3de5': {'truth': 'weekend', 'predicted': 'weekend midnight', 'question': 'During what part of the week is the time change most often scheduled?'}, '56e7305b37bdd419002c3de7': {'truth': 'weekday schedules', 'predicted': '', 'question': \"What do we avoid disrupting by doing the time shift during days most people don't work?\"}, '572b8eb4111d821400f38f09': {'truth': 'a Czech linguistic revival', 'predicted': '', 'question': 'What did Josef Jungmann advocate for?'}, '5a36b2f895360f001af1b304': {'truth': '', 'predicted': 'one thousand', 'question': 'How many civilians were killed in the East End on 7 September 1940?'}, '5a36b2f895360f001af1b305': {'truth': '', 'predicted': '13', 'question': 'On what date in September 1939 did Britain declare war on Nazi Germany?'}, '5726ceaa5951b619008f7e94': {'truth': \"Ottoman Empire of the Tsar's\", 'predicted': 'Ottoman Empire', 'question': 'Who recognized and gave Russia the special guardian role?'}, '5730487aa23a5019007fd074': {'truth': '70% to 80%', 'predicted': '80% coverage or greater. Estimates of treatment coverage range from 70% to 80%', 'question': 'What percentage of HIV/AIDS infected in Swaziland are believed to be treated?'}, '5a56991a6349e2001acdce73': {'truth': '', 'predicted': '38.8%', 'question': 'What percentage of women have HIV?'}, '5a56991a6349e2001acdce74': {'truth': '', 'predicted': 'Themba Dlamini', 'question': 'Who declared a humanitarian crisis in Africa?'}, '572fc1dd947a6a140053cc5c': {'truth': 'The league held its first season in 1992–93', 'predicted': '1992–93', 'question': 'When did the Premier League hold its first season?'}, '572fc1dd947a6a140053cc5d': {'truth': 'was originally composed of 22 clubs.', 'predicted': '22', 'question': 'Originally, how many clubs did the Premier League have?'}, '572fc1dd947a6a140053cc5e': {'truth': 'The first ever Premier League goal was scored by Brian Deane of Sheffield United in a 2–1 win against Manchester United.', 'predicted': 'Brian Deane', 'question': 'Who scored the first ever goal for the Premier League'}, '572fc1dd947a6a140053cc5f': {'truth': 'Luton Town, Notts County and West Ham United were the three teams relegated from the old first division', 'predicted': 'Luton Town, Notts County and West Ham United', 'question': \"Which blubs were relegated from the old first division at the end of the 1991-1992 season and didn't take part in the first Premier League season?\"}, '5ad0c457645df0001a2d026c': {'truth': '', 'predicted': '22', 'question': 'During the leagues first season in 1991 how many clubs was it made up of?'}, '5ad0c457645df0001a2d026d': {'truth': '', 'predicted': '1992–93', 'question': 'In which year was the league made up of 21 clubs and had its first season?'}, '5725f36689a1e219009ac0f4': {'truth': 'first season', 'predicted': '', 'question': 'What did Arsenal want to commemorate by wearing dark red shirts in their last season at Highbury?'}, '5acd0cc307355d001abf3257': {'truth': '', 'predicted': 'Highbury', 'question': 'In what stadium did Arsenal play after the 2005-06 season?'}, '5acd0cc307355d001abf3258': {'truth': '', 'predicted': '1933', 'question': 'In what year did Herbert Chapman become manager of Arsenal?'}, '570e6d560dc6ce1900205051': {'truth': 'same criteria as usual, and not by the level of gallantry', 'predicted': '', 'question': 'What grade was determined?'}, '5730c36ab7151e1900c0152c': {'truth': 'Greek theologians of Byzantium', 'predicted': '', 'question': \"What Empire held  Grecian teachers of the virginity of Mary's conception ?\"}, '5730c36ab7151e1900c0152d': {'truth': 'St. Gregory Nazianzen, his explanation of the \"purification\" of Jesus and Mary', 'predicted': '', 'question': 'Who gave a reason for the purging of evil for the Blessed Virgin and her first child ?'}, '5730c36ab7151e1900c0152e': {'truth': 'the circumcision', 'predicted': 'purification\" of Jesus and Mary at the circumcision', 'question': 'What procedure was being performed while he gave his reasoning ?'}, '5730c36ab7151e1900c0152f': {'truth': 'Luke', 'predicted': '', 'question': 'Who was compelled to write of this instance that was also an author of one of the book of the Bible ?'}, '5a271a77c93d92001a4003ef': {'truth': '', 'predicted': 'Mother of God', 'question': 'The commemoration of what was growing around by Byzantium?'}, '56cfe6d2234ae51400d9c046': {'truth': 'New Jersey', 'predicted': 'New York and New Jersey', 'question': 'The Statue of Liberty is also in what other US state?'}, '570af6876b8089140040f644': {'truth': 'the 2010s', 'predicted': '2010s', 'question': 'In what decade did developers extend the capabilities of videoconferencing to more devices?'}, '570af6876b8089140040f645': {'truth': 'Mobile collaboration systems', 'predicted': '', 'question': 'What allows people in remote locations the ability to video-conference with colleagues far away?'}, '570af6876b8089140040f646': {'truth': 'still image streaming', 'predicted': 'live and still image streaming', 'question': 'What is one example of an application that videoconferencing manufacturers have begun to offer?'}, '5a1f38073de3f40018b26542': {'truth': '', 'predicted': 'live and still image streaming', 'question': 'What is one type of application that mobile collaboration manufacturers offer?'}, '5727d383ff5b5019007d962e': {'truth': 'Jesuit Matteo Ricci', 'predicted': 'Matteo Ricci', 'question': 'Who made predictions in 1601?'}, '573198280fdd8d15006c63ca': {'truth': 'a Nike advertisement', 'predicted': 'a Nike advertisement featuring Ronaldinho', 'question': 'What was the first video to reach a million views?'}, '5acd677a07355d001abf40d8': {'truth': '', 'predicted': 'Ronaldinho', 'question': 'What hockey star was in the Nike advertisement?'}, '56f9313f9b226e1400dd1285': {'truth': 'C', 'predicted': 'Avenue C', 'question': 'The first part of 13th Street is a dead end from which Avenue?'}, '56f9313f9b226e1400dd1287': {'truth': '13th Street', 'predicted': '', 'question': 'Which street has its third section between Eighth Avenue and Tenth Avenue?'}, '572fcc11947a6a140053ccd3': {'truth': 'Spirochaetes of the genus Borrelia', 'predicted': 'Spirochaetes', 'question': 'What bacteria is an exception to single circular chromosome rule?'}, '5727dad64b864d1900163e9e': {'truth': 'the USB specification also defines limits to the size of a connecting device', 'predicted': 'To address a weakness present in some other connector standards, the USB specification also defines limits to the size of a connecting device in the area around its plug', 'question': 'How is a weakness addressed in some other connector standards?'}, '5727dad64b864d1900163e9f': {'truth': 'fit within the size restrictions or support a compliant extension cable that does', 'predicted': '', 'question': 'Due to size restrictions compliant devices must what?'}, '572800942ca10214002d9b14': {'truth': 'the inrush current', 'predicted': 'inrush current', 'question': 'What does the USB specification limit?'}, '572800942ca10214002d9b16': {'truth': 'ultra low-power suspend mode when the USB host is suspended', 'predicted': 'ultra low-power suspend mode', 'question': 'What are USB devices required to enter?'}, '572800942ca10214002d9b17': {'truth': 'cut off the power supply to USB devices when they are suspended', 'predicted': 'cut off the power supply to USB devices', 'question': 'Many USB host interfaces do not what?'}, '572e82aacb0c0d14000f120c': {'truth': 'coal mines', 'predicted': 'coal mines, by the mid-19th century elevators were operated with steam power and were used for moving goods in bulk in mines and factories', 'question': 'For which industry were elevators first used?'}, '56df20e5c65bf219000b3f7a': {'truth': 'Galilean village', 'predicted': 'Galilean village of Nazareth', 'question': 'What village did Jesus come from?'}, '5ad2d8e3d7d075001a42a458': {'truth': '', 'predicted': 'Judaism', 'question': 'Which religion agrees that Jesus is the Messiah?'}, '5ad2d8e3d7d075001a42a459': {'truth': '', 'predicted': 'Judaism', 'question': 'Which religion does not accept Yehudim as the Messiah?'}, '5ad2d8e3d7d075001a42a45a': {'truth': '', 'predicted': 'Judaism', 'question': 'Which religion does not accept Notzri as the Messiah?'}, '56f89cb39e9bad19000a01c7': {'truth': 'the 1930s and 1940s', 'predicted': '1930s and 1940s', 'question': 'In what time span were the theories to integrate molecular genetic with Darwinian evolution developed?'}, '56f89cb39e9bad19000a01c8': {'truth': 'the modern evolutionary synthesis', 'predicted': 'modern evolutionary synthesis', 'question': 'What are the theories that integrate molecular genetics with Darwinian evolution called?'}, '56f89cb39e9bad19000a01ca': {'truth': '\"that which segregates and recombines with appreciable frequency.\"', 'predicted': '\"that which segregates and recombines with appreciable frequency', 'question': 'What is the definition of the concept of the gene as a unit of natural selection?'}, '5728b45f3acd2414000dfd16': {'truth': 'Wodeyar', 'predicted': 'Wodeyar dynasty', 'question': 'What kingdom was founded in Mysore in 1400 CE?'}, '5728b45f3acd2414000dfd17': {'truth': 'British and Marathas', 'predicted': 'combined forces of the British and Marathas', 'question': 'What groups was Mysore fighting in the later half of the 18th century?'}, '5728b45f3acd2414000dfd18': {'truth': 'the French', 'predicted': 'French', 'question': 'What country promised aid to Mysore to fight the British?'}, '5728b45f3acd2414000dfd19': {'truth': 'Hyder Ali', 'predicted': 'Hyder Ali and his son Tipu Sultan', 'question': 'Who took over rule of Mysore in the 18th century?'}, '5727e6f8ff5b5019007d97f8': {'truth': 'six red bands in the Tibetan flag', 'predicted': 'the six red bands in the Tibetan flag', 'question': 'How are the original ancestors of the Tibetan people represented?'}, '5ad0021777cf76001a686750': {'truth': '', 'predicted': 'the Se, Mu, Dong, Tong, Dru and Ra', 'question': 'What groups represented by the red bands in the flag of India are the original ancestors of the Tibetan people?'}, '5ad0021777cf76001a686751': {'truth': '', 'predicted': 'ethnic Tibetans', 'question': 'What other ethnic groups live throughout all of Tibet?'}, '5ad0021777cf76001a686752': {'truth': '', 'predicted': 'ethnic Tibetans and some other ethnic groups', 'question': 'What does the population of Tibet consist of?'}, '572f9a2ba23a5019007fc7c9': {'truth': 'blast', 'predicted': '', 'question': 'What type of furnace was functional in China in 722 BC?'}, '572f9a2ba23a5019007fc7cb': {'truth': 'wrought', 'predicted': 'wrought iron and steel', 'question': 'What type of iron could pig iron be converted into?'}, '5731c7ade17f3d14004223da': {'truth': 'Wittenberg', 'predicted': 'Castle Church in Wittenberg', 'question': \"Where was All Saints' Church?\"}, '572f9e5504bcaa1900d76aee': {'truth': 'genus Mycoplasma', 'predicted': 'Mycoplasma', 'question': 'What are one of the smallest bacteria?'}, '570a661f6d058f1900182e0a': {'truth': 'the appraisal theory of emotions', 'predicted': '', 'question': 'What theory was developed by Arnold?'}, '570a661f6d058f1900182e0b': {'truth': '2002', 'predicted': '1922–2002', 'question': 'When did Richard Lazarus die?'}, '570a661f6d058f1900182e0c': {'truth': 'Robert C. Solomon', 'predicted': 'Robert C. Solomon (1942–2007), an American philosopher who contributed to the theories on the philosophy of emotions with books such as What Is An Emotion?: Classic and Contemporary Readings', 'question': \"Who wrote 'What Is An Emotion?'?\"}, '5ad27982d7d075001a4295b1': {'truth': '', 'predicted': '1922–2002', 'question': 'When did Richard Lazarus work?'}, '572b8f5d111d821400f38f14': {'truth': 'Germany', 'predicted': '', 'question': 'Of the five countries with the greatest use of Czech, which country had the lowest percent of use?'}, '5a7a185f17ab25001a8a0320': {'truth': '', 'predicted': '24.86 percent', 'question': 'How long has Slovakia been a member of the EU?'}, '5a7a185f17ab25001a8a0324': {'truth': '', 'predicted': '1.93 percent), Poland (0.98 percent) and Germany (0.47 percent', 'question': 'What percentage of people in Portugal speak German?'}, '5731b71e0fdd8d15006c6484': {'truth': '\"[A] hedge or wall of separation between the garden of the church and the wilderness of the world\"', 'predicted': '', 'question': 'What phrase did Roger Williams first use?'}, '5ad13aec645df0001a2d12f2': {'truth': '', 'predicted': 'Thomas Jefferson', 'question': \"Who used William's phrase as a description of the Third Amendment and its restriction on the legislative branch?\"}, '572eb077c246551400ce451a': {'truth': 'Triton', 'predicted': '', 'question': 'What near Neptune did a spacecraft visit dangerously close?'}, '572eb077c246551400ce451b': {'truth': 'Neptune All Night', 'predicted': '', 'question': 'What program aired on PBS about Neptune?'}, '56df6c5a56340a1900b29af9': {'truth': 'lowest', 'predicted': 'the lowest', 'question': \"Where did Plymouth's life expectancy rank out of the regions of South West England?\"}, '5733f55e4776f419006615ac': {'truth': 'four', 'predicted': '', 'question': 'How many provinces does Pakistan have?'}, '5733f55e4776f419006615ad': {'truth': '205,344 square kilometres (79,284 square miles)', 'predicted': '205,344 square kilometres', 'question': 'How large is Punjab?'}, '5a68b42b8476ee001a58a76c': {'truth': '', 'predicted': '205,344 square kilometres', 'question': 'What is the area of Pakistan?'}, '57274eb3f1498d1400e8f60a': {'truth': 'Thale cress, Arabidopsis thaliana', 'predicted': 'Arabidopsis', 'question': 'What was the first plant to have its genome sequenced?'}, '57274eb3f1498d1400e8f60c': {'truth': 'one of the smallest genomes', 'predicted': '', 'question': 'Why was this plant chosen for sequencing?'}, '57274eb3f1498d1400e8f60d': {'truth': 'understanding the genetics', 'predicted': '', 'question': 'Why is sequencing done on plants?'}, '57274eb3f1498d1400e8f60e': {'truth': 'new knowledge about plant function', 'predicted': '', 'question': 'What results from  sequencing of DNA pairs?'}, '571adfb39499d21900609b6f': {'truth': 'knew Greek', 'predicted': 'Greek', 'question': 'Did Athanasius speak Greek?'}, '571adfb39499d21900609b70': {'truth': 'not knowing Hebrew', 'predicted': '', 'question': 'Did he understand Hebrew?'}, '571adfb39499d21900609b72': {'truth': 'Scriptural study and of Greek', 'predicted': '', 'question': 'What did students learn in the school in Alexandria?'}, '5ace995332bba1001ae4ac33': {'truth': '', 'predicted': 'Alexandrian School', 'question': 'What is the name of the school where Athanasius learned Hebrew?'}, '5aceb60032bba1001ae4b119': {'truth': '', 'predicted': 'Hebrew', 'question': 'What part of the Old Testament did he not know?'}, '5aceb60032bba1001ae4b11b': {'truth': '', 'predicted': 'Scriptures', 'question': 'What copy of the scriptures did he have in exile?'}, '56deeaae3277331400b4d811': {'truth': 'information', 'predicted': 'information rather than matter to be fundamental', 'question': 'Digital physicists consider what to be more important than matter?'}, '5725ca1589a1e219009abeac': {'truth': 'U.S. Libraries initiative with a goal of \"ensuring that if you can get to a public library, you can reach the internet', 'predicted': 'ensuring that if you can get to a public library, you can reach the internet', 'question': 'what is the US libraries initiative '}, '5725ca1589a1e219009abead': {'truth': \"Only 35% of the world's population has access to the Internet\", 'predicted': '35%', 'question': 'How much of the worlds population can reach the internet'}, '5725ca1589a1e219009abeae': {'truth': 'The foundation has given grants, installed computers and software, and provided training and technical support in partnership with public libraries nationwide', 'predicted': 'installed computers and software, and provided training and technical support', 'question': 'what have the grants provided public libraries'}, '5a0cdf07f5590b0018dab5fa': {'truth': '', 'predicted': '35%', 'question': \"How much of the world's population has access to libraries?\"}, '5a0cdf07f5590b0018dab5fb': {'truth': '', 'predicted': 'grants, installed computers and software, and provided training and technical support', 'question': 'What has the library given to increase access and knowledge?'}, '57264dedf1498d1400e8db8e': {'truth': '1886', 'predicted': '1886) merged in 1893', 'question': 'When was the Berne Convention?'}, '57264dedf1498d1400e8db90': {'truth': 'United International Bureaux for the Protection of Intellectual Property', 'predicted': '', 'question': 'What name did the merged secretariats adopt?'}, '5a157229a54d420018529436': {'truth': '', 'predicted': 'administrative secretariats', 'question': 'What position was established by the Paris convention in 1886 and the Berne Convention 1883?'}, '5a157229a54d420018529437': {'truth': '', 'predicted': 'Berne Convention', 'question': 'Secretaties from what convention merged with the secrataries of the Paris Convention  in 1886?'}, '5731e0ad0fdd8d15006c65eb': {'truth': 'constitution of the United States', 'predicted': 'the constitution of the United States', 'question': 'What does Bellah say the separation of church and state is grounded firmly in?'}, '5731e0ad0fdd8d15006c65ec': {'truth': 'the specific relation between politics and religion', 'predicted': 'the specific relation between politics and religion in the United States', 'question': 'What does Bellah use the term \"civil religion\\' to describe?'}, '5731e0ad0fdd8d15006c65ef': {'truth': 'a religious dimension', 'predicted': 'religious dimension', 'question': 'What has the separation of church and state failed to deny the political realm of?'}, '5ad14c5d645df0001a2d1650': {'truth': '', 'predicted': 'religious dimension', 'question': 'What has the separation of church and state failed to approve the political realm of?'}, '57094dde9928a81400471518': {'truth': '13', 'predicted': '', 'question': 'How many nations do the 25 foreign banks operating in Houston represent?'}, '5709c252200fba14003682b2': {'truth': 'third in the U.S', 'predicted': 'third', 'question': 'How did Houston rank in the U.S. for business?'}, '5ad420c6604f3c001a400717': {'truth': '', 'predicted': 'Forty', 'question': 'How many foreign governments maintain trade and commercial offices in Texas?'}, '57273aa2dd62a815002e99c2': {'truth': 'Order of the Garter', 'predicted': 'English Order of the Garter', 'question': 'What was the chivalric order established by Edward III in 1348?'}, '5ad022b677cf76001a686b5f': {'truth': '', 'predicted': 'Charles I of Hungary', 'question': 'Who co-founded the Order of St. George?'}, '572f9e8204bcaa1900d76af6': {'truth': 'Tang', 'predicted': 'Tang dynasty', 'question': 'What dynasty can the oldest wooden buildings in China be dated to?'}, '572f9e8204bcaa1900d76af7': {'truth': '907 AD', 'predicted': '618–907 AD', 'question': 'What is considered to be the last year of the Tang dynasty?'}, '5ad2ccebd7d075001a42a2ad': {'truth': '', 'predicted': 'Jesus has a unique significance', 'question': 'All Christians disagree with what about Jesus?'}, '57301bfca23a5019007fcd84': {'truth': 'unclear', 'predicted': 'it is unclear whether or not antibiotics cause obesity in humans', 'question': 'Do antibiotics cause obesity in humans?'}, '57301bfca23a5019007fcd85': {'truth': 'weighed against the beneficial effects', 'predicted': '', 'question': 'Why do physicians use antibiotics on infants when the relationship has been proven? '}, '5a65bfbec2b11c001a425d30': {'truth': '', 'predicted': 'subtherapeutic antibiotic treatment (STAT)– with either penicillin, vancomycin, penicillin and vancomycin', 'question': 'What are some microbiota that can be used for STAT?'}, '56f8ee329e9bad19000a071b': {'truth': '67.7%', 'predicted': '', 'question': 'What was the gross primary enrollment rate for males?'}, '56f8ee329e9bad19000a071c': {'truth': '40%', 'predicted': '', 'question': 'What was the gross primary enrollment rate for females?'}, '572eaae1dfa6aa1500f8d288': {'truth': 'Hijazi', 'predicted': '', 'question': 'What script were the Birmingham Quran fragments written in?'}, '572eaae1dfa6aa1500f8d289': {'truth': 'Arabic', 'predicted': '', 'question': 'Which modern script descends from the script on the Birmingham Quran fragments?'}, '5ad218cfd7d075001a42840a': {'truth': '', 'predicted': 'dots and chapter separators', 'question': \"What feature of the Birmingham Quran fragments' text make some doubt that it is newer than other known versions of the Quran?\"}, '57299fb33f37b31900478517': {'truth': 'divides the mountain system into two unequal portions', 'predicted': '', 'question': 'What does the Great Appalachian Valley do?'}, '57299fb33f37b31900478518': {'truth': 'has no axis of dominating altitudes, but in every portion the summits rise to rather uniform heights', 'predicted': '', 'question': 'What is common among all the mountains in the range?'}, '57299fb33f37b31900478519': {'truth': 'None of the summits reaches the region of perpetual snow.', 'predicted': '', 'question': 'What is the climate like on the summits?'}, '57318f33a5e9cc1400cdc083': {'truth': 'streets', 'predicted': '', 'question': 'What other cityscape is done with Portuguese pavement?'}, '570e63fe0dc6ce1900205006': {'truth': 'Olympic Stand', 'predicted': 'the Olympic Stand', 'question': 'Where did the National Sports Museum reopen in 2008?'}, '5728498d3acd2414000df8a1': {'truth': 'the powers of foreign policy and national defense as exclusive federal powers', 'predicted': '', 'question': 'In nearly all federalism countries, central powers enjoy what?'}, '5728498d3acd2414000df8a2': {'truth': 'federation would not be a single sovereign state', 'predicted': '', 'question': 'Per the UN definition, what is federalism?'}, '5728498d3acd2414000df8a3': {'truth': 'Germany retain the right to act on their own behalf at an international level,', 'predicted': '', 'question': 'What is the German Empire?'}, '5728498d3acd2414000df8a4': {'truth': 'powers which are not either exclusively of European competence or shared between EU and state as concurrent powers are retained by the constituent states.', 'predicted': '', 'question': 'What is the libson treaty?'}, '56f8c9d99b226e1400dd1003': {'truth': 'teleost fishes', 'predicted': 'teleost', 'question': 'The forebrain is everted in what type of fishes?'}, '5733f37ed058e614000b6651': {'truth': 'primarily as a self-defense force whose mission is to protect the territorial integrity of the country and provide humanitarian assistance and security', 'predicted': 'to protect the territorial integrity of the country', 'question': 'What is the primary purpose of the Portuguese armed forces?'}, '5733f37ed058e614000b6653': {'truth': '$5.2 billion, representing 2.1 percent of GDP', 'predicted': '$5.2 billion', 'question': 'How much money was spent on the Portuguese armed forced in 2009?'}, '572ac154be1ee31400cb8216': {'truth': 'President Bush and not American troops in general', 'predicted': 'President Bush', 'question': 'Who was Kerry saying was stuck in Iraq?'}, '57270870dd62a815002e9822': {'truth': 'The capacitance', 'predicted': 'capacitance', 'question': 'What value of some capacitors decreases with age?'}, '57270870dd62a815002e9824': {'truth': 'The type of dielectric', 'predicted': 'The type of dielectric, ambient operating and storage temperatures', 'question': 'What is one of the most important aging factors in capacitors?'}, '57270870dd62a815002e9825': {'truth': 'ambient operating and storage temperatures', 'predicted': 'The type of dielectric, ambient operating and storage temperatures', 'question': 'What is another important factor which governs how a capacitor ages?'}, '57270870dd62a815002e9826': {'truth': 'the Curie point', 'predicted': 'above the Curie point', 'question': 'At what point can the aging effect of a capacitor be reversed if the component is heated beyond?'}, '5acf5e9177cf76001a684c86': {'truth': '', 'predicted': 'capacitance', 'question': 'What value of some capacitors increases with age?'}, '5acf5e9177cf76001a684c87': {'truth': '', 'predicted': 'degradation of the dielectric', 'question': 'What causes the increase of capacitance in ceramic capacitors as they age?'}, '5732a8a6328d981900601fed': {'truth': 'the Antarctic Circumpolar Current', 'predicted': 'Antarctic Circumpolar Current', 'question': 'Which current resulted in the cooling of Antarctica?'}, '5a4ebc82af0d07001ae8cc14': {'truth': '', 'predicted': 'permanent ice cap', 'question': 'What semi-perminant feature developed in Antarctica?'}, '5a4ebc82af0d07001ae8cc16': {'truth': '', 'predicted': 'African plate', 'question': 'What plate was pushing south creating the Alps?'}, '56f8b9549e9bad19000a03b5': {'truth': 'A broad operational definition', 'predicted': 'broad operational definition', 'question': 'What sort of definition can be used to conveniently encompass the complexity of diverse phenomena?'}, '5727cf924b864d1900163dae': {'truth': 'Richard Owen,', 'predicted': 'Richard Owen', 'question': 'Who was the leading naturalist in Britain?'}, '5727cf924b864d1900163daf': {'truth': 'bitterly attacked Huxley, Hooker and Darwin, but also signalled acceptance of a kind of evolution as a teleological plan in a continuous \"ordained becoming\"', 'predicted': 'bitterly attacked Huxley, Hooker and Darwin,', 'question': 'How did Owen respond to On the Origin of Species with his review?'}, '5727cf924b864d1900163db1': {'truth': 'legendary 1860 Oxford evolution debate', 'predicted': 'Oxford evolution', 'question': 'What was the primary debate at the British Association for the Advancement of Science meeting of 1860?'}, '5727cf924b864d1900163db2': {'truth': 'Darwin published his own explanation in the Descent of Man (1871)', 'predicted': '1871', 'question': 'When did Darwin publish his own explanation of the question of the evolution of man and ape?'}, '572841842ca10214002da1ae': {'truth': 'Weapons Systems Evaluation Group', 'predicted': 'Weapons Systems Evaluation Group (WSEG)', 'question': 'In 1950 von Neumann became a consultant for what organization?'}, '572841842ca10214002da1af': {'truth': 'Armed Forces Special Weapons Project', 'predicted': 'Armed Forces Special Weapons Project (AFSWP)', 'question': 'What military organization did von NEumann also consult with for military aspect of nuclear weapons?'}, '572a1ef73f37b319004786ff': {'truth': 'Jacques Cauvin and Oliver Aurenche', 'predicted': '', 'question': 'What are the names of two researchers who divided neolithic chronology into ten periods?'}, '572a1ef73f37b31900478701': {'truth': '2002', 'predicted': '', 'question': 'When was the original division of 10 advanced to a division of 5?'}, '572a1ef73f37b31900478702': {'truth': 'Danielle Stordeur and Frédéric Abbès', 'predicted': '', 'question': 'What are the names of two researchers who divided neolithic chronology into five periods?'}, '5a7d103570df9f001a874f4e': {'truth': '', 'predicted': 'social, economic and cultural characteristics', 'question': 'What traits did researchers use to divide the nine periods?'}, '572ecec0cb0c0d14000f15b7': {'truth': 'The Yellow River', 'predicted': '', 'question': 'What emptied out to the south of the Shandong Peninsula?'}, '5731ce3ab9d445190005e575': {'truth': '1524–25', 'predicted': '1524', 'question': \"When was the German Peasants' War?\"}, '5731ce3ab9d445190005e576': {'truth': \"the Thirty Years' War\", 'predicted': \"Thirty Years' War\", 'question': 'What war was waged from 1618 to 1648?'}, '5731ce3ab9d445190005e578': {'truth': 'the Peace of Westphalia', 'predicted': 'Peace of Westphalia', 'question': \"What treaty ended the Thirty Years' War?\"}, '572774e65951b619008f8a5c': {'truth': 'the Carnival de la Laetare', 'predicted': 'Carnival de la Laetare', 'question': 'What Carnival takes place on Laetare Sunday?'}, '56d39d2359d6e41400146822': {'truth': '21', 'predicted': 'number 21', 'question': 'According to The American Library Association, what rank did the book have among the most frequently challenged books from 2000 to 2009?'}, '572a2c791d04691400779811': {'truth': '1998', 'predicted': '', 'question': 'In what year did the network end American programming?'}, '572a2c791d04691400779813': {'truth': 'a handful of British programs, and a few American movies and off-network repeats', 'predicted': '', 'question': 'What foreign programming is shown on the CBC after 1998?'}, '5a54e92d134fea001a0e1772': {'truth': '', 'predicted': 'CTV and Global', 'question': 'What two private Canadian broadcasters did the CBC merge with?'}, '5a54e92d134fea001a0e1774': {'truth': '', 'predicted': 'Canadian', 'question': 'What type of programming is permitted to be broadcast in Canada?'}, '57261f2fec44d21400f3d92c': {'truth': 'larger', 'predicted': 'generally larger', 'question': 'Were Greek temples built in the Hellenistic period larger or smaller than classical temples?'}, '57261f2fec44d21400f3d92f': {'truth': 'villa of Cassander at Vergina', 'predicted': 'Cassander at Vergina', 'question': 'What is the first example of Hellenistic period royal palace?'}, '5706ef6c9e06ca38007e9225': {'truth': '7th', 'predicted': '7th century', 'question': 'Which century did the lower-case script for the Greek Alphabet originate?'}, '5706ef6c9e06ca38007e9226': {'truth': '8th', 'predicted': '8th century', 'question': 'Which century did the Greek alphabet acquire its quadrilinear form?'}, '5706ef6c9e06ca38007e9229': {'truth': 'Uspenski Gospels', 'predicted': 'Uspenski Gospels (MS 461)', 'question': 'In which literature is the earliest dated Greek lower-case text?'}, '5728e7254b864d1900165062': {'truth': \"explained the empirical refutations of Newton's theory\", 'predicted': \"since Einstein also explained the empirical refutations of Newton's theory, general relativity was immediately deemed suitable for tentative acceptance on the Popperian account\", 'question': 'What did general relativity do that made it tentatively acceptable when it was proposed?'}, '5728e7254b864d1900165063': {'truth': \"Einstein's theory\", 'predicted': 'Einstein', 'question': \"In contrast to Gray, which theory did Popper argue was at least equally consistent with Newton's on the available evidence?\"}, '572cb395dfb02c14005c6c01': {'truth': 'no plenary reception statute at the federal level that continued the common law', 'predicted': '', 'question': 'How did the federal agencies differ from their English counter-parts?'}, '572cb395dfb02c14005c6c02': {'truth': 'granted federal courts the power to formulate legal precedent', 'predicted': '', 'question': 'What did the missing plenary reception do?'}, '572cb395dfb02c14005c6c03': {'truth': 'the federal Judiciary Acts', 'predicted': '', 'question': 'Where do the federal courts fall?'}, '572cb395dfb02c14005c6c04': {'truth': 'Article Three', 'predicted': '', 'question': 'Where is judicial power found in the original Constitution?'}, '572cb395dfb02c14005c6c05': {'truth': 'implied judicial power of common law courts to formulate persuasive precedent', 'predicted': '', 'question': 'What does Article Three give federal agencies?'}, '570d61a5b3d812140066d7a6': {'truth': 'visit of the pope to Valencia', 'predicted': 'the visit of the pope to Valencia', 'question': 'What happened at the same time as the crash, which may have contributed to the government downplaying it?'}, '5ace853932bba1001ae4a954': {'truth': '', 'predicted': 'septa and from each other by vertical mesenteries', 'question': \"What are annelids' body cavities combined with other segments by?\"}, '5ace853932bba1001ae4a955': {'truth': '', 'predicted': 'septa and from each other by vertical mesenteries', 'question': \"What are annelids' body cavities separated from eyes by?\"}, '56e087957aa994140058e5c2': {'truth': 'triplet state', 'predicted': '', 'question': 'What state are the protons in when in the orthohydrogen form?'}, '56e087957aa994140058e5c3': {'truth': 'normal', 'predicted': 'normal form', 'question': 'When hydrogen gas is in standard temperature and pressure, what form is it considered in>'}, '56df81eb5ca0a614008f9bc1': {'truth': 'Sir Joshua Reynolds', 'predicted': 'Painter Sir Joshua Reynolds', 'question': 'What notable artist and Royal Academician was born in Plympton?'}, '572a37a3af94a219006aa8bd': {'truth': 'heart of downtown', 'predicted': 'at the heart of downtown', 'question': 'Where is Yale University located.'}, '572ea0bedfa6aa1500f8d21e': {'truth': 'high-school diploma', 'predicted': 'a high-school diploma', 'question': 'What is required for university attendance?'}, '5727ff933acd2414000df1bd': {'truth': 'Anyone can do this', 'predicted': '', 'question': 'Is it difficult to transfer recording from historic interest to newer technologies?'}, '5727ff933acd2414000df1c1': {'truth': 'manipulated to remove analog flaws', 'predicted': '', 'question': 'What is one benefit of transferring an older format to a newer format?'}, '56f957d89e9bad19000a0853': {'truth': 'Morningside Avenue', 'predicted': 'Morningside Avenue at Morningside Park', 'question': 'At which intersection does W 122nd Street terminate?'}, '56e07ea2231d4119001ac1eb': {'truth': 'a former print and BBC journalist', 'predicted': '', 'question': 'Who runs Saint Helena online?'}, '56e07ea2231d4119001ac1ec': {'truth': 'Saint FM and the St Helena Independent', 'predicted': 'Saint FM', 'question': 'Who is partnered with Saint Helena online?'}, '5729234a1d046914007790a5': {'truth': 'hypotheses', 'predicted': 'hypotheses of the researcher and the populations sampled', 'question': 'Cluster structure of genetic data is dependent on what initial thing?'}, '5729234a1d046914007790a9': {'truth': 'methodologies', 'predicted': 'two separate methodologies', 'question': 'What are diversity partition and clustering analysis are examples of?'}, '572b8b54111d821400f38efd': {'truth': 'Age of Enlightenment a half-century earlier', 'predicted': 'Age of Enlightenment', 'question': \"What inspired the Czech's national pride?\"}, '572b8b54111d821400f38efe': {'truth': 'accomplishments', 'predicted': \"their people's accomplishments\", 'question': 'What did Czech historians emphasize about their countrymen?'}, '5a7a161417ab25001a8a030c': {'truth': '', 'predicted': 'denigrated Czech and other non-Latin languages', 'question': 'What emotion did the Counter-Reformation support during the mid-eighteenth century?'}, '5a7a161417ab25001a8a0310': {'truth': '', 'predicted': 'high culture', 'question': 'What did the Counter-Reformation advocate for the language to return to?'}, '570a85944103511400d59802': {'truth': '10th', 'predicted': '10th century', 'question': 'In what century did the Winchester standard arise?'}, '570a85944103511400d59804': {'truth': 'the Norman Conquest', 'predicted': 'Norman Conquest', 'question': 'What event led to English temporarily losing its importance as a literary language?'}, '5a678d67f038b7001ab0c2b5': {'truth': '', 'predicted': 'Ælfric of Eynsham', 'question': 'What writer helped develop the Winchester standard?'}, '570ddc210dc6ce1900204ccf': {'truth': 'early maturing', 'predicted': 'early', 'question': 'Are early or late maturing girls more exposed to alcohol and drug abuse?'}, '570ddc210dc6ce1900204cd1': {'truth': 'inexperienced', 'predicted': '', 'question': 'Who performs better in school: sexually experienced or inexperienced teen females?'}, '5ace2b5c32bba1001ae49c79': {'truth': '', 'predicted': 'Olin Business School', 'question': 'What university did Mahendra R. Gupta graduate from?'}, '5ace2b5c32bba1001ae49c7b': {'truth': '', 'predicted': 'Undergraduate BSBA students take 40–60% of their courses within the business school', 'question': 'How can undergraduate students attend BSBA?'}, '5ace2b5c32bba1001ae49c7c': {'truth': '', 'predicted': '40–60%', 'question': 'What percentage of an MBA students courses are at the business school?'}, '56f8d8959e9bad19000a05e1': {'truth': 'PAIGC (African Party for the Independence of Guinea and Cape Verde)', 'predicted': 'PAIGC', 'question': 'What party did Sanha belong to?'}, '56f8d8959e9bad19000a05e3': {'truth': 'more than 20', 'predicted': '20', 'question': 'How many minor political parties are there?'}, '5726c9c25951b619008f7e1f': {'truth': 'dispatched the United States Seventh Fleet to the Taiwan Strait', 'predicted': 'dispatched the United States Seventh Fleet', 'question': \"What did President Truman do to prevent hostilities between the People's Republic of China and Taiwan?\"}, '5726c9c25951b619008f7e21': {'truth': 'American aggression in the guise of the UN', 'predicted': 'American aggression', 'question': 'What provoked China to join the war?'}, '5726c9c25951b619008f7e22': {'truth': 'the Politburo', 'predicted': \"the Politburo that he would intervene in Korea when the People's Liberation Army's (PLA) Taiwan invasion force was reorganized into the PLA North East Frontier Force\", 'question': 'Where did Mao Zedong declare that he would intervene in the Korean conflict?'}, '5726e581f1498d1400e8ef29': {'truth': 'The corridors are treated as public places like streets, and the route between the station and the city centre is open all night', 'predicted': 'public places like streets', 'question': 'how are the shopping center corridors treated '}, '5726e581f1498d1400e8ef2a': {'truth': \"Parts of the city's network of canals, which were filled to create the shopping center and central station area, will be recreated\", 'predicted': \"Parts of the city's network of canals, which were filled to create the shopping center and central station area\", 'question': 'What is being recreated '}, '5726e581f1498d1400e8ef2b': {'truth': 'The Jaarbeurs, one of the largest convention centres in the Netherlands, is located at the west side of the central railway station', 'predicted': 'The Jaarbeurs', 'question': 'what is located on the west side of the rail station'}, '5a580e33770dc0001aeeffa0': {'truth': '', 'predicted': 'Hoog Catharijne', 'question': 'What shopping center is located in the city center?'}, '5a580e33770dc0001aeeffa1': {'truth': '', 'predicted': 'the route between the station and the city centre', 'question': 'What streets are open all night?'}, '5a580e33770dc0001aeeffa3': {'truth': '', 'predicted': 'The Jaarbeurs', 'question': 'What is one of the largest convention centres in Europe?'}, '56fc3f0e00a8df190040382a': {'truth': 'phonological theory', 'predicted': '', 'question': 'What is Evolutionary Phonology an integrated approach to?'}, '56fc3f0e00a8df190040382b': {'truth': 'synchronic and diachronic accounts', 'predicted': '', 'question': 'What is combined to sound patterns by Evolutionary Phonology?'}, '56fc3f0e00a8df190040382c': {'truth': 'recent years.', 'predicted': 'recent years', 'question': 'When did Evolutionary Phonology come into being?'}, '56fc3f0e00a8df190040382d': {'truth': 'integrated', 'predicted': '', 'question': 'What sort of approach did Evolutionary Phonology take?'}, '56e14acbcd28a01900c6774b': {'truth': '\"clean picture\"', 'predicted': 'clean picture\"', 'question': 'what was the policy that Universal followed in its early years?'}, '56e14acbcd28a01900c6774d': {'truth': 'Universal was losing money', 'predicted': 'generating more profit while Universal was losing money', 'question': 'Why did Laemmle change his position on \"unclean pictures\"?'}, '5ad13864645df0001a2d122d': {'truth': '', 'predicted': 'April 1927', 'question': 'When did Carl Laemmle decide to establish a clean picture policy?'}, '5728d51a4b864d1900164f08': {'truth': 'native construction, native blacksmithing, native textile design', 'predicted': 'native construction, native blacksmithing', 'question': 'What parts of native culture does the Viljandi Culture Academy highlight?'}, '56fa85dd8f12f3190063016f': {'truth': 'the United Kingdom', 'predicted': 'United Kingdom', 'question': 'Which European country first deployed HD content using the new DVB-T2 standard?'}, '5ad3bee3604f3c001a3fef4b': {'truth': '', 'predicted': 'United Kingdom', 'question': 'Which European country first deployed SD content using the new DVB-T2 standard?'}, '5ad3bee3604f3c001a3fef4c': {'truth': '', 'predicted': 'December 2009', 'question': 'When did the UK deploy SD content using the new DVB-T2 transmission standard?'}, '5ad3bee3604f3c001a3fef4e': {'truth': '', 'predicted': 'Digital TV Group', 'question': ' What does DTD stand for?'}, '5732bcead6dcfa19001e8a98': {'truth': '\"less than lethal\" or \"less-lethal\"', 'predicted': 'less than lethal', 'question': 'What should non-lethal weapons properly be called?'}, '5732bcead6dcfa19001e8a9b': {'truth': 'allows police to use deadly force against any person who poses a significant threat to them or civilians', 'predicted': 'allows police to use deadly force', 'question': 'What is South Africa\\'s \"shoot-to-kill\" policy?'}, '5732bcead6dcfa19001e8a9c': {'truth': 'Brazil', 'predicted': '', 'question': 'Where can police shoot fleeing convicts?'}, '5acece2e32bba1001ae4b4ab': {'truth': '', 'predicted': 'less than lethal', 'question': 'What should lethal weapons properly be called?'}, '5acece2e32bba1001ae4b4ad': {'truth': '', 'predicted': 'use of firearms or deadly force', 'question': 'What is supposed to be the first resort for police?'}, '5acece2e32bba1001ae4b4ae': {'truth': '', 'predicted': 'allows police to use deadly force', 'question': 'What is North Africa\\'s \"shoot-to-kill\" policy?'}, '572ee3a8c246551400ce4780': {'truth': 'he was forced to commit suicide', 'predicted': 'suicide', 'question': 'How did Dou Wu pass away?'}, '56f81f0ea6d7ea1400e173d8': {'truth': '1,200 kilometres', 'predicted': '1,200', 'question': 'How many kilometres do the Alps stretch?'}, '56f81f0ea6d7ea1400e173db': {'truth': 'the \"four-thousanders\"', 'predicted': 'four-thousanders', 'question': 'The Alpine region is also known as what? '}, '57313c12a5e9cc1400cdbd6c': {'truth': 'dot-matrix', 'predicted': 'seven-segment, starburst and dot-matrix', 'question': 'What is another format that Alphanumeric LEDs are available in?'}, '57313c12a5e9cc1400cdbd6d': {'truth': 'Starburst', 'predicted': 'Starburst displays', 'question': 'Which alphanumeric LED display can display all letters?'}, '57313c12a5e9cc1400cdbd6f': {'truth': 'liquid crystal displays', 'predicted': 'rising use of liquid crystal displays', 'question': 'What has reduced the popularity of numeric LED displays?'}, '56f744beaef2371900625a78': {'truth': 'The common practice period', 'predicted': '', 'question': 'What began with the Baroque era?'}, '56f744beaef2371900625a7b': {'truth': 'about 1910', 'predicted': '1910', 'question': 'When did the romantic era end?'}, '56d5307f2593cc1400307abc': {'truth': 'about 9,000', 'predicted': '9,000', 'question': 'What was the previous population of Yingxiu?'}, '56d5307f2593cc1400307abf': {'truth': 'Eight schools', 'predicted': 'Eight', 'question': 'What is the number of schools that collapsed in Dujiangyan?'}, '572a69cefed8de19000d5bfe': {'truth': 'Zinc', 'predicted': 'Zinc supplementation', 'question': 'What has been used and shown successful in a decrease in incidence of diarrheal disease?'}, '572a69cefed8de19000d5bff': {'truth': 'zinc supplementation', 'predicted': '', 'question': 'WHat strategy was found out to be more cost effective?'}, '5a0e3462d7c8500018864549': {'truth': '', 'predicted': 'promoting better eating practices', 'question': 'How can zinc deficiencies be combated?'}, '5a0e3462d7c850001886454b': {'truth': '', 'predicted': 'vitamin A supplementation', 'question': 'What dietary deficiency was found to be less effective in reducing diarrhea incidence?'}, '57110273b654c5140001fab2': {'truth': 'Charles Porset', 'predicted': 'Porset', 'question': 'Who believed the avoidance of thematic and heirarhical systems allowed free interpretation of the works and caused them to beomce an example of eglitarianism?'}, '572817474b864d190016445c': {'truth': 'seven stories', 'predicted': 'up to seven stories', 'question': 'How high were some of the buildings in urban Germany?'}, '5acd6c4a07355d001abf4185': {'truth': '', 'predicted': 'lavishly ornate façade of painted bricks, woodwork and majolica', 'question': 'For what purpose was the Ecole des Arts decoratifs built?'}, '572f820a04bcaa1900d76a39': {'truth': 'the Acadian population', 'predicted': 'Acadian population', 'question': 'Who did the British deport from Lie Saint-Jean?'}, '572f820a04bcaa1900d76a3a': {'truth': 'Île Saint-Jean (present-day Prince Edward Island)', 'predicted': 'Prince Edward Island', 'question': 'What is Lie Saint-Jean called today?'}, '572f820a04bcaa1900d76a3b': {'truth': '4,000 French troops repulsed 16,000 British', 'predicted': '4,000', 'question': 'How much were the French outnumbered at the Battle of Carillion?'}, '5728f64eaf94a219006a9e7d': {'truth': 'Chinese arts', 'predicted': '', 'question': 'What was Japanese culture influenced by?'}, '5728f64eaf94a219006a9e7f': {'truth': '1275', 'predicted': '1275–1351', 'question': 'When was Muso Soseki born?'}, '5a6a14595ce1a5001a9696da': {'truth': '', 'predicted': 'Malcolm Turnbull', 'question': \"Who is the Abbott Party's leader?\"}, '5a6a14595ce1a5001a9696dc': {'truth': '', 'predicted': 'Tony Abbott', 'question': 'Who was Giles elected to replace?'}, '5a6a14595ce1a5001a9696de': {'truth': '', 'predicted': 'Adam Giles', 'question': 'Who is Chief Minister of the Liberal Party?'}, '570d2b46b3d812140066d4de': {'truth': '\"Enlisted Reserve Corps\" and \"Officer Reserve Corps\"', 'predicted': 'Enlisted Reserve Corps\" and \"Officer Reserve Corps', 'question': 'Who filled vacancies in the Regular Army?'}, '5acecfac32bba1001ae4b4fa': {'truth': '', 'predicted': 'World War I', 'question': 'When was the National Army started?'}, '5acecfac32bba1001ae4b4fb': {'truth': '', 'predicted': 'the Regular Army, the Organized Reserve Corps, and the State Militias', 'question': 'What was added to the National Army?'}, '57261b6dec44d21400f3d8f1': {'truth': 'challenging', 'predicted': 'challenging conditions that demand repetitive manual labor', 'question': 'What type of conditions do many garment works endure?'}, '57261b6dec44d21400f3d8f2': {'truth': 'Mass-produced', 'predicted': 'Mass-produced clothing', 'question': 'What type of clothing is frequently the product of sweatshops?'}, '57261b6dec44d21400f3d8f4': {'truth': 'industrialized', 'predicted': '', 'question': 'Poor conditions found in developing countries may also be found in what type of nations?'}, '5a0cfe3af5590b0018dab6fc': {'truth': '', 'predicted': 'sweatshops', 'question': 'Here my clothing is often produced in what?'}, '57268816dd62a815002e885c': {'truth': 'the second HoHoKam Park', 'predicted': 'HoHoKam Park', 'question': 'Where was the former location in Mesa?'}, '57268816dd62a815002e885d': {'truth': '25,000 square feet', 'predicted': '25,000', 'question': 'How many square feet does Fitch Park provide?'}, '5727ba9f4b864d1900163ba3': {'truth': '13th-century', 'predicted': '13th', 'question': 'In what century did Nimbarka live?'}, '5727ba9f4b864d1900163ba6': {'truth': 'Krishna', 'predicted': 'Krishna and his consort Radha', 'question': 'Who is named as the highest entity worshiped in Dvaitadvaita?'}, '5a5e608c5bc9f4001a75af87': {'truth': '', 'predicted': 'Brahman', 'question': 'Which two categories are independent?'}, '5a5e608c5bc9f4001a75af88': {'truth': '', 'predicted': 'Krishna', 'question': \"Who is Radha's consort?\"}, '5726572af1498d1400e8dc7e': {'truth': 'indigo dye', 'predicted': 'indigo', 'question': \"why type of  dye was one of the East  India company's main  products?\"}, '5a8456db7cf838001a46a75f': {'truth': '', 'predicted': '1717', 'question': 'What year did the Mughal emperor strictly require customs duties?'}, '5a8456db7cf838001a46a760': {'truth': '', 'predicted': 'Bengal', 'question': 'What region was made available to black market traders by the Mughal emperor?'}, '5a8456db7cf838001a46a762': {'truth': '', 'predicted': 'indigo', 'question': \"What type of dye was one of the North India company's main products?\"}, '570cbdcfb3d812140066d265': {'truth': 'Nontrinitarians', 'predicted': 'Nontrinitarians, such as Unitarians, Christadelphians and Jehovah\\'s Witnesses also acknowledge Mary as the biological mother of Jesus Christ, but do not recognise Marian titles such as \"Mother of God\" as these groups generally reject Christ\\'s divinity. Since Nontrinitarian churches are typically also mortalist', 'question': \"Unittarians, Christadelphians and Jehovah's Witnesses are examples of what kind of church?\"}, '570cbdcfb3d812140066d266': {'truth': 'Marian', 'predicted': '', 'question': '\"Mother of God\" is an example of what kind of title?'}, '5ad186c5645df0001a2d1e97': {'truth': '', 'predicted': 'the biological mother of Jesus Christ', 'question': 'Who did Mary recognize herself as?'}, '5ad186c5645df0001a2d1e98': {'truth': '', 'predicted': \"Unitarians, Christadelphians and Jehovah's Witnesses\", 'question': 'Which Nontrinitarian churches once acknowledged Mary as \"Mother of God\"?'}, '5ad186c5645df0001a2d1e99': {'truth': '', 'predicted': 'Jesus Christ', 'question': 'Whom did Mary give access to the evil heredity of the human race?'}, '56e0fd33231d4119001ac54c': {'truth': 'Chen Fangyun and his colleagues', 'predicted': 'Chen Fangyun', 'question': 'Who first came up with the idea for a Chinese satellite navigation system?'}, '56e0fd33231d4119001ac54d': {'truth': 'in the 1980s', 'predicted': '1980s', 'question': 'When did Chen Fangyun come up with the idea for a satellite navigation system?'}, '5acd345807355d001abf3948': {'truth': '', 'predicted': 'three', 'question': 'The American National Space Administration said the satellite navigation system would be developed in how many steps?'}, '572ea349cb0c0d14000f13b9': {'truth': 'Municipal art galleries', 'predicted': 'Nicosia Municipal Art Centre. Municipal art galleries exist in all the main towns and there is a large and lively commercial art scene. Cyprus was due to host the international art festival Manifesta in 2006 but this was cancelled at the last minute following a dispute between the Dutch organizers of Manifesta and the Cyprus Ministry of Education and Culture', 'question': 'What types of places can Cypriots visit to experience art?'}, '57271234f1498d1400e8f321': {'truth': 'Suśrutasamhitā of Suśruta', 'predicted': 'Suśrutasamhitā', 'question': 'What is the name of the text that has information regarding Ayurveda?'}, '56f7e518aef2371900625c4c': {'truth': 'clans (ród)', 'predicted': 'clans', 'question': 'What ruled over the tribes?'}, '56f7e518aef2371900625c4d': {'truth': 'theoretically descending from a common ancestor', 'predicted': '', 'question': 'What did the clans people all have in common?'}, '56f7e518aef2371900625c4e': {'truth': 'related by blood or marriage', 'predicted': '', 'question': 'What gave them a sense of solidarity? '}, '56f7e518aef2371900625c4f': {'truth': 'grόd', 'predicted': '', 'question': 'What were stronghold called?'}, '5ad13aca645df0001a2d12d5': {'truth': '', 'predicted': 'by-election losses and defections to the breakaway Scottish Labour Party', 'question': 'What left Callaghan leading a majority government?'}, '5ad13aca645df0001a2d12d6': {'truth': '', 'predicted': 'order to govern', 'question': 'Why did Callaghan trade with the larger parties?'}, '572690d45951b619008f76cd': {'truth': 'golden Angel of Independence', 'predicted': 'Angel of Independence', 'question': 'What is the most popular icon of Mexico City?'}, '572690d45951b619008f76cf': {'truth': 'National Palace (seat of government) with the Castle of Chapultepec', 'predicted': 'the National Palace (seat of government) with the Castle of Chapultepec, the imperial residence', 'question': 'The angel of independence is over the roadway that connects what?'}, '5726a1ccf1498d1400e8e56a': {'truth': 'Agriculture and food and drink production continue to be major industries in the county, employing over 15,000 people', 'predicted': '', 'question': 'What area employs 15000 people in the couinty'}, '5726a1ccf1498d1400e8e56b': {'truth': 'Apple orchards were once plentiful, and Somerset is still a major producer of cider', 'predicted': '', 'question': 'What type of orchids used to be pleantiful '}, '5726a1ccf1498d1400e8e56c': {'truth': 'Gerber Products Company in Bridgwater is the largest producer of fruit juices in Europe, producing brands such as \"Sunny Delight\" and \"Ocean Spray', 'predicted': '', 'question': 'What area is important to the fruit juice industry '}, '5726a1ccf1498d1400e8e56d': {'truth': 'Cheddar cheese—some of which has the West Country Farmhouse Cheddar Protected Designation of Origin (PDO).', 'predicted': '', 'question': 'What area is PDO '}, '5acf76ad77cf76001a684e78': {'truth': '', 'predicted': '15,000', 'question': 'How many people work in the Apple Orchards?'}, '5707070890286e26004fc809': {'truth': 'rock', 'predicted': '', 'question': 'The Jornado painted onto what surface?'}, '5731d56fe17f3d1400422471': {'truth': 'the modern Pentecostal movement', 'predicted': 'Pentecostal', 'question': 'What modern movement began in the 20th century?'}, '5731d56fe17f3d1400422472': {'truth': 'Methodist and Wesleyan', 'predicted': '', 'question': 'What were the roots of the modern Pentecostal movement?'}, '5731d56fe17f3d1400422473': {'truth': 'the Charismatic movement', 'predicted': 'Charismatic movement', 'question': 'What movement did Pentecostalism create?'}, '5731d56fe17f3d1400422474': {'truth': 'Los Angeles', 'predicted': 'Azusa Street in Los Angeles', 'question': 'In what city did the modern Pentecostal movement begin?'}, '5728b2813acd2414000dfcf7': {'truth': 'Portuguese for \"mandolin\"', 'predicted': 'mandolin', 'question': 'What does bandolim mean?'}, '5728b2813acd2414000dfcfb': {'truth': 'over 17', 'predicted': '', 'question': 'How many active mandolin orchestras does the Madiera Island have? '}, '5ad22fced7d075001a42869a': {'truth': '', 'predicted': 'mandolin', 'question': ' What does bandolin mean?'}, '572ec004c246551400ce45f9': {'truth': 'he was now forced to withdraw further into Prussian-controlled territory', 'predicted': 'break off his invasion of Bohemia, he was now forced to withdraw further into Prussian-controlled territory', 'question': \"What was Frederick's response to the Russian invasion?\"}, '5727721add62a815002e9d04': {'truth': 'Ottoman', 'predicted': 'Ottoman Empire', 'question': 'Which empire completed its conquest of the Byzantines at the end of the 15th century?'}, '5727721add62a815002e9d06': {'truth': 'Vladislaus I', 'predicted': 'Vladislaus I of Hungary', 'question': 'Which Hungarian ruler was killed at the Battle of Varna?'}, '5727721add62a815002e9d07': {'truth': 'count John Hunyadi', 'predicted': 'John Hunyadi', 'question': 'Who was appointed regent-governor of the Kingdom of Hungary in 1446?'}, '5ad02eec77cf76001a686d5e': {'truth': '', 'predicted': 'Ottoman Empire', 'question': 'Which empire completed its conquest of the Byzantines at the end of the 14th century?'}, '5ad02eec77cf76001a686d60': {'truth': '', 'predicted': 'Vladislaus I of Hungary', 'question': 'Which Hungarian ruler was saved at the Battle of Varna?'}, '572782b6dd62a815002e9f30': {'truth': 'charter', 'predicted': 'charter amendments', 'question': 'Voters in the city approve which kind of amendment?'}, '572782b6dd62a815002e9f31': {'truth': 'Left-wing politics', 'predicted': 'Left-wing', 'question': 'What kind of politics have been strong in the municipal government?'}, '5ace451232bba1001ae4a147': {'truth': '', 'predicted': 'Debbie Dingell', 'question': 'Who is the representative for Ann Arbor in the 21st congressional district?'}, '57313b16e6313a140071cd50': {'truth': 'anti-Manchu writer', 'predicted': '', 'question': 'Who did Yongzheng behead?'}, '57313b16e6313a140071cd52': {'truth': '1723', 'predicted': '', 'question': 'When did Yongzheng ban christianity?'}, '57327ed206a3a419008aca89': {'truth': 'Latin literary texts', 'predicted': 'close study of Latin literary texts', 'question': 'How were humanist able to identify the development of humanist thought?'}, '57327ed206a3a419008aca8a': {'truth': 'Patristic literature', 'predicted': 'manuscripts of Patristic literature as well as pagan authors', 'question': 'What was included in this quest for knowledge of the belief system?'}, '57327ed206a3a419008aca8b': {'truth': 'philology', 'predicted': '', 'question': 'If your were unsure of the authenticity of an ancient text how could you verify it?'}, '5a81ff2b31013a001a335086': {'truth': '', 'predicted': 'manuscripts of Patristic literature as well as pagan authors', 'question': 'What was missing in this quest for knowledge of the belief system?'}, '5726d3475951b619008f7f26': {'truth': 'General MacArthur', 'predicted': '', 'question': 'Who refused to meet on continental United States?'}, '5726d3475951b619008f7f28': {'truth': 'General MacArthur', 'predicted': '', 'question': 'Who was not concerned about the idea of Chinese troops moving south into Korea?'}, '5726d3475951b619008f7f29': {'truth': 'the greatest slaughter', 'predicted': 'there would be the greatest slaughter', 'question': 'What was believed would happen if the Chinese entered the conflict?'}, '57318a1305b4da19006bd25c': {'truth': 'Muslim', 'predicted': '', 'question': 'Who conquered the Eastern Provinces of the Byzantine empire?'}, '57318a1305b4da19006bd25f': {'truth': 'remained a flourishing art form', 'predicted': 'flourishing art form', 'question': 'The Umayyad Dynasty made mosaic making do what in the Islamic culture?'}, '56e477998c00841900fbafa1': {'truth': \"art for art's sake\", 'predicted': 'art', 'question': \"What is a reactionary thing to limit formalism's meaning to?\"}, '56e477998c00841900fbafa2': {'truth': 'quest for perfection or originality', 'predicted': '', 'question': 'What sort of quest lacks purpose?'}, '56e477998c00841900fbafa3': {'truth': 'form', 'predicted': '', 'question': 'What ends up being reduced in quality by this quest?'}, '5acf9e8d77cf76001a68557d': {'truth': '', 'predicted': 'perfection or originality', 'question': ' What sort of quest defines purpose?'}, '5acf9e8d77cf76001a68557e': {'truth': '', 'predicted': 'originality', 'question': ' What ends up being maximized in quality by this quest?'}, '572816302ca10214002d9d9a': {'truth': \"in front of St. George's Cathedral.\", 'predicted': \"St. George's Cathedral\", 'question': 'Where was the moleben held?'}, '5726a792dd62a815002e8c1d': {'truth': '\"Crazy for You\" and \"Gambler\"', 'predicted': 'Crazy for You\" and \"Gambler', 'question': \"What is the name of Madonna's two new singles?\"}, '5726a792dd62a815002e8c1e': {'truth': '\"Into the Groove\"', 'predicted': '\"Into the Groove', 'question': 'What song did the comedy Desperately Seeking Susan promote?'}, '5726fd0f5951b619008f8424': {'truth': 'into the city.', 'predicted': 'the city', 'question': 'Where did the Russians retreat to? '}, '570d3ebefed7b91900d45d8d': {'truth': '20', 'predicted': '20%', 'question': \"What percentage of Spanish exports does Valencia's port handle?\"}, '572f02fbcb0c0d14000f1708': {'truth': 'their respective governments', 'predicted': '', 'question': 'What dispatched of The Dutch East India Company and the British East India Company?'}, '5732488d0fdd8d15006c68ec': {'truth': 'Fort Lewis', 'predicted': 'Fort Lewis, Washington', 'question': 'At what military installation was the 15th Infantry based?'}, '5732488d0fdd8d15006c68ed': {'truth': 'Kenyon Joyce', 'predicted': 'Major General Kenyon Joyce', 'question': 'In the spring of 1941, who commanded IX Corps?'}, '5732488d0fdd8d15006c68ee': {'truth': 'San Antonio', 'predicted': 'San Antonio, Texas', 'question': 'In what city was the 3rd Army based in June of 1941?'}, '57266708708984140094c4e5': {'truth': 'London, Paris and New York.', 'predicted': 'London, Paris and New York', 'question': 'What cities influenced how department stores in Germany operated? '}, '56dedfc3c65bf219000b3d9d': {'truth': 'the USC Sol Price School of Public Policy', 'predicted': 'USC Sol Price School of Public Policy', 'question': 'What school within the University of Southern California does the Schwarzenegger Institute for State and Global Policy belong to?'}, '56f8cc9a9b226e1400dd1030': {'truth': 'tourism', 'predicted': 'advent of tourism', 'question': 'What made farming less dominant in the 20th century? '}, '56f8cc9a9b226e1400dd1031': {'truth': 'because of the steep and rocky topography of the Alps', 'predicted': 'steep and rocky topography of the Alps', 'question': 'Why is pasture land limited? '}, '572ff633a23a5019007fcbbb': {'truth': '1935', 'predicted': '', 'question': 'When did Reza Shan request that Iran officially be referred to as Iran and not Persia?'}, '573192cde6313a140071d0c2': {'truth': '1989', 'predicted': '', 'question': 'When did the 3rd Indiana Jones film come out?'}, '573192cde6313a140071d0c3': {'truth': 'Indiana Jones and the Last Crusade', 'predicted': '', 'question': 'What was the 3rd Indiana Jones film called?'}, '5ad4c0485b96ef001a109f30': {'truth': '', 'predicted': '1989', 'question': 'When did the second Indiana Jones movie come out?'}, '5ad4c0485b96ef001a109f33': {'truth': '', 'predicted': 'Indiana Jones and the Last Crusade', 'question': \"What is Spielberg's most profitable movie?\"}, '5ad4c0485b96ef001a109f34': {'truth': '', 'predicted': 'Batman', 'question': \"What is Tim Burton's most profitable movie?\"}, '5ad243b2d7d075001a428a1a': {'truth': '', 'predicted': 'Paul Ekman', 'question': 'Who has agreed that emotions are discrete?'}, '5726a88e5951b619008f794f': {'truth': 'the end of the 20th century', 'predicted': 'end of the 20th century', 'question': 'When did republicanism and the capability approach arise?'}, '5726a88e5951b619008f7951': {'truth': 'The capability approach', 'predicted': 'capability approach', 'question': 'Mahbub ul Haq and Amartya Sen pioneered what approach?'}, '5a21dc2f8a6e4f001aa08faf': {'truth': '', 'predicted': 'free', 'question': 'To a liberal, what is the state of a political perspective that is not interfered with? '}, '5a21dc2f8a6e4f001aa08fb0': {'truth': '', 'predicted': 'capability approach', 'question': 'What is the approach of republicanism as developed by Mahbub ul Haq and Amartya Sen?'}, '572722c0708984140094da53': {'truth': 'a rotational motion on the rotor', 'predicted': '', 'question': 'What is the main winding on a squirrel cage motor capable of withstanding?'}, '572722c0708984140094da54': {'truth': 'series non-polarized starting capacitor', 'predicted': '', 'question': 'What sort of capacitors are used on the second winding of a squirrel cage motor?'}, '572722c0708984140094da55': {'truth': 'introduce a lead in the sinusoidal current', 'predicted': '', 'question': 'What does the capacitor on the second winding of a squirrel cage motor do?'}, '572722c0708984140094da56': {'truth': 'disconnects the capacitor', 'predicted': 'disconnects', 'question': 'What does the centrifugal switch do the capacitor when the rotor achieves operating speed?'}, '572722c0708984140094da57': {'truth': 'to the side of the motor housing', 'predicted': 'the side of the motor housing', 'question': 'Where is the start capacitor commonly mounted?'}, '5acf8ff377cf76001a685279': {'truth': '', 'predicted': 'the side of the motor housing', 'question': ' Where is the start capacitor commonly mounted?'}, '572f78f5b2c2fd1400568163': {'truth': 'machine-to-machine data', 'predicted': 'machine-to-machine data interoperability standard', 'question': 'How is XML used in enterprise database management?'}, '572f78f5b2c2fd1400568164': {'truth': 'ACID-compliant transaction processing', 'predicted': '', 'question': 'What type of processing is used in enterprise database software?'}, '57313c97e6313a140071cd6a': {'truth': '3,400 books', 'predicted': '3,400', 'question': 'How many books were in the Siku Quanshu?'}, '57313c97e6313a140071cd6b': {'truth': '36,304 volumes', 'predicted': '36,304', 'question': 'How many volumes were in the Siku Quanshu?'}, '56f754a3a6d7ea1400e171be': {'truth': 'Marcin Woźniak and colleagues', 'predicted': 'Marcin Woźniak', 'question': 'Who searched for specifically Slavic sub-group of R1a1a [M17]?'}, '5ad4c6d55b96ef001a10a01d': {'truth': '', 'predicted': 'gave a date far too old to be Slavic', 'question': 'What is the evolutionary effective mutation rate more in line with?'}, '5ad4c6d55b96ef001a10a01e': {'truth': '', 'predicted': 'M458', 'question': 'What marker did Wozniak discover?'}, '56cecf68aab44d1400b88ab7': {'truth': '7,000', 'predicted': 'over 7,000', 'question': 'How many schoolrooms collapsed in the quake?'}, '56d64a821c85041400947079': {'truth': 'legal replacements', 'predicted': 'replacements', 'question': 'What can illegal children be registered as in place of their dead siblings?'}, '5727c9722ca10214002d9633': {'truth': 'performance comparable to internal drives', 'predicted': 'performance', 'question': 'What do external portable USB hard drive disks offer?'}, '570a88724103511400d5981a': {'truth': '12,500', 'predicted': '', 'question': 'How many student graduate from the University of Houston per year?'}, '570a88724103511400d5981b': {'truth': 'in Houston', 'predicted': 'Houston', 'question': 'Where do most university graduates stay after acquiring a degree?'}, '5ad42096604f3c001a400704': {'truth': '', 'predicted': 'Houston', 'question': ' Where do most university graduates go to after acquiring a degree?'}, '57282c833acd2414000df63d': {'truth': 'turned inside out to extend', 'predicted': 'turned inside out to extend it', 'question': \"What does 'everted' mean?\"}, '57282c833acd2414000df63e': {'truth': 'pharynx', 'predicted': 'muscular pharynx', 'question': 'What part of a polychaete can be everted?'}, '57282c833acd2414000df640': {'truth': 'seizing prey, biting off pieces of vegetation, or grasping dead and decaying matter', 'predicted': 'seizing prey', 'question': 'What do annelids use jaws for?'}, '573238630fdd8d15006c6860': {'truth': 'Vestals', 'predicted': 'the Vestals', 'question': 'What group did Gratian seek the abolish?'}, '572618d0ec44d21400f3d8c3': {'truth': 'Antikythera mechanism', 'predicted': 'Antikythera', 'question': 'What is the name of the 37 gear computer which noted the motions of the Sun and Moon?'}, '572618d0ec44d21400f3d8c4': {'truth': '10th', 'predicted': '10th century', 'question': 'Until what century were similar devices like the Antikythera mechanism found?'}, '5a7e3aec70df9f001a8755c9': {'truth': '', 'predicted': 'Servius and Donatus', 'question': 'Who wrote the most about Virgil?'}, '5709720ded30961900e84162': {'truth': 'Grape juice', 'predicted': 'Grape', 'question': 'What juice is made when grapes are crushed and blended?'}, '5709720ded30961900e84163': {'truth': 'fermented', 'predicted': '', 'question': 'What kind of grapes are made into vinegar?'}, '5709720ded30961900e84164': {'truth': 'Concord grapes', 'predicted': 'purple and made from Concord grapes', 'question': 'What is the most common grape used to make juice in North America?'}, '5727aa682ca10214002d9338': {'truth': 'French alone is the official language', 'predicted': '', 'question': 'What does the French constitution state for the language in Aslace?'}, '5727aa682ca10214002d933a': {'truth': 'one in ten children', 'predicted': 'one in ten', 'question': 'With Alsatian language on the decline, what is the ration of children using the language regularly today?'}, '5a6fe7e38abb0b001a67603a': {'truth': '', 'predicted': 'one in four', 'question': 'How many children speak French?'}, '57269b6b708984140094cb76': {'truth': 'they will be soluble, dissolving into the mixture', 'predicted': 'they will be soluble', 'question': 'What happens when an alloy is mixed with a molten base?'}, '5a2052ef54a786001a36b304': {'truth': '', 'predicted': 'pure or fairly pure chemical elements', 'question': 'Mixing any to what produces an alloy'}, '5a2052ef54a786001a36b305': {'truth': '', 'predicted': 'wrought iron', 'question': 'What other metal is pure like an alloy?'}, '57105362b654c5140001f8cf': {'truth': 'middle', 'predicted': 'middle classes', 'question': 'In which class did the Enlightenment reach deepest, expressing a nationalistic tone?'}, '572818a9ff5b5019007d9d24': {'truth': 'mass grave', 'predicted': 'a mass grave of Ukrainian and Polish victims of Stalinist terror', 'question': 'At what sight was the Bykivnia meeting held?'}, '5a362317788daf001a5f8748': {'truth': '', 'predicted': 'lack of marketing facilities', 'question': 'Why did the production of woolen and pashmina shawls decline?'}, '5a362317788daf001a5f874a': {'truth': '', 'predicted': 'increased', 'question': 'Has the demand for horese-hair bangles increased or decreased?'}, '57265630708984140094c2d2': {'truth': 'Stephan Kinsella', 'predicted': '', 'question': 'Who has objected to the idea of IP because \"property\" implies scarcity?'}, '57265630708984140094c2d3': {'truth': 'tangible', 'predicted': '', 'question': 'Having no natural scarcity makes IP different from what kind of property?'}, '57265630708984140094c2d4': {'truth': 'indefinitely', 'predicted': '', 'question': 'How much can IP be duplicated without diminishing the original?'}, '572675a3dd62a815002e85b3': {'truth': '400 million', 'predicted': 'roughly 400 million', 'question': 'How many people were added to the British Empire between 1815 and 1914?'}, '5726dc97708984140094d3f6': {'truth': 'northern Soviet Union near Murmansk', 'predicted': 'Murmansk', 'question': 'Where was the sub base located?'}, '5ad28a70d7d075001a4299ba': {'truth': '', 'predicted': 'Murmansk', 'question': 'Where was the plane base located?'}, '5728a75e4b864d1900164b94': {'truth': 'four single-strings', 'predicted': 'four', 'question': 'How many strings did the Cremonese Mandolin have?'}, '5728a75e4b864d1900164b95': {'truth': 'uncomfortable to play', 'predicted': 'uncomfortable', 'question': 'Did Bortolazzi like playing the new wire strung mandolins? '}, '5728a75e4b864d1900164b96': {'truth': 'less pleasing...hard, zither-like tone', 'predicted': '', 'question': 'What did Bortolazzi say about the sound? '}, '5726e2bf708984140094d4c5': {'truth': 'prove the existence of God', 'predicted': 'to prove the existence of God and His creation of the world scientifically and through reason and logic', 'question': 'What did Avicenna hope to do through his work?'}, '5726e2bf708984140094d4c7': {'truth': 'the prophets', 'predicted': 'prophets', 'question': 'Who did Avicenna view as inspired philosophers?'}, '5acea21632bba1001ae4ae1c': {'truth': '', 'predicted': '19th century', 'question': \"Up until what century was Avicenna's work slightly influential?\"}, '572ac9cbbe1ee31400cb8256': {'truth': 'from around $165 million to as high as $3.2 billion', 'predicted': '$750 million. However, estimates have frequently varied, ranging from around $165 million to as high as $3.2 billion', 'question': \"What range of estimates have been given for Teresa Heinz Kerry's net worth?\"}, '572ac9cbbe1ee31400cb8257': {'truth': 'Rosemary Forbes Kerry', 'predicted': 'Rosemary Forbes', 'question': \"What was Kerry's mother's name?\"}, '572ac9cbbe1ee31400cb8259': {'truth': 'third-richest', 'predicted': 'third', 'question': 'Where would Kerry have ranked among richest US presidents, adjusted for inflation?'}, '572947026aef051400154c4d': {'truth': 'married Pocahontas', 'predicted': '', 'question': 'What is one of the main things John Rolfe is known for?'}, '572947026aef051400154c4e': {'truth': '1612,', 'predicted': '1612', 'question': 'When did the English begin their intentional settlement of Bermuda?'}, '572947026aef051400154c50': {'truth': 'oldest continually inhabited English town in the New World.', 'predicted': '', 'question': 'What is St. George credited with?'}, '5ad40858604f3c001a3ffee3': {'truth': '', 'predicted': 'Pocahontas', 'question': 'Who did Rolfe John marry?'}, '5726fec6dd62a815002e973f': {'truth': '15 m (50 ft)', 'predicted': '', 'question': 'How much ice and snow is minimally necessary to begin to slide on steep glaciers?'}, '5ad50a975b96ef001a10aa90': {'truth': '', 'predicted': '250 kg/m3', 'question': 'What is the range of paper weight?'}, '572f8a4904bcaa1900d76a69': {'truth': 'A-delta', 'predicted': '', 'question': 'What are some spinal cord fibers exclusive to?'}, '572f8a4904bcaa1900d76a6d': {'truth': 'somatosensory', 'predicted': 'primary and secondary somatosensory cortices', 'question': 'Pain which is distinctly located also activates what cortices? '}, '5acd38ac07355d001abf3980': {'truth': '', 'predicted': 'the brain', 'question': 'Where do pain signals travel up to from the thalamus?'}, '5acd38ac07355d001abf3981': {'truth': '', 'predicted': 'spinal cord fibers', 'question': 'What are dynamic wide range neurons?'}, '5acd38ac07355d001abf3983': {'truth': '', 'predicted': 'the motivational element of pain', 'question': 'What does the cingulate cortex anterior embody?'}, '572688895951b619008f7603': {'truth': 'he personally recommended his successor', 'predicted': 'personally recommended his successor', 'question': 'What did Jack Brickhouse do when he approached his retirement age?'}, '572745d8f1498d1400e8f591': {'truth': 'help to open doors for historically excluded groups in workplace settings and higher education', 'predicted': 'open doors for historically excluded groups in workplace settings and higher education', 'question': 'What did proponents of affirmative action claim some of the effects of policies that support affirmative action had done?'}, '572745d8f1498d1400e8f592': {'truth': 'actively seek to promote an inclusive workplace', 'predicted': '', 'question': 'What has the concept of workplace diversity caused employers to do?'}, '572745d8f1498d1400e8f593': {'truth': 'draw in talent and ideas from all segments of the population', 'predicted': 'create an environment in which there is a culture of respect for individual differences as well as the ability to draw in talent and ideas from all segments of the population. By creating this diverse workforce, these employers and companies gain a competitive advantage in an increasingly global economy', 'question': 'Having a diverse workplace allows for employers to do what?'}, '572745d8f1498d1400e8f594': {'truth': 'U.S. Equal Employment Opportunity Commission', 'predicted': 'Equal Employment Opportunity Commission', 'question': 'Which organization claims that private sector employers believe having a diverse workplace is beneficial?'}, '570d2d61b3d812140066d4ee': {'truth': 'more than $850 million', 'predicted': '$850 million', 'question': 'How much money has GE invested in renewable energy commercialization?'}, '570d2d61b3d812140066d4f0': {'truth': 'more than 4,900', 'predicted': '4,900', 'question': 'As of 2009, how many people did GE employ in its renewable energy initiatives?'}, '59d291ec2763a600182840d0': {'truth': '', 'predicted': '4,900', 'question': 'How many people did GE employ overall as of 2009?'}, '59d291ec2763a600182840d2': {'truth': '', 'predicted': '$850 million', 'question': 'How much was GE worth as of 2002?'}, '572a54e07a1753140016aeb4': {'truth': 'reminding broadcasters that analog transmitters had to be shut off by the deadline in mandatory markets', 'predicted': '', 'question': 'Why did the CTRC send out a bulletin to broadcasters?'}, '5a5535e0134fea001a0e19da': {'truth': '', 'predicted': 'August 31, 2011', 'question': 'When did the CBC decide to request an extension to convert their transmitters?'}, '5731dd32e17f3d14004224b9': {'truth': 'no-aid', 'predicted': '', 'question': 'What position do Jeffries and Ryan argue was the reason for support from a coalition of separationists?'}, '5731dd32e17f3d14004224bb': {'truth': 'most Protestants (and most Jews)', 'predicted': 'most Protestants', 'question': 'Who supported the ban against government aid to religious schools before 1970?'}, '5ad14b17645df0001a2d15e2': {'truth': '', 'predicted': 'most Protestants', 'question': 'Who supported the unban against government aid to religious schools before 1970?'}, '5726bbc15951b619008f7c59': {'truth': 'Raleigh also has two alternative high schools.', 'predicted': 'two alternative high schools', 'question': 'Does Raleigh have alternate high schools?'}, '571008d5a58dae1900cd67ea': {'truth': \"sexual orientation based on the relative amounts of heterosexual and homosexual experience or psychic response in one's history at a given time.\", 'predicted': 'sexual orientation', 'question': 'What does the Kinsey scale provide a classification for?'}, '571008d5a58dae1900cd67eb': {'truth': 'such that individuals in the same category show the same balance between the heterosexual and homosexual elements in their histories', 'predicted': 'individuals in the same category show the same balance between the heterosexual and homosexual elements in their histories', 'question': 'How does the classifiacation scheme work on the Kinsey scale?'}, '571008d5a58dae1900cd67ed': {'truth': 'the actual amount of overt experience or psychic response', 'predicted': '', 'question': 'What positions does the KInsey scale not use?'}, '57265269708984140094c253': {'truth': 'They were originally kept as songbirds, and they are thought to have been regularly used in song contests.', 'predicted': 'song contests', 'question': 'Have quails ever been used for entertainment purposes ?'}, '57265269708984140094c255': {'truth': 'seeds, insects, and other small invertebrates.', 'predicted': 'seeds, insects, and other small invertebrates', 'question': 'What is the typical diet consist of for qails ?'}, '57265269708984140094c256': {'truth': 'modern domesticated flocks are mostly of Japanese quail (Coturnix japonica)', 'predicted': 'Japanese', 'question': 'From what country do most domesticated  quails today descend from ?'}, '5a86079eb4e223001a8e73ce': {'truth': '', 'predicted': 'Japan', 'question': 'What country do all domesticated quails descend from?'}, '572b65e4be1ee31400cb835a': {'truth': 'twelve bird species', 'predicted': 'twelve', 'question': 'How many bird species have been driven to extinction in Guam?'}, '572b65e4be1ee31400cb835b': {'truth': \"ko'ko' birds\", 'predicted': '', 'question': 'What other bird was very common before WWII according to the elders?'}, '5ace640e32bba1001ae4a551': {'truth': '', 'predicted': 'brown tree snake', 'question': 'What is thought to have been the first bird species on Guam?'}, '5ace640e32bba1001ae4a552': {'truth': '', 'predicted': \"ko'ko' birds\", 'question': 'What was the most populous Guam bird prior to World War II?'}, '5ace640e32bba1001ae4a554': {'truth': '', 'predicted': 'bird', 'question': 'Which bird species was first driven to extinction by brown tree snakes?'}, '5ad3f42d604f3c001a3ff92b': {'truth': '', 'predicted': 'coronation site of Norman kings', 'question': 'What was the abbey to Norman queens?'}, '5ad3f42d604f3c001a3ff92d': {'truth': '', 'predicted': 'Richard II', 'question': 'Who was reigning when Henry Yevele started his work on the abbey?'}, '5728ee664b864d19001650b4': {'truth': 'Yuan', 'predicted': 'Yuan dynasty', 'question': 'Which Chinese dynasty was founded by Mongols?'}, '5730501a396df91900096052': {'truth': 'The Boulton Paul Defiant', 'predicted': 'Boulton Paul Defiant', 'question': 'What performed better during night fighting?'}, '5730501a396df91900096053': {'truth': 'engage the unsuspecting German bomber from beneath', 'predicted': 'its configuration of four machine guns in a turret could (much like German night fighters in 1943–1945 with Schräge Musik) engage the unsuspecting German bomber from beneath', 'question': 'How could aircraft engage bombers when fitted with a turret?'}, '5728b1163acd2414000dfcd3': {'truth': 'The Dubliners,', 'predicted': 'The Dubliners', 'question': \"What was John Sheahan and Barney Mckenna's band called?\"}, '5728b1163acd2414000dfcd4': {'truth': 'UK luthier Roger Bucknall of Fylde Guitars', 'predicted': 'Roger Bucknall', 'question': 'Who made the instruments used by the Dubliners?'}, '5ad22b4bd7d075001a4285ee': {'truth': '', 'predicted': 'fiddle player and tenor banjo player with The Dubliners', 'question': \" What was John Sheahan and Barney Mckenna's job called?\"}, '5ad22b4bd7d075001a4285f0': {'truth': '', 'predicted': 'Rory Gallagher', 'question': ' What Irish guitarist played the mandolin off stage?'}, '570a6e2f4103511400d596fa': {'truth': 'terror', 'predicted': '', 'question': 'What is an example of an extreme form of fear?'}, '570a6e2f4103511400d596fb': {'truth': 'embarrassment', 'predicted': '', 'question': 'What would be an example of mild shame?'}, '5ad24351d7d075001a4289e4': {'truth': '', 'predicted': 'Moods', 'question': ' What are intense feelings that lack a contextual stimulus called?'}, '570f40f65ab6b81900390eb5': {'truth': 'Melatonin', 'predicted': '', 'question': 'At its onset, what can be measured in blood or saliva?'}, '570f40f65ab6b81900390eb6': {'truth': 'hormone', 'predicted': 'hormone in the blood or saliva', 'question': 'The presence of what is a circadian marker?'}, '572a630e7a1753140016aefa': {'truth': 'a \"lifetime of looking beyond the horizon\"', 'predicted': '\"lifetime of looking beyond the horizon', 'question': \"What was the reason given for Hayek's 1991 award from the President?\"}, '5726a4465951b619008f78c0': {'truth': '21', 'predicted': '11 of the 21', 'question': 'How many national airlines are there in Mexico?'}, '5726f3eef1498d1400e8f0c7': {'truth': '3500 BC', 'predicted': '', 'question': 'How far back do the Mesopotamian people go?'}, '572fa91e04bcaa1900d76b68': {'truth': 'between the cytoplasm and the periplasmic space', 'predicted': 'across the cell membrane between the cytoplasm and the periplasmic space', 'question': 'How does electron transit occur in bacteria?'}, '5731e810e17f3d1400422537': {'truth': 'Abdu El Hamouli, Almaz and Mahmoud Osman, who influenced the later work of Sayed Darwish, Umm Kulthum, Mohammed Abdel Wahab and Abdel Halim Hafez', 'predicted': 'Sayed Darwish, Umm Kulthum, Mohammed Abdel Wahab and Abdel Halim Hafez', 'question': 'What artist are considered the golden age of Egyptian music?'}, '56e7b14c37bdd419002c4370': {'truth': '2016', 'predicted': '2014 scheduled beginning proved to be too ambitious for the group; its official website now cites an anticipated beginning of professional play in 2016', 'question': 'When does the CAFL plan on start its first season?'}, '56e7b14c37bdd419002c4373': {'truth': 'November', 'predicted': 'November, 2015', 'question': 'In what month did the CAFL tournament occur?'}, '5733f0774776f4190066156d': {'truth': 'The Council of Ministers', 'predicted': '', 'question': 'What group acts as the presidential cabinet?'}, '5733f0774776f4190066156f': {'truth': 'an absolute majority of deputies', 'predicted': '', 'question': \"What is needed to reject a cabinet's policy?\"}, '5727e243ff5b5019007d977b': {'truth': \"Oklahoma City's Ford Center\", 'predicted': \"Oklahoma City's Ford Center, now known as Chesapeake Energy Arena, for two seasons following Hurricane Katrina in 2005. In July 2008, the Seattle SuperSonics, a franchise owned by the Professional Basketball Club LLC, a group of Oklahoma City businessmen led by Clayton Bennett, relocated to Oklahoma City and announced that play would begin at the Ford Center\", 'question': 'Where did the temporary NBA team play in Oklahoma?'}, '56cff6f3234ae51400d9c192': {'truth': 'national modes and idioms', 'predicted': 'use of national modes and idioms', 'question': \"Many people were considered influenced by Chopin's what?\"}, '57296ab01d046914007793e8': {'truth': 'bulk metallic glasses', 'predicted': 'bulk metallic glasses (BMG)', 'question': 'What are thick alloys made in layers called?'}, '57296ab01d046914007793e9': {'truth': 'zirconium', 'predicted': '', 'question': 'What does Liquidmetal Technologies use for their alloys?'}, '57296ab01d046914007793ea': {'truth': 'amorphous steel', 'predicted': '', 'question': 'What type of metal makes better alloys than traditional steel?'}, '5a67151af038b7001ab0c1c8': {'truth': '', 'predicted': 'zirconium-based BMGs', 'question': 'What does Liquidmetal Technologies use for their cooling?'}, '5a67151af038b7001ab0c1ca': {'truth': '', 'predicted': 'Caltech', 'question': 'At what university did Klement produce BMGs?'}, '5acfc92477cf76001a685f88': {'truth': '', 'predicted': 'Cubist', 'question': 'What style is The Desmoiselles?'}, '56dcf8689a695914005b94a1': {'truth': 'Congolese Observatory of Human Rights', 'predicted': 'the Congolese Observatory of Human Rights', 'question': 'Which group provided oversight for the electoral process in 2009?'}, '56dcf8689a695914005b94a2': {'truth': 'very low', 'predicted': 'very low\" turnout', 'question': 'What kind of turnout did the Congolese Observatory of Human Rights experience in the 2009 elections?'}, '5ad00a9d77cf76001a6867e0': {'truth': '', 'predicted': 'presidential election', 'question': 'What election did Sassou lose?'}, '5ad00a9d77cf76001a6867e1': {'truth': '', 'predicted': '2009', 'question': 'What year did Sassou lose re-election?'}, '5ad00a9d77cf76001a6867e3': {'truth': '', 'predicted': 'the Congolese Observatory of Human Rights', 'question': 'What governmental organization commented on the election?'}, '56f78981aef2371900625baa': {'truth': 'the German Empire', 'predicted': 'Spain sold the islands to the German Empire', 'question': 'In 1884, which country purchased the Marshall Islands?'}, '56f952ba9b226e1400dd1313': {'truth': 'the German Empire', 'predicted': 'German Empire', 'question': 'Who bought the Marshall Islands from the Spanish in 1884?'}, '56f952ba9b226e1400dd1314': {'truth': 'Japan', 'predicted': 'Empire of Japan', 'question': 'Who occupied the Marshall Islands during the First World War?'}, '56cfebbd234ae51400d9c0c7': {'truth': '30% (4.65 EJ/yr)', 'predicted': '30%', 'question': 'How much energy does an HVAC system use in commercial locations?'}, '5728bf3a2ca10214002da6d2': {'truth': 'Île de la Cité', 'predicted': 'The Île de la Cité', 'question': 'What was the site of the royal palace in the 12th century?'}, '5730eef6a5e9cc1400cdbb07': {'truth': 'unstressed /e/ and /a/ following palatalized consonants and preceding a stressed syllable', 'predicted': '', 'question': 'What is pronounced [a] in Southern Russian?'}, '5730eef6a5e9cc1400cdbb08': {'truth': 'unstressed /e/ and /a/ following palatalized consonants and preceding a stressed syllable', 'predicted': '', 'question': \"What is pronounced [ɪ] in Moscow's dialect?\"}, '5730eef6a5e9cc1400cdbb09': {'truth': 'modern Belarusian and some dialects of Ukrainian (Eastern Polesian)', 'predicted': '', 'question': 'What does Southern Russian have a linguistic continuum with?'}, '5a864143b4e223001a8e7509': {'truth': '', 'predicted': 'akanye', 'question': 'What is called yakanye in Standard and Northern dialects?'}, '572808cd2ca10214002d9c22': {'truth': 'Sunday, 27 April 2014', 'predicted': '27 April 2014', 'question': 'On what date was John XXIII and Pope John Paul II declared saints?'}, '5a61447ae9e1cc001a33d02e': {'truth': '', 'predicted': '3 June 2013', 'question': \"When was the 50th anniversary of Pope Francis's death?\"}, '5a61447ae9e1cc001a33d02f': {'truth': '', 'predicted': 'Bergamo', 'question': 'Where is Pope Francis originally from?'}, '5a61447ae9e1cc001a33d030': {'truth': '', 'predicted': 'Pope John XXIII', 'question': 'What was approved by Pope John Paul II on July 5, 2013 for Pope Francis?'}, '56dfbe7c231d4119001abd71': {'truth': 'Asynchronous Transfer Mode', 'predicted': 'ATM (Asynchronous Transfer Mode', 'question': 'what does atm stand for in relation to internet providers? '}, '56dfbe7c231d4119001abd72': {'truth': 'customers with more demanding requirements', 'predicted': '', 'question': 'what is high-speed dsl used for? '}, '5a10d9ce06e79900185c3420': {'truth': '', 'predicted': 'ATM', 'question': \"What's the abbreviation for synchronous transfer mode\"}, '5a10d9ce06e79900185c3421': {'truth': '', 'predicted': 'SONET', 'question': 'What are the abbreviations for asynchronous optical networking'}, '572e8519c246551400ce42b1': {'truth': 'Tyneside docks', 'predicted': 'for use at the Tyneside docks for loading cargo', 'question': 'Were was the hydraulic crane initially used?'}, '572e8519c246551400ce42b3': {'truth': 'encased inside a vertical cylinder', 'predicted': 'vertical cylinder', 'question': 'The water pump supplied water pressure to a plunger located where?'}, '572fa79aa23a5019007fc83c': {'truth': 'October 2, 2008', 'predicted': '2008', 'question': 'When was the only vice presidential debate held at Washington University?'}, '572fa79aa23a5019007fc83d': {'truth': 'Republican Sarah Palin and Democrat Joe Biden', 'predicted': 'Sarah Palin and Democrat Joe Biden', 'question': 'Who were the candidates in the vice presidential debate at Washington University?'}, '5725eaf7271a42140099d31b': {'truth': 'two ways', 'predicted': 'two', 'question': 'In how many ways can one define luminous efficacy of a light source?'}, '5725c71189a1e219009abe88': {'truth': 'The Reinvent the Toilet Challenge is a long-term research and development effort to develop a hygienic, stand-alone toilet', 'predicted': '', 'question': 'What is reinvent the toilet trying to develop'}, '5725c71189a1e219009abe89': {'truth': 'This challenge is being complemented by another investment program to develop new technologies for improved pit latrine emptying', 'predicted': '', 'question': 'What compliments the challenge '}, '5725c71189a1e219009abe8a': {'truth': 'The aim of the \"Omni Processor\" is to convert excreta (for example fecal sludge) into beneficial products such as energy and soil nutrients', 'predicted': '', 'question': 'What does the Omni processor do'}, '572734eb5951b619008f86b6': {'truth': '1919–20.', 'predicted': '', 'question': 'what year did competition resume after world war 1?'}, '572734eb5951b619008f86b7': {'truth': '1922–23', 'predicted': '', 'question': 'When did Wembly stadium open? '}, '572734eb5951b619008f86b9': {'truth': \"didn't celebrate its centenary year until 1980–81\", 'predicted': '', 'question': 'When did the competition celebrate its centennial? '}, '5a8c802dfd22b3001a8d8954': {'truth': '', 'predicted': 'Ricky Villa', 'question': 'Who replaced the goal scored by Steven Gerrard?'}, '5735c0d8e853931400426b49': {'truth': '1955', 'predicted': '1906–1955', 'question': 'When did Tribhuvan die?'}, '5735c0d8e853931400426b4a': {'truth': '1920', 'predicted': '1920–1972', 'question': 'What was the birth year of King Mahendra?'}, '5735c0d8e853931400426b4d': {'truth': 'medieval', 'predicted': '', 'question': 'During what era was the Hanumandhoka Palace constructed?'}, '572830622ca10214002da032': {'truth': 'Charles Darwin', 'predicted': '', 'question': 'Who published a book about worms in 1881?'}, '572830622ca10214002da034': {'truth': 'generally in moist leaf litter', 'predicted': 'moist leaf litter', 'question': 'Where do earthworms prefer to live on the surface?'}, '572830622ca10214002da036': {'truth': 'storks', 'predicted': 'robins to storks', 'question': 'What is the largest bird that eats earthworms?'}, '5ace8a7232bba1001ae4a9db': {'truth': '', 'predicted': 'moist leaf litter', 'question': 'Where do earthworms prefer to live in space?'}, '5ace8a7232bba1001ae4a9dc': {'truth': '', 'predicted': 'loosen the soil so that oxygen and water can penetrate it', 'question': \"How does burrowers' tightening help the soil?\"}, '56ddfe9d4396321400ee2539': {'truth': 'three', 'predicted': '', 'question': \"Rather than four-year Bachelor's degrees, Politeknik offer a diploma after how many years?\"}, '56e79c1c00c9c71400d773a1': {'truth': 'a humid subtropical climate', 'predicted': 'humid subtropical', 'question': 'What type of climate does Nanjing enjoy?'}, '56e79c1c00c9c71400d773a2': {'truth': 'the East Asian monsoon', 'predicted': 'East Asian', 'question': 'What monsoon affects Nanjing?'}, '56e79c1c00c9c71400d773a4': {'truth': '115 days', 'predicted': '115', 'question': 'How many days of rain does Nanjing get a year, on average?'}, '56e79c1c00c9c71400d773a5': {'truth': '1,983 hours', 'predicted': '1,983', 'question': 'How many hours of bright sunshine does Nanjing get each year?'}, '570ac16f4103511400d5998d': {'truth': 'heavily laden aircraft', 'predicted': 'aircraft', 'question': 'What cannot launch using a ski-jump due to their high loaded weight?'}, '5acd84c307355d001abf453a': {'truth': '', 'predicted': 'penalty it exacts on aircraft size, payload, and fuel load', 'question': 'What is the advantage of the ski-jump?'}, '572901d03f37b31900477f73': {'truth': '75', 'predicted': '75%', 'question': \"What percentage of France's Jewish population survived the holocaust?\"}, '56cdd83862d2951400fa68e3': {'truth': '2013', 'predicted': '', 'question': 'In what year were rights to Spectre worked out?'}, '56cf39c4aab44d1400b88ebb': {'truth': 'Spectre', 'predicted': '', 'question': 'How was the Spectre acronym originally written?'}, '5ad22870d7d075001a42855b': {'truth': '', 'predicted': 'Danjaq, LLC', 'question': 'Who is a brother company to Eon Productions?'}, '57267c12dd62a815002e86c6': {'truth': 'the First World War', 'predicted': 'First World War', 'question': \"The passed Home Rule Bill wasn't implemented because of which war?\"}, '5731edafe17f3d140042255b': {'truth': 'General Baptists', 'predicted': '', 'question': \"Who believed that Christ's atonement was for everyone?\"}, '57321a39e99e3014001e651c': {'truth': 'since prehistoric times', 'predicted': 'prehistoric times', 'question': 'When were birds represented in early cave paintings?'}, '57321a39e99e3014001e6520': {'truth': 'The Rime of the Ancient Mariner', 'predicted': \"Samuel Taylor Coleridge's The Rime of the Ancient Mariner\", 'question': 'The relationship between an albatross and a sailor is the central theme of what book?'}, '5731f0ffe99e3014001e63ed': {'truth': 'Decius Mus', 'predicted': 'Decius', 'question': 'What Roman general had a dream of his fate in battle?'}, '5731f0ffe99e3014001e63ee': {'truth': 'Roman gladiator', 'predicted': 'the Roman gladiator', 'question': 'With what does the devotio link to military ethics?'}, '5731f0ffe99e3014001e63ef': {'truth': 'disastrous consequences', 'predicted': '', 'question': 'By dying what did Decius avoid for the battle?'}, '5731f0ffe99e3014001e63f0': {'truth': 'Livy', 'predicted': '', 'question': 'Who wrote a detailed account of the demise of Decius Mus?'}, '57324b39b9d445190005e9d1': {'truth': 'an Armenian-styled barbecue', 'predicted': 'barbecue', 'question': 'What is khorovats?'}, '57324b39b9d445190005e9d2': {'truth': 'Armenian flat bread', 'predicted': 'Lavash is a very popular Armenian flat bread', 'question': 'What is lavash?'}, '57324b39b9d445190005e9d3': {'truth': 'a popular dessert made from filo dough', 'predicted': 'dessert made from filo dough', 'question': 'What is paklava?'}, '57324b39b9d445190005e9d4': {'truth': 'a skewer of marinated roasted meat and vegetables', 'predicted': 'skewer of marinated roasted meat and vegetables', 'question': 'What is a kabob?'}, '5a53f1c6bdaabd001a386814': {'truth': '', 'predicted': 'cornelian cherries', 'question': 'What is the favorite fruit that is part of Armenian-style barbecue?'}, '5a53f1c6bdaabd001a386817': {'truth': '', 'predicted': 'Prunus armeniaca, also known as Armenian Plum) have been grown in Armenia for centuries and have a reputation for having an especially good flavor. Peaches', 'question': 'What is a dish called that uses fruits, grapes and figs?'}, '5a53f1c6bdaabd001a386818': {'truth': '', 'predicted': 'cabbage', 'question': 'What is a popular grape leaf in Armenia?'}, '5727f9a14b864d190016410c': {'truth': 'early 1930s', 'predicted': '', 'question': 'When was the Western Electric System introduced?'}, '5727f9a14b864d190016410d': {'truth': 'Bell Telephone Laboratories and Western Electric', 'predicted': 'Bell Telephone Laboratories', 'question': 'What two companies worked together to develop the Western Electric System?'}, '5727f9a14b864d190016410e': {'truth': 'Western Electric system', 'predicted': '', 'question': 'What system was used by Warner Brothers?'}, '5727f9a14b864d1900164110': {'truth': 'improve the overall quality', 'predicted': 'improve the overall quality of disc recording and playback', 'question': 'What was the Western Electric System believed to do?'}, '572a31481d0469140077982f': {'truth': 'nine Digimon movies', 'predicted': 'nine', 'question': 'How many Digimon movies have been released in Japan?'}, '572a31481d04691400779831': {'truth': 'October 6, 2000', 'predicted': '2000', 'question': 'What year was Digimon: The movie released in the US/Canada?'}, '5a110ff906e79900185c3515': {'truth': '', 'predicted': 'nine', 'question': 'How many Digimon movies have been released in the United States?'}, '56de2d84cffd8e1900b4b62c': {'truth': 'not', 'predicted': 'a building is not truly a work of architecture unless it is in some way \"adorned\"', 'question': 'Does Ruskin believe all buildings are works of architecture?'}, '56e4731e8c00841900fbaf94': {'truth': 'it is in some way \"adorned\"', 'predicted': 'it is in some way \"adorned', 'question': \"To be true architecture in Ruskin's opinion what should be done to a structure?\"}, '5acf9cb577cf76001a6854df': {'truth': '', 'predicted': 'string courses or rustication', 'question': 'What features at maximum did Ruskin insist on for a building to be considered functional?'}, '5725ee0a38643c19005aceab': {'truth': 'infrared radiation', 'predicted': 'infrared', 'question': 'What type of radiation makes up the majority of tungsten filament emissions?'}, '5725ee0a38643c19005aceac': {'truth': 'the light emitted does not appear white,', 'predicted': '', 'question': 'What is the flaw in the color of light produced by an incandescent bulb?'}, '5725c64b271a42140099d190': {'truth': 'The grants were in the order of 400,000 USD for their first phase, followed by typically 1-3 million USD for their second phase', 'predicted': '400,000 USD', 'question': 'How much were the grants for '}, '5725c64b271a42140099d191': {'truth': 'many of them investigated resource recovery or processing technologies for excreta or fecal sludge.', 'predicted': 'resource recovery or processing technologies for excreta or fecal sludge', 'question': 'What did many investigate '}, '5a0cc174f5590b0018dab542': {'truth': '', 'predicted': 'resource recovery or processing technologies for excreta or fecal sludge', 'question': 'What did the Reinvent the Toilet Challenge investigate?'}, '5a0cc174f5590b0018dab544': {'truth': '', 'predicted': '400,000 USD', 'question': 'After receiving 1-3 million for the first phase how much were the grants for the second phase?'}, '5a0cc174f5590b0018dab545': {'truth': '', 'predicted': 'more than a dozen', 'question': 'How many teams received grants to help universities?'}, '5731c5ba0fdd8d15006c6520': {'truth': '2010', 'predicted': '', 'question': \"When was 'The Pacific' released?\"}, '57293d583f37b3190047816e': {'truth': '19 percent', 'predicted': '', 'question': 'Renewables contributed what percentage to our energy consumption?'}, '57293d583f37b3190047816f': {'truth': '22 percent', 'predicted': '', 'question': 'Renewables contributed what percentage to our electricity generation?'}, '5ad1248d645df0001a2d0f21': {'truth': '', 'predicted': '1,360 GW', 'question': 'By the beginning of 2011, total renewable power capacity worldwide exceeded what number?'}, '5ad27e04d7d075001a4296f6': {'truth': '', 'predicted': 'Detroit techno music', 'question': 'What is Juan House an originator of?'}, '5ad27e04d7d075001a4296f7': {'truth': '', 'predicted': 'the exclusive association of particular tracks with particular clubs and DJs', 'question': 'What did House claim the term house Atkins reflected?'}, '5ad27e04d7d075001a4296f8': {'truth': '', 'predicted': 'to maintain such exclusives', 'question': 'Why were DJs inspired to create their own Atkins records?'}, '5ad27e04d7d075001a4296f9': {'truth': '', 'predicted': 'Juan Atkins', 'question': 'Who was the originator of DJ music?'}, '5ad27e04d7d075001a4296fa': {'truth': '', 'predicted': 'house\" records', 'question': 'What was Juan Atkins inspired to create?'}, '570e437d0dc6ce1900204eeb': {'truth': 'Martin Heinrich Klaproth', 'predicted': '', 'question': 'Who discovered uranium?'}, '570e437d0dc6ce1900204eec': {'truth': 'Berlin', 'predicted': '', 'question': 'In what city was uranium discovered?'}, '570e437d0dc6ce1900204eed': {'truth': '1789', 'predicted': '', 'question': 'In what year did the discovery of uranium occur?'}, '570e437d0dc6ce1900204eef': {'truth': 'William Herschel', 'predicted': '', 'question': 'Who discovered the planet Uranus?'}, '5ad1171b645df0001a2d0d2b': {'truth': '', 'predicted': 'yellow compound', 'question': 'What did Klaproth definitely create when he dissolved pitchblende in nitric acid?'}, '56e6d988de9d371400068085': {'truth': 'urban', 'predicted': 'chart-based CHR, along with the top 40, urban and even Latino', 'question': 'Along with CHR, Top 40 and Latino, what format have former AC stations transitioned to?'}, '5729667c3f37b3190047833b': {'truth': 'anarchists and the Church, specifically the Archbishop of Palermo', 'predicted': 'anarchists and the Church', 'question': 'Who was blamed for the week long rebellion of 1866?'}, '5a3ea39b5a76c5001a3a83f0': {'truth': '', 'predicted': 'Sicilians', 'question': 'Who prefered the Savoia kingdom to independence?'}, '5a3ea39b5a76c5001a3a83f3': {'truth': '', 'predicted': 'Italian', 'question': 'What government enacted pro-sicilian policies?'}, '572e7aa8dfa6aa1500f8d00d': {'truth': 'Canadian', 'predicted': '', 'question': 'Which North American version of football calls for 12 player per side on the field?'}, '572e7aa8dfa6aa1500f8d00f': {'truth': 'American', 'predicted': '', 'question': 'Which version of North American football has smaller end zones?'}, '5a0dcbf06e16420018587b3a': {'truth': '', 'predicted': 'neutral zone', 'question': 'In America what term is used for both Canadian foobal and Americam football'}, '5a0dcbf06e16420018587b3b': {'truth': '', 'predicted': '10 yards', 'question': 'An American football field is how much wider than a Canadian football field?'}, '5a0dcbf06e16420018587b3c': {'truth': '', 'predicted': '12 players', 'question': 'Canadian and American football have the same number of what on the field?'}, '5a0dcbf06e16420018587b3d': {'truth': '', 'predicted': 'gain 10 yards', 'question': 'American football players have three downs to do what?'}, '5a0f39cfdecec900184754ed': {'truth': '', 'predicted': 'three', 'question': 'In American football how many downs are needed to gain 10 yards?'}, '56df4fb48bc80c19004e4a60': {'truth': 'South Oklahoma City', 'predicted': '', 'question': 'Which side is known for primarily being industrial?'}, '572f39d804bcaa1900d7679d': {'truth': 'distribution of seeds', 'predicted': 'seeds', 'question': 'What are animals also a part of?'}, '572f39d804bcaa1900d7679f': {'truth': 'seed-dispersal', 'predicted': '', 'question': 'What does a plant get out of forming fruit?'}, '572f39d804bcaa1900d767a0': {'truth': 'fragile', 'predicted': 'too fragile', 'question': 'What are many mutualistic relationships, thus failing to survive competition? '}, '5a3ad7933ff257001ab842b4': {'truth': '', 'predicted': 'land plant life', 'question': 'What did mutualistic relationships spread to eventually become?'}, '5a3ad7933ff257001ab842b7': {'truth': '', 'predicted': 'Fruit', 'question': 'What did accidentally scattering seeds help flower parts evolve to form?'}, '56df84d756340a1900b29cd3': {'truth': 'the public Horace Mann School for the Deaf', 'predicted': 'Horace Mann School for the Deaf', 'question': 'What name does the Boston School for Deaf Mutes go by now?'}, '56df84d756340a1900b29cd4': {'truth': 'April', 'predicted': 'April 1871', 'question': 'What month did Bell go to Boston?'}, '56df84d756340a1900b29cd5': {'truth': 'instructors', 'predicted': \"Fuller's instructors, but he declined the post in favor of his son. Traveling to Boston in April 1871, Bell proved successful in training the school's instructors\", 'question': 'Bell trained who in Boston?'}, '56df84d756340a1900b29cd6': {'truth': 'Hartford', 'predicted': 'Hartford, Connecticut', 'question': 'What city was the American Asylum in?'}, '5726c1825951b619008f7d52': {'truth': 'approximately two hundred square miles', 'predicted': '', 'question': 'During the ice age, what area of land was above water?'}, '5726c1825951b619008f7d53': {'truth': 'The top of the seamount has gone through periods of complete submergence', 'predicted': '', 'question': 'Has the seamount always been above sealevel?'}, '572931113f37b319004780c9': {'truth': 'a submarine volcano', 'predicted': 'submarine volcano', 'question': 'What type of volcano forms the archipelago?'}, '572931113f37b319004780ca': {'truth': 'periods of complete submergence,', 'predicted': '', 'question': 'Why is the top of the seamount formed by marine organisms?'}, '572931113f37b319004780cb': {'truth': 'an island', 'predicted': '', 'question': 'What was the result of the whole cladera being above sea level during the Ice Ages?'}, '5ad3e9c6604f3c001a3ff6a3': {'truth': '', 'predicted': 'Ice Ages', 'question': 'When was the caldera partially above sea level?'}, '5ad3e9c6604f3c001a3ff6a4': {'truth': '', 'predicted': 'a range', 'question': 'What is the volcano part of?'}, '57279543f1498d1400e8fcc7': {'truth': 'commune, canton and federal levels', 'predicted': 'the commune, canton and federal levels', 'question': 'What are the legal jurisdictions that Swiss citizens are subject to?'}, '57279543f1498d1400e8fcc8': {'truth': 'direct', 'predicted': 'direct democracy', 'question': 'What type of democracy was defined in the 1848 federal constitution?'}, '5730187d947a6a140053d0d2': {'truth': 'Sabino Creek', 'predicted': '', 'question': 'What is the Foothills west of?'}, '5730187d947a6a140053d0d4': {'truth': 'Oracle Road', 'predicted': '', 'question': 'What is the Foothills east of?'}, '5730187d947a6a140053d0d5': {'truth': 'upscale outdoor shopping mall', 'predicted': 'an upscale outdoor shopping mall', 'question': 'What is La Encantada?'}, '573428154776f419006619b6': {'truth': 'Catalina Foothills', 'predicted': 'on the north side is the suburban community of Catalina Foothills', 'question': 'Where are the most expensive homes in the Tucson metro area?'}, '573428154776f419006619b7': {'truth': 'River Road', 'predicted': 'north of River Road', 'question': 'What is the southern edge of the Catalina Foothills area?'}, '5ad2e74a604f3c001a3fd95d': {'truth': '', 'predicted': 'French kings', 'question': 'Who reasserted the earliest influence of temporal subjects?'}, '572a18b96aef051400155266': {'truth': 'a growing papyrus sprout', 'predicted': 'papyrus sprout', 'question': 'What is the ancient Egyptian hieroglyph for green?'}, '5a74e30342eae6001a389b0d': {'truth': '', 'predicted': 'Osiris', 'question': 'Who ruled the ancient Egyptians?'}, '56d0875b234ae51400d9c34a': {'truth': 'produce cucumbers year-round for the Roman emperor Tiberius', 'predicted': 'to produce cucumbers year-round for the Roman emperor Tiberius', 'question': 'What was one of the first uses of a greenhouse?'}, '572eb9a703f98919007569a8': {'truth': 'metaphysics', 'predicted': 'metaphysics and physics, and advised against empirically probing certain matters on which \"physics is silent and will remain so,\" such as the doctrine of \"creation from nothing\"', 'question': 'Salam suggests physics and science be kept separate from which topics which are more suited to religion?'}, '5ad2292fd7d075001a42857f': {'truth': '', 'predicted': 'metaphysics and physics, and advised against empirically probing certain matters on which \"physics is silent and will remain so,\" such as the doctrine of \"creation from nothing\"', 'question': 'Salam suggests physics and science be kept together from which topics which are more suited to religion?'}, '5ad2292fd7d075001a428580': {'truth': '', 'predicted': 'metaphysics and physics, and advised against empirically probing certain matters on which \"physics is silent and will remain so,\" such as the doctrine of \"creation from nothing', 'question': 'Salam suggests physics and science be kept separate from which topics which are less suited to religion?'}, '5727c25f2ca10214002d9594': {'truth': 'politicians, judges and academics', 'predicted': 'politicians, judges and academics, the rule of law has been described as \"an exceedingly elusive notion\". Among modern legal theorists', 'question': 'Who commonly, or at least attempt to, abide by the rule of law?'}, '5727c25f2ca10214002d9595': {'truth': 'specific procedural attributes', 'predicted': 'specific procedural attributes that a legal framework must have in order to be in compliance with the rule of law', 'question': 'On what do aspects of the rule of law do formalist definitions focus?'}, '5727c25f2ca10214002d9596': {'truth': 'functional', 'predicted': '', 'question': 'What is the third and lesser referred to approach on defining the rule of law?'}, '56e10245e3433e1400422a98': {'truth': 'timing of 0.2 microseconds, and speed of 0.2 meters/second', 'predicted': '0.2 meters/second', 'question': 'What was the timing and speed that China promised to offer in 2008 with the BeiDou system?'}, '5728d9e23acd2414000e0032': {'truth': 'terrorism and building an ownership society', 'predicted': 'defending America against terrorism and building an ownership society', 'question': 'Which two topics did Bush remain steadfast on, during his campaign?'}, '5a7224040efcfe001a8afe48': {'truth': '', 'predicted': 'September 2, 2004', 'question': 'When did Cheney accept the nomination?'}, '5a7224040efcfe001a8afe4a': {'truth': '', 'predicted': 'defending America against terrorism and building an ownership society', 'question': 'What two themes did Cheney speak about often during his campaign?'}, '5a7224040efcfe001a8afe4b': {'truth': '', 'predicted': 'defending America against terrorism and building an ownership society', 'question': 'What was one idea endorsed by Cheney in his 2004 speech?'}, '5732a7f71d5d2e14009ff87e': {'truth': 'Tethys Sea', 'predicted': '', 'question': 'The Mediterranean sea is a remaining part of which sea from the Eocene?'}, '5732a7f71d5d2e14009ff87f': {'truth': 'the Himalayan orogeny', 'predicted': 'Himalayan orogeny', 'question': 'Which oregeny was created when India collided with Asia?'}, '56e8648f37bdd419002c44eb': {'truth': 'Zytglogge', 'predicted': 'Zytglogge tower', 'question': 'What was the name of the tower that was the western boundary?'}, '56e8648f37bdd419002c44ec': {'truth': 'Käfigturm', 'predicted': 'the Käfigturm', 'question': 'What tower took over after Zytglogge?'}, '56e8648f37bdd419002c44ee': {'truth': 'the whole area of the peninsula', 'predicted': 'whole area of the peninsula', 'question': 'What did the big and small Schanze protect?'}, '5728ddb1ff5b5019007da885': {'truth': 'committed troops to the Korean War and attempted to ban the Communist Party of Australia in an unsuccessful referendum during the course of that war', 'predicted': 'committed troops to the Korean War', 'question': \"What actions showed Menzies' anti-Communist beliefs?\"}, '5728ddb1ff5b5019007da886': {'truth': 'concerns about the influence of the Communist Party over the Trade Union movement,', 'predicted': 'concerns about the influence of the Communist Party over the Trade Union movement', 'question': 'Over what did the Labor party divide?'}, '5a6a5f32a9e0c9001a4e9db7': {'truth': '', 'predicted': 'atomic bomb', 'question': 'What actions showed Stalin anti-Communist beliefs?'}, '572b85aef75d5e190021fe20': {'truth': 'plant sources', 'predicted': '', 'question': 'What is one way that vegetarians and vegans obtain zinc?'}, '572b85aef75d5e190021fe22': {'truth': 'seeds and cereal bran,', 'predicted': 'seeds and cereal bran', 'question': 'Where is zinc chelator phytate found?'}, '572b85aef75d5e190021fe23': {'truth': 'diet is high in phytates,', 'predicted': '', 'question': 'What kind of diet may require more than 15mg of zinc daily?'}, '572acbfe111d821400f38d7f': {'truth': 'Tel Aviv', 'predicted': 'Tel Aviv and Florence', 'question': 'What city in Israel is a sister city to Philadelphia?'}, '572acbfe111d821400f38d81': {'truth': 'Copernicus monument', 'predicted': 'the Copernicus monument', 'question': 'What else does the Triangle contain?'}, '5726ba315951b619008f7c04': {'truth': 'too soft', 'predicted': '', 'question': 'Why was tin was rarely used for everyday use?'}, '57277d20f1498d1400e8f991': {'truth': 'Yale', 'predicted': 'Yale – New Haven Hospital', 'question': 'What entity serves as the largest employer in New Haven?'}, '57277d20f1498d1400e8f993': {'truth': 'Yale – New Haven Hospital', 'predicted': 'Yale', 'question': 'What is the second largest employer in New Haven?'}, '57277d20f1498d1400e8f994': {'truth': 'Alexion', 'predicted': 'Alexion Pharmaceuticals', 'question': 'What pharmaceutical company serves as a large employment provider for New Haven? '}, '5729778f6aef051400154f5c': {'truth': 'over half (56%)', 'predicted': '', 'question': 'In modern day how much does New Haven depend on blue collar jobs?'}, '5729778f6aef051400154f5d': {'truth': 'Yale', 'predicted': 'Yale – New Haven Hospital', 'question': \"What institution has largest impact on the city's job market?\"}, '573040dd947a6a140053d33b': {'truth': 'attack British port facilities.', 'predicted': 'attack British port facilities', 'question': 'Raeder convinced Hitler to do what?'}, '57315327e6313a140071ce20': {'truth': 'Syria in the east, to Cyrene to the west, and south to the frontier with Nubia', 'predicted': 'southern Syria in the east, to Cyrene to the west, and south to the frontier with Nubia', 'question': 'What were the reaches of the Ptolemaic Kingdom?'}, '57315327e6313a140071ce22': {'truth': 'Pharaohs', 'predicted': 'successors to the Pharaohs', 'question': 'What were leaders known as during Ptolemaic Kingdom?'}, '5729e5501d0469140077965c': {'truth': 'from a reservoir of electrical potential energy between electrons', 'predicted': 'a reservoir of electrical potential energy between electrons', 'question': 'Where is chemical energy stored and released?'}, '5acd166907355d001abf341a': {'truth': '', 'predicted': 'kinetic', 'question': 'All types of energy are a varying mix of potential and what other kind of energy?'}, '56e1688fe3433e1400422eba': {'truth': 'every fourth year', 'predicted': 'fourth year', 'question': 'How often are elections for mayor held in Boston?'}, '56e1688fe3433e1400422ebb': {'truth': 'extensive executive power', 'predicted': 'executive power', 'question': 'What kind of power does the mayor have?'}, '56f81fe6aef2371900625df8': {'truth': 'Paleolithic era', 'predicted': 'Paleolithic', 'question': 'Evidence of human habitation in the Alps goes as far back to what era? '}, '57268d16dd62a815002e8946': {'truth': 'Many Somerset soldiers died during the First World War, with the Somerset Light Infantry suffering nearly 5,000 casualties', 'predicted': '', 'question': 'How many Somerset soldiers were killed in WW1'}, '57268d16dd62a815002e8947': {'truth': 'only nine, described as the Thankful Villages, had none of their residents killed', 'predicted': '', 'question': 'How many counties had no casualties in WW1 '}, '57268d16dd62a815002e8948': {'truth': 'for troops preparing for the D-Day landings', 'predicted': 'troops preparing for the D-Day landings', 'question': 'The county was base for what in WW2'}, '57268d16dd62a815002e8949': {'truth': 'The Taunton Stop Line was set up to repel a potential German invasion', 'predicted': 'The Taunton Stop Line', 'question': 'What is the Tauton stop line '}, '5acf5b6977cf76001a684c18': {'truth': '', 'predicted': 'nine', 'question': 'What was one of the Thankful Villages?'}, '5acf5b6977cf76001a684c19': {'truth': '', 'predicted': 'Somerset Light Infantry', 'question': 'Which village suffered the most First World War casualties?'}, '572950423f37b3190047822d': {'truth': 'May 1994.', 'predicted': '', 'question': 'When was homosexuality legalized in Bermuda?'}, '572950423f37b3190047822e': {'truth': 'The OBA government simultaneously introduced a bill to permit Civil Unions', 'predicted': '', 'question': 'What occurred in February of 2016?'}, '572950423f37b3190047822f': {'truth': 'same sex spouses of Bermuda citizens could not be denied basic Human Rights.', 'predicted': '', 'question': 'What did the Chief Justice decide?'}, '573149e7e6313a140071cdce': {'truth': 'Nintendo Wii', 'predicted': \"Nintendo Wii's sensor bar\", 'question': 'What video game console uses infrared LEDs?'}, '573149e7e6313a140071cdd0': {'truth': 'RGB LEDs', 'predicted': 'RGB', 'question': 'Some flatbed scanners use what type of LED?'}, '573149e7e6313a140071cdd2': {'truth': 'increase photosynthesis in plants', 'predicted': 'increase photosynthesis', 'question': 'Grow lights use LEDs for what process?'}, '5ad1981f645df0001a2d20e2': {'truth': '', 'predicted': 'RGB LEDs', 'question': ' Some flatbed scanners use what type of what?'}, '57268054dd62a815002e8765': {'truth': 'Pesticides are substances meant for attracting, seducing, and then destroying any pest', 'predicted': 'attracting, seducing, and then destroying', 'question': 'What is the purpose of a pesticide?'}, '57268054dd62a815002e8767': {'truth': 'protect plants from damaging influences such as weeds, fungi, or insects', 'predicted': 'damaging influences', 'question': 'What can pesticides protect plants from?'}, '57268054dd62a815002e8768': {'truth': 'sanitizer', 'predicted': '', 'question': 'What item commonly used in hospitals, schools and offices is a pesticide?'}, '572781a5f1498d1400e8fa1f': {'truth': 'mayor', 'predicted': 'the mayor', 'question': 'Who is elected every even numbered year?'}, '572781a5f1498d1400e8fa20': {'truth': 'Two', 'predicted': \"Two council members are elected from each of the city's five\", 'question': \"How many council members are elected for the city's ward?\"}, '5727d0914b864d1900163dc2': {'truth': 'average 62', 'predicted': '62', 'question': 'How many tornadoes hit Oklahoma each year?'}, '5727d0914b864d1900163dc3': {'truth': 'severe thunderstorms, damaging thunderstorm winds, large hail and tornadoes', 'predicted': 'severe thunderstorms', 'question': 'What types of severe weather does Oklahoma get?'}, '570b4c3c6b8089140040f861': {'truth': 'Islamic Extremist', 'predicted': 'Islamic Extremist terrorist groups', 'question': 'What religious groups are primarily targeted by this war?'}, '570b4c3c6b8089140040f862': {'truth': 'al-Qaeda', 'predicted': 'Islamic Extremist terrorist groups, including al-Qaeda', 'question': 'What is one prominent, specific terrorist group targeted by the War on Terrorism?'}, '570b4c3c6b8089140040f863': {'truth': 'the September 11, 2001 attacks', 'predicted': 'September 11, 2001', 'question': 'The War On Terrorism was caused by what event?'}, '5ad1787c645df0001a2d1d72': {'truth': '', 'predicted': 'Islamic Extremist terrorist groups, including al-Qaeda', 'question': 'What is one prominent, specific terrorist group targeted by the War on Terrorism?'}, '5ad1787c645df0001a2d1d74': {'truth': '', 'predicted': 'Arkansas and Texas', 'question': 'Since the start of the war on Terrorism, attacks on UK service members have occurred in which two UK states?'}, '572a9e1bf75d5e190021fba8': {'truth': 'Between 1979 and 1993', 'predicted': '1993', 'question': 'During which years did the Synod have power to enact measures?'}, '5acfb9fb77cf76001a685ab3': {'truth': '', 'predicted': 'Order in Council', 'question': 'Before 1994, royal dissent was granted by whom?'}, '56ddc1d966d3e219004dacd3': {'truth': '2009', 'predicted': '', 'question': 'When did Internet Archive chance its platform for data storage?'}, '5a6b0974a9e0c9001a4e9e8c': {'truth': '', 'predicted': 'California', 'question': 'What state is home to the first data center?'}, '5732836406a3a419008acaad': {'truth': 'Early humanists', 'predicted': '', 'question': 'Who was able to reconcile their religious beliefs with those of humanism?'}, '5732836406a3a419008acaae': {'truth': 'secular', 'predicted': '', 'question': 'What phrase that has come to be associated with a lack of faith was not seen as an issue for Christians?'}, '5a82062531013a001a335102': {'truth': '', 'predicted': 'Christian Humanism', 'question': 'What phrase that has come to be associated with a lack of faith was seen as an issue for Christians?'}, '5a82062531013a001a335103': {'truth': '', 'predicted': 'nineteenth century', 'question': 'What time period did secular have an ultra-positive connotation?'}, '5a82062531013a001a335104': {'truth': '', 'predicted': 'Gherardo', 'question': 'Who did Petrarch feel superior to in every possible way?'}, '5731f24bb9d445190005e6d4': {'truth': 'sea', 'predicted': 'sea campaign', 'question': 'What type of campaign did Publius fight?'}, '5731f24bb9d445190005e6d6': {'truth': 'defeated', 'predicted': 'He was defeated, and on being bidden by the senate to appoint a dictator, he appointed his messenger Glycias, as if again making a jest of his country\\'s peril.\" His impiety not only lost the battle but ruined his career', 'question': 'How did Publius fare in his battle?'}, '570e70c30b85d914000d7f01': {'truth': 'addressing and postal purposes', 'predicted': 'for addressing and postal purposes', 'question': 'Why is urban Melbourne divided into hundreds of suburbs?'}, '56cf7c394df3c31400b0d83b': {'truth': 'Kanye West Foundation', 'predicted': '\"Kanye West Foundation', 'question': 'With the help of his mom, what foundation did Kanye create early in his career?'}, '56cf7c394df3c31400b0d83c': {'truth': 'battle dropout and illiteracy rates, while partnering with community organizations to provide underprivileged youth access to music education', 'predicted': 'dropout and illiteracy rates', 'question': 'What is the goal of the Kanye West Foundation?'}, '56d11ed617492d1400aab9dd': {'truth': 'Kanye West Foundation', 'predicted': '\"Kanye West Foundation', 'question': 'What was founded by Kanye West and his mother?'}, '56d4647c2ccc5a1400d83132': {'truth': 'music education', 'predicted': 'battle dropout and illiteracy rates, while partnering with community organizations to provide underprivileged youth access to music education', 'question': 'What other mission besides dropout and illiteracy rates did the Kanye West Foundation seek to improve?'}, '56de9a164396321400ee2a46': {'truth': 'Cruz Bustamante', 'predicted': 'Bustamante', 'question': \"Who was Schwarzenegger's closest rival in the gubernatorial race of 2003?\"}, '57279d0aff5b5019007d9106': {'truth': 'served as sailors', 'predicted': '', 'question': 'What did Aslations do to avoid conflict amongst themselves during the first World War?'}, '57279d0aff5b5019007d9108': {'truth': 'French', 'predicted': 'French troops', 'question': 'Who entered Alsace just two weeks after they declared independence? '}, '5a6fdd4c8abb0b001a675fd3': {'truth': '', 'predicted': 'First World War', 'question': 'During what war were Alsatians primarily ground units?'}, '5a6fdd4c8abb0b001a675fd7': {'truth': '', 'predicted': 'Treaty of Versailles', 'question': 'What treaty gave Alsace to Germany?'}, '570b421bec8fbc190045b924': {'truth': 'the Mason–Dixon line', 'predicted': '', 'question': 'What dividing line separated slave states from free states?'}, '570b421bec8fbc190045b926': {'truth': 'states in the South seceded from the United States', 'predicted': '', 'question': \"How did slave states react to Lincoln's election?\"}, '5ad17172645df0001a2d1bcb': {'truth': '', 'predicted': 'April 12, 1861', 'question': 'When did Confederate forces bombard Fort Pumter?'}, '570ba2b3ec8fbc190045baa0': {'truth': 'imbedded stars', 'predicted': '', 'question': 'What irradiates clouds of gas in the galaxy and makes them glow?'}, '570ba2b3ec8fbc190045baa2': {'truth': 'Stars', 'predicted': '', 'question': 'What objects emit less of their energy as infrared light versus visible light?'}, '5a07f8453fc87400182070cc': {'truth': '', 'predicted': 'Infrared', 'question': 'What can be used to detect protostars when they are cool?'}, '570b2dd76b8089140040f7d4': {'truth': '\"Old Style\" (OS) and \"New Style\"', 'predicted': 'Old Style', 'question': 'What designation was added to British dates to differentiate them from countries not using the new calendar? '}, '570b2dd76b8089140040f7d7': {'truth': 'confusion', 'predicted': '', 'question': 'What did the use of Old Style and New Style cause?'}, '5726b597f1498d1400e8e84e': {'truth': 'writing that possesses high quality or distinction', 'predicted': '', 'question': 'What is the main component of the qualitative judgment definition of literature?'}, '5726b597f1498d1400e8e850': {'truth': '\"the best expression of the best thought reduced to writing.\"', 'predicted': '', 'question': 'Encyclopedia Britannica defined literature in its 1911 editions how?'}, '5726b597f1498d1400e8e851': {'truth': 'anything which is universally regarded as literature has the potential to be excluded', 'predicted': '', 'question': 'What effect does the evolving definition of literature have?'}, '5a7a34d717ab25001a8a03a2': {'truth': '', 'predicted': 'fine writing', 'question': 'What is the meaning of the Spanish term, \"belles-lettres?\"'}, '5a7a34d717ab25001a8a03a4': {'truth': '', 'predicted': 'change', 'question': 'Value-decisions can do what over time?'}, '5a7a34d717ab25001a8a03a5': {'truth': '', 'predicted': '1910–11', 'question': 'What years does the Encyclopedia Brittanica Tenth Edition cover?'}, '5a7cfda9e8bc7e001a9e2108': {'truth': '', 'predicted': 'Encyclopædia Britannica Eleventh Edition', 'question': 'Which edition of the World Book Encyclopedia uses the value judgment definition of \"literature?\"'}, '5a7d25d970df9f001a874fe1': {'truth': '', 'predicted': '1910–11', 'question': 'When was the definition of value judgement first used?'}, '5a7d25d970df9f001a874fe3': {'truth': '', 'predicted': '1910–11', 'question': 'What year was the Eleventh Edition of Encyclopedia Britannica written?'}, '5a7d25d970df9f001a874fe4': {'truth': '', 'predicted': 'over time', 'question': 'How often do value-judgments change?'}, '57268e59708984140094c9f9': {'truth': 'discovered in the form of cave paintings', 'predicted': '', 'question': 'What form was the evidence of ancient cultures discovered in ?'}, '5ad3ece8604f3c001a3ff78e': {'truth': '', 'predicted': '$70 million', 'question': 'How much did the 1954 class donate for their 40th reunion?'}, '57262d20271a42140099d706': {'truth': 'further strengthened and smoothed the filament', 'predicted': '', 'question': 'What properties of graphite improved the filament?'}, '56d09354234ae51400d9c3aa': {'truth': 'driven by an expectation that coal would soon become scarce', 'predicted': 'coal would soon become scarce', 'question': 'Why was solar technology developed in the 1860s?'}, '56d09354234ae51400d9c3ab': {'truth': 'increasing availability, economy, and utility of coal and petroleum', 'predicted': '', 'question': 'What slowed the development of solar technologies in the early 20th century?'}, '572efc3503f9891900756b14': {'truth': '319 million years ago', 'predicted': '319 million years ago and 192 million years ago', 'question': 'When did the first whole genome duplication event occur?'}, '572efc3503f9891900756b16': {'truth': 'by sequencing the genome of an ancient flowering plant', 'predicted': 'sequencing the genome', 'question': 'How are duplication events studied?'}, '5a3acb0f3ff257001ab8428b': {'truth': '', 'predicted': 'whole genome duplication events', 'question': 'What appears to be the cause of duplicaton of seed plants?'}, '5a3acb0f3ff257001ab8428d': {'truth': '', 'predicted': 'Amborella trichopoda', 'question': 'When was the ancient plant used in the study dated to?'}, '570888bf9928a814004714dc': {'truth': 'grey', 'predicted': 'grey shirts, shorts and socks', 'question': 'Three times in 1996, England wore what color socks in their away kits instead of the traditional red socks?'}, '59fb256bee36d60018400d49': {'truth': '', 'predicted': 'blue', 'question': \"What shade of blue was used in England's first away kits?\"}, '572931841d04691400779146': {'truth': '2.6–7.8 million species', 'predicted': '2.6–7.8 million', 'question': 'How many insect species are estimated to exist?'}, '572931841d04691400779148': {'truth': 'less than 20%', 'predicted': '20%', 'question': 'Of all the species on earth, how much do insects make up?'}, '572a016b3f37b3190047863f': {'truth': 'transmission of electromagnetic energy via photons', 'predicted': 'the transmission of electromagnetic energy via photons, physical collisions which transfer kinetic energy,[note 5] and the conductive transfer of thermal energy', 'question': 'Give one example of how energy can be transferred between systems?'}, '5acd5d9e07355d001abf3efe': {'truth': '', 'predicted': 'Energy transfer', 'question': 'What can be considered for the typical case of systems which are opened to transfers of matter?'}, '5acd5d9e07355d001abf3eff': {'truth': '', 'predicted': 'heat', 'question': 'The portion of energy which works during the transfer is called what?'}, '5706be1e0eeca41400aa0deb': {'truth': '23.6%', 'predicted': '19.1% of families and 23.6%', 'question': 'What percentage of the population in the city were living below the poverty line?'}, '5727bb132ca10214002d94f7': {'truth': 'Netherlands, France and England', 'predicted': '', 'question': 'Which other countries was Spain at war with during the 16 century?'}, '5a5447e0134fea001a0e16ea': {'truth': '', 'predicted': 'windmill', 'question': 'What technology was developed from advances in irrigation and farming?'}, '5a5447e0134fea001a0e16eb': {'truth': '', 'predicted': 'almonds and citrus fruit', 'question': 'What crops were brought from Europe?'}, '5a5447e0134fea001a0e16ec': {'truth': '', 'predicted': 'sugar cultivation', 'question': 'What cultivation was gradually adopted by Muslims>'}, '5a5447e0134fea001a0e16ed': {'truth': '', 'predicted': 'trade in the Indian Ocean', 'question': 'Where did Arab merchants dominate after the 16th century?'}, '56d1091117492d1400aab7bd': {'truth': '50 Cent', 'predicted': \"50 Cent's Curtis\", 'question': \"What artist was Kanye's third album release competing against?\"}, '57303b58947a6a140053d2e1': {'truth': 'as many as a thousand ships', 'predicted': 'as many as a thousand', 'question': 'Around how many ships were sent into service from Greek cities?'}, '57303b58947a6a140053d2e4': {'truth': 'Roman navy', 'predicted': 'the Roman navy', 'question': 'What aspect of the Roman military saw a decline in size after the subjugation of the Mediterranean?'}, '5719c6a94faf5e1900b8a7ea': {'truth': 'Denny Party', 'predicted': '', 'question': 'What group of settlers established a site at Pioneer Square?'}, '5719c6a94faf5e1900b8a7ec': {'truth': 'New York Alki', 'predicted': 'New York', 'question': \"What was the Chinook enhanced name of Terry and Low's settlement?\"}, '5719c6a94faf5e1900b8a7ed': {'truth': 'New York Alki', 'predicted': 'Alki', 'question': 'What site was eventually abandoned when the settlers moved back in with Denny?'}, '5725f7a389a1e219009ac116': {'truth': 'Dornier Do 17 bomber', 'predicted': 'bomber', 'question': 'What type of airplane was the German craft Holmes rammed into?'}, '5726245b89a1e219009ac2ee': {'truth': 'the Battle of Britain Day', 'predicted': 'Battle of Britain Day', 'question': 'What is September 15, 1940 known as?'}, '5726245b89a1e219009ac2ef': {'truth': 'the Palace', 'predicted': 'bomb the Palace', 'question': 'What did Ray Holmes believe the German pilot was targeting?'}, '5726245b89a1e219009ac2f1': {'truth': 'Dornier Do 17 bomber', 'predicted': '', 'question': 'What type of aircraft was the German Plane?'}, '5a7a513117ab25001a8a04ea': {'truth': '', 'predicted': '15 September 1940', 'question': 'What is the date of the celebration of Britain Day?'}, '5a7a513117ab25001a8a04ed': {'truth': '', 'predicted': \"King's Messenger\", 'question': 'What did Holmes become during the war?'}, '56d09a0e234ae51400d9c3c3': {'truth': 'could play a key role in de-carbonizing the global economy alongside improvements in energy efficiency and imposing costs on greenhouse gas emitters', 'predicted': 'de-carbonizing the global economy', 'question': \"What could the sun's energy do to help limit climate change?\"}, '56d8e18ddc89441400fdb38b': {'truth': 'the Eiffel Tower', 'predicted': 'scaling the Eiffel Tower', 'question': 'What did Reporters Without Borders scale in order to put a protest banner on it?'}, '56d8e18ddc89441400fdb38c': {'truth': 'Notre Dame cathedral', 'predicted': 'Notre Dame', 'question': 'Which cathedral did Reporters Without Borders hang another protest banner?'}, '56db1ba8e7c41114004b4d39': {'truth': 'banner', 'predicted': 'a protest banner', 'question': 'What was hung from the Eiffel Tower?'}, '56db1ba8e7c41114004b4d3b': {'truth': 'Notre Dame cathedral.', 'predicted': 'Notre Dame cathedral', 'question': 'Where else was a copy of the banner at Eiffel Tower hung?'}, '57305ed58ab72b1400f9c4aa': {'truth': 'Proto-Indo-European pantheon', 'predicted': 'Proto-Indo-European', 'question': 'Where did a large amount of the deities that were worshiped in Roman civilization come from?'}, '5730b55f396df919000962cc': {'truth': \"Android devices, Apple's iPhone and iPad\", 'predicted': 'Android', 'question': 'What smartphones have SNES emulators?'}, '5730b55f396df919000962ce': {'truth': \"Nintendo's Virtual Console service for the Wii\", 'predicted': '', 'question': \"What was Nintendo's first approved emulator?\"}, '57304e618ab72b1400f9c413': {'truth': '45', 'predicted': '45 minutes', 'question': 'How long are each period in a standard football match?'}, '57304e618ab72b1400f9c415': {'truth': 'referee', 'predicted': '', 'question': 'Who gets to decide how long stoppage time can go on for?'}, '56f9ebe18f12f3190062fffc': {'truth': 'develops and agrees upon specifications', 'predicted': 'broadcasters, consumer electronics manufacturers and regulatory bodies. The DVB develops and agrees upon specifications which are formally standardised by ETSI', 'question': \"What is the DVB's role?\"}, '5ad3b2f6604f3c001a3fed2d': {'truth': '', 'predicted': \"International Telecommunication Union's radio telecommunications sector\", 'question': 'What does ETU-R stand for?'}, '572ea993cb0c0d14000f1416': {'truth': 'palimpsests', 'predicted': '', 'question': 'What is the term for recycled parchments used in ancient manuscripts?'}, '572ea993cb0c0d14000f1417': {'truth': '671 AD', 'predicted': '', 'question': \"Before which year were the Sana'a manuscripts produced?\"}, '572ea993cb0c0d14000f1419': {'truth': 'Uthmanic', 'predicted': '', 'question': \"What version of the Quran was the scriptio superior of the Sana'a manuscripts?\"}, '5729058baf94a219006a9f6a': {'truth': 'Festivals can be held on a grand scale.', 'predicted': 'on a grand scale', 'question': 'Are public displays allowed for the celebration of religion in Myanmar ?'}, '5729058baf94a219006a9f6c': {'truth': 'it is hard, if not impossible, for non-Buddhists to join the army', 'predicted': 'non-Buddhists', 'question': 'Can anyone in Burma Join the military forces in Burma ?'}, '5729058baf94a219006a9f6d': {'truth': 'join the army or get government jobs, the main route to success in the country.', 'predicted': 'army or get government jobs', 'question': 'What are the best routes for career achievement in Burma ?'}, '571a8f6a4faf5e1900b8aa7b': {'truth': 'from just four women', 'predicted': 'four', 'question': 'A 2006 study by Behar et al, suggested that a large percentage of the current Ashkenazi population is descended matrilineally from how many women? '}, '56e7a21637bdd419002c42a1': {'truth': 'March 9, 2012', 'predicted': '', 'question': 'What was the first day of the 2015 AFL season?'}, '56e7a21637bdd419002c42a2': {'truth': 'San Antonio', 'predicted': 'San Antonio, Texas', 'question': 'To what city did the Tulsa Talons relocate?'}, '572fe399a23a5019007fcae4': {'truth': 'non-traditional', 'predicted': 'non-traditional PCBs such as MCMs and microwave PCBs', 'question': 'What class of PCBs are even more susceptible to static than standard ones?'}, '5ace846a32bba1001ae4a941': {'truth': '', 'predicted': 'static', 'question': 'Many assembled PABs are sensitive to what?'}, '56e039947aa994140058e3d3': {'truth': 'komiks', 'predicted': 'Комикс, komiks', 'question': 'What Russian word is used for comics?'}, '56e039947aa994140058e3d4': {'truth': 'comics', 'predicted': '', 'question': 'What German word is used for comics?'}, '572817584b864d1900164462': {'truth': 'the Metropolitan Green Belt', 'predicted': 'Metropolitan Green Belt', 'question': 'What statutory policy minimizes outward expansion of urban London?'}, '572817584b864d1900164464': {'truth': 'Eleanor Cross at Charing Cross near the junction of Trafalgar Square and Whitehall', 'predicted': 'Eleanor Cross at Charing Cross', 'question': 'Where is the centre of London said to be located?'}, '572817584b864d1900164466': {'truth': 'London commuter belt', 'predicted': '', 'question': 'What metropolitan area lies beyond the Metropolitan Green Belt?'}, '5727b1f42ca10214002d941a': {'truth': '23.3%', 'predicted': '', 'question': 'In 2010, what percentage of the population was made up of foreigners?'}, '5727b1f42ca10214002d941b': {'truth': 'Italians', 'predicted': '', 'question': 'Who were the largest single group of foreigners in 2010?'}, '572fce07b2c2fd140056848d': {'truth': 'Catholic Saint Didacus', 'predicted': '', 'question': 'Who was the harbor named for?'}, '5727b2e53acd2414000dea18': {'truth': '2008', 'predicted': '', 'question': 'In what year did the risk of dying from TB reach half what it was in 1995?'}, '5727b2e53acd2414000dea19': {'truth': 'reinfection', 'predicted': '', 'question': 'New studies have found that half of reactivation cases of tuberculosis might actually be due to what other \"re-\" word?'}, '5a871df11d3cee001a6a10e2': {'truth': '', 'predicted': 'HIV', 'question': 'Reactivation is caused by what disease?'}, '5a871df11d3cee001a6a10e4': {'truth': '', 'predicted': '4%', 'question': 'What was the chance of death from HIV in 2008?'}, '5a871df11d3cee001a6a10e6': {'truth': '', 'predicted': 'DNA fingerprinting', 'question': 'What methods have recent studies used to examine HIV strains?'}, '56de471ccffd8e1900b4b76f': {'truth': 'Grover Cleveland', 'predicted': '', 'question': 'Who was the first president to veto over 400 bills?'}, '5ad3a323604f3c001a3fea4f': {'truth': '', 'predicted': 'great damage', 'question': \"What did Cleveland's impeachment do to the presidency?\"}, '5ad3a323604f3c001a3fea50': {'truth': '', 'predicted': 'Johnson', 'question': \"Who's impeachment was perceived as having helped the presidency?\"}, '5ad3a323604f3c001a3fea51': {'truth': '', 'predicted': 'Congress', 'question': 'Which body of government because subordinate to the presidency?'}, '5ad3a323604f3c001a3fea52': {'truth': '', 'predicted': 'Johnson', 'question': 'After Grover Cleveland, who was the first Democratic President?'}, '5ad3a323604f3c001a3fea53': {'truth': '', 'predicted': 'Grover Cleveland', 'question': 'Who was the first Republican President after Johnson?'}, '56e11d89e3433e1400422c20': {'truth': 'December 2011', 'predicted': '', 'question': 'When did the Beidou system begin operating in China?'}, '56e11d89e3433e1400422c21': {'truth': 'by 2020', 'predicted': '2020', 'question': 'When is it projected that the global navigation system will be finished?'}, '56dfbe777aa994140058e0e1': {'truth': 'The Illuminating Engineering Society of North America', 'predicted': 'Illuminating Engineering Society of North America', 'question': 'What does the IESNA stand for? '}, '56dfbe777aa994140058e0e4': {'truth': 'distribution of light released', 'predicted': '', 'question': 'What defines photo metric data?'}, '5722d357f6b826140030fc68': {'truth': 'Her family and retainers', 'predicted': '', 'question': 'Who accused Karim of spying?'}, '5723e1e80dadf01500fa1f70': {'truth': 'Equerry Frederick Ponsonby', 'predicted': 'Frederick Ponsonby', 'question': \"Who discovered that Victoria's new Munshi lied about his parentage?\"}, '5723e1e80dadf01500fa1f71': {'truth': 'Lord Elgin, Viceroy of India', 'predicted': 'Lord Elgin', 'question': 'Who did Ponsonby report the Munshis lies about his parentage to?'}, '5724e8960ba9f01400d97bbf': {'truth': 'Muslim Patriotic League', 'predicted': 'the Muslim Patriotic League', 'question': \"Who was Karim accused of spying for by Victoria's family? \"}, '5724e8960ba9f01400d97bc0': {'truth': 'Equerry Frederick Ponsonby', 'predicted': 'Frederick Ponsonby', 'question': 'Who discovered that Karim had lied about his parentage to Victoria? '}, '572635ccec44d21400f3dc40': {'truth': 'the fiftieth anniversary of her accession', 'predicted': 'marked the fiftieth anniversary of her accession', 'question': 'What is the point of the Golden Jubilee?'}, '572635ccec44d21400f3dc42': {'truth': 'spying for the Muslim Patriotic League, and biasing the Queen against the Hindus', 'predicted': 'spying for the Muslim Patriotic League, and biasing the Queen against the Hindus. Equerry Frederick Ponsonby (the son of Sir Henry) discovered that the Munshi had lied about his parentage, and reported to Lord Elgin, Viceroy of India, \"the Munshi occupies very much the same position as John Brown used to do.\" Victoria dismissed their complaints as racial prejudice', 'question': \"Why did Victoria's family disapprove of Abdul Karim?\"}, '572635ccec44d21400f3dc43': {'truth': 'until he returned to India with a pension on her death', 'predicted': 'until he returned to India with a pension', 'question': \"How long was Karim in the Queen's employment?\"}, '5ad17cf2645df0001a2d1e16': {'truth': '', 'predicted': '1887', 'question': \"What year was Victoria's Silver Jubilee held?\"}, '57277778708984140094de58': {'truth': 'is, to the devout, taboo.', 'predicted': '', 'question': 'Why is there opposition to textual criticism of Jewish and Muslim religious books?'}, '57277778708984140094de59': {'truth': 'a period of about five millennia', 'predicted': 'five millennia', 'question': 'Over approximately what expanse of time can textual criticism be applied to written works?'}, '5a8c775dfd22b3001a8d885a': {'truth': '', 'predicted': 'Jewish', 'question': \"It's too taboo to criticize the Torah, Qur'an, and what other religion?\"}, '56f7f695aef2371900625cfe': {'truth': 'removing Lithuania from the names of the Gubernyas', 'predicted': 'removing Lithuania from the names of the Gubernyas few years after the November Uprising', 'question': 'What was an example of imposing sanctions on lithuanians?'}, '56f7f695aef2371900625d00': {'truth': 'Lithuanian language.', 'predicted': 'Lithuanian', 'question': 'What language was banned from printing on books?'}, '572802332ca10214002d9b50': {'truth': 'direct manipulation', 'predicted': 'direct manipulation of the medium', 'question': 'What is a reason a DJ would prefer vinyl to CD?'}, '572802332ca10214002d9b52': {'truth': 'provided the stylus, record player, and record itself are built to withstand it', 'predicted': 'indirect manipulation options, e.g., the play, stop, and pause buttons. With a record one can place the stylus a few grooves farther in or out, accelerate or decelerate the turntable, or even reverse its direction, provided the stylus, record player, and record itself are built to withstand it', 'question': 'Are all turn tables capable of DJ manipulation of vinyl records?'}, '572802332ca10214002d9b54': {'truth': 'However, many CDJ and DJ advances, such as DJ software and time-encoded vinyl, now have these capabilities and more.', 'predicted': '', 'question': 'Had vinyl technology ceased expanding?'}, '572f7b31b2c2fd1400568181': {'truth': 'an exposition of social, artistic, and cultural dynamism.', 'predicted': 'social, artistic, and cultural dynamism', 'question': 'what did the \"Jazz Age\" usher in? '}, '5ad3c98e604f3c001a3ff09f': {'truth': '', 'predicted': 'a garrison of British Marines', 'question': 'By who was the sailing ships occupied?'}, '5ad3c98e604f3c001a3ff0a0': {'truth': '', 'predicted': 'Whalers', 'question': 'Who used the islands as a base in the Northern Atlantic?'}, '570bd2ec6b8089140040fa68': {'truth': 'a Control-S', 'predicted': 'Control-S', 'question': 'What casued the automatic paper tape reader to stop?'}, '573039c004bcaa1900d773c6': {'truth': '8 men', 'predicted': '8 men each', 'question': 'How many troops were placed into each tent group?'}, '573039c004bcaa1900d773c8': {'truth': 'cohort', 'predicted': \"heavy infantry. The legion's main sub-unit was called a cohort\", 'question': 'What designation of troops was considered to make up the majority of a legion?'}, '5ad3d84a604f3c001a3ff379': {'truth': '', 'predicted': 'equality', 'question': 'On what did William Glass base his island on?'}, '5ad3d84a604f3c001a3ff37a': {'truth': '', 'predicted': 'it votes for a change in its law', 'question': 'What will not allow for a outsiders to buy land or settle on Tristan?'}, '5ad3d84a604f3c001a3ff37b': {'truth': '', 'predicted': '90 days', 'question': 'For how many months does the nominal fishing season last?'}, '572785c8dd62a815002e9f6c': {'truth': '2.55 million', 'predicted': '2.55 million 10-14 year-olds were illegally holding jobs. They were joined by 3.7 million 15-17 year-olds and about 375,000 5-9 year-olds. Due to the raised age restriction of 14, at least half of the recorded young workers had been employed illegally which lead to many not being protect by important labour laws. Although substantial time has passed since the time of regulated child labour, there is still a large number', 'question': 'In 1999 how many children were working illegally in Brazil?'}, '572785c8dd62a815002e9f6f': {'truth': 'an increase', 'predicted': 'there has been an increase', 'question': 'Has these dangers caused and increase or decrease in child labour with drug cartels?'}, '572842a0ff5b5019007da031': {'truth': 'bluffing', 'predicted': 'bluffing game', 'question': 'What gambit did Nasser fail at in his bluster with Israel?'}, '5734465d879d6814001ca464': {'truth': 'some tribal societies', 'predicted': 'tribal societies', 'question': 'Where does the hunter-gathering lifestyle persist, though in decline?'}, '5734465d879d6814001ca466': {'truth': 'the Hadza of Tanzania', 'predicted': 'the Hadza', 'question': 'Who are the only remaining full-time hunter-gatherers in Africa?'}, '5735e8736c16ec1900b92888': {'truth': 'European Age of Discovery', 'predicted': 'the European Age of Discovery', 'question': 'Hunter-gathering lifestyles remained prevalent until when?'}, '5735e8736c16ec1900b92889': {'truth': 'Sub-Saharan Africa, and Siberia, as well as all of Australia', 'predicted': 'Sub-Saharan Africa, and Siberia', 'question': 'What parts of the New World did the hunter-gathering lifestyles remain?'}, '5726275589a1e219009ac3ed': {'truth': '20th', 'predicted': '20th century', 'question': 'Simpson laid the systematics of mammal origins and was taught universally up until the end of what century?'}, '5a39eb432f14dd001ac72643': {'truth': '', 'predicted': 'a systematics of mammal origins and relationships', 'question': 'What did Simpson lay out that was universally taught until 1945?'}, '5a39eb432f14dd001ac72647': {'truth': '', 'predicted': 'end of the 20th century', 'question': 'Up until what century were cladistics universally taught?'}, '573192bca5e9cc1400cdc0e8': {'truth': 'CAD', 'predicted': 'computer aided design (CAD', 'question': 'Which software can aid in the design of robotically created mosaics?'}, '573192bca5e9cc1400cdc0ea': {'truth': 'a command file', 'predicted': 'according to a command file', 'question': 'How does the robot pick the tiles it places?'}, '573192bca5e9cc1400cdc0eb': {'truth': 'different look', 'predicted': '', 'question': 'What is not the same between hand made and robotic amde mosaics?'}, '5a82188631013a001a3351e5': {'truth': '', 'predicted': 'stress and intonation', 'question': 'Under what topic is phonological alternation studied?'}, '5a82188631013a001a3351e7': {'truth': '', 'predicted': 'prosody', 'question': 'Phonotactics, phonological alternation and stress are topics contained in what discipline?'}, '56fb91df8ddada1400cd64f7': {'truth': 'scholasticism', 'predicted': '', 'question': 'What school of thought was Duns Scotus opposed to?'}, '56fb91df8ddada1400cd64f9': {'truth': 'England', 'predicted': '', 'question': 'What country did not see the increasing influence of Roman law?'}, '56fb91df8ddada1400cd64fb': {'truth': 'universals', 'predicted': '', 'question': 'What Platonic idea lost influence as a result of the work of Ockham and Duns Scotus?'}, '572eb63603f989190075697f': {'truth': 'few consultations result in jeopardy opinions', 'predicted': 'so few consultations result in jeopardy opinions', 'question': 'Why is this exemption provision often considered a nonfactor?'}, '572eb63603f9891900756980': {'truth': 'in the identification of reasonable and prudent alternatives to avoid jeopardy.', 'predicted': '', 'question': 'How can jeopardy opinions be dissuaded?'}, '5a848a6d7cf838001a46a8ed': {'truth': '', 'predicted': 'three', 'question': 'How many exemptions were withdrawn in 2009?'}, '5728cc17ff5b5019007da6e5': {'truth': 'The radicals', 'predicted': 'radicals', 'question': 'Which faction of the Congress did Tilak lead?'}, '5728cc17ff5b5019007da6e6': {'truth': 'The moderates', 'predicted': 'moderates', 'question': 'Which faction wanted reform within British rule?'}, '572f7d6f04bcaa1900d76a1a': {'truth': 'tropical wet and dry climate', 'predicted': 'tropical wet and dry', 'question': 'Köppen Aw refers to what kind of climate?'}, '572f7d6f04bcaa1900d76a1c': {'truth': '10 °C', 'predicted': '10 °C (50 °F)', 'question': 'What is the typical lowest temperature in Celsius during winter in Hyderabad?'}, '56e0ccb3231d4119001ac3b5': {'truth': 'Chrome', 'predicted': '', 'question': 'Which browser is the newest to enter the field?'}, '5a4d38437a6c4c001a2bbc5e': {'truth': '', 'predicted': 'December 2011', 'question': 'When did Internet Explorer 8 overtake Chrome as the more popular browser?'}, '5726e8eb708984140094d58a': {'truth': 'species as a whole', 'predicted': '', 'question': 'Does aposematism benefit only the organism ddirectly, or the entire population as a whole?'}, '5a6bdaf94eec6b001a80a5e9': {'truth': '', 'predicted': 'brightly colored', 'question': 'What do predators use to warn their prey?'}, '5a6bdaf94eec6b001a80a5eb': {'truth': '', 'predicted': 'prey species', 'question': 'What group benefits from using camoflage if an organism is killed using it?'}, '572f5d5eb2c2fd1400568083': {'truth': 'inability to damage industries', 'predicted': '', 'question': \"Why did Hitler feel bombing wasn't working?\"}, '572f5d5eb2c2fd1400568084': {'truth': 'the moment was right', 'predicted': 'when the moment was right', 'question': 'In 1939 Hitler said bombing of Britain would begin when?'}, '56e8dca40b45c0140094cd22': {'truth': '£1,310,000 to £1,530,000', 'predicted': '', 'question': \"What was the abbey's annual income as of 2016?\"}, '56e8dca40b45c0140094cd24': {'truth': 'the assessment attendant on the Dissolution of the Monasteries', 'predicted': 'Dissolution of the Monasteries', 'question': 'During what was the abbey made second in wealth?'}, '5ad3f4c7604f3c001a3ff959': {'truth': '', 'predicted': '1535', 'question': \"When was the abbey's annual income £2400–2700?\"}, '5ad3f4c7604f3c001a3ff95b': {'truth': '', 'predicted': 'Glastonbury Abbey', 'question': 'To which other abbey was Westminster Abbey first in wealth?'}, '5ad3f4c7604f3c001a3ff95c': {'truth': '', 'predicted': 'Dissolution of the Monasteries', 'question': 'During what was the abbey made third in wealth?'}, '573029cd947a6a140053d1f1': {'truth': 'a simpler and modern design with less technical information displayed', 'predicted': '', 'question': 'What changes were made to the BSoD?'}, '573029cd947a6a140053d1f2': {'truth': 'applications, background processes and Windows processes', 'predicted': '', 'question': 'What are some of the process type groups Windows 8 implemented?'}, '573029cd947a6a140053d1f3': {'truth': 'search the web', 'predicted': '', 'question': 'How can users find out more about obscure Windows 8 processes?'}, '570e25b30dc6ce1900204dfe': {'truth': '\"Pasta with tomato sauce and berbere\" (spice)', 'predicted': '\"Pasta with tomato sauce and berbere\" (spice', 'question': \"How does 'Pasta al Sugo e Berbere' translate in English?\"}, '5ad0d8bf645df0001a2d06a7': {'truth': '', 'predicted': 'Pasta al Sugo e Berbere', 'question': 'What is a common dish in Ethiopia?'}, '570624f252bb8914006898f7': {'truth': 'filter bank', 'predicted': '', 'question': 'What did the working group integrate their ideas with?'}, '570624f252bb8914006898fa': {'truth': 'MP2 at 192 kbit/s', 'predicted': '', 'question': 'What quality were they hoping to match at 128 kbit/s?'}, '5727ff0c2ca10214002d9ae8': {'truth': 'early 18th century', 'predicted': '18th century', 'question': 'During what century did the Mughal empire decline?'}, '572820842ca10214002d9e7c': {'truth': '€212.1bn', 'predicted': '', 'question': 'By 2012, how much did the ECB spend in covering bad debt?'}, '572820842ca10214002d9e7d': {'truth': 'Outright Monetary Transactions', 'predicted': '', 'question': 'How does the ECB plan to increase the available credit for businesses?'}, '572820842ca10214002d9e80': {'truth': 'temporary', 'predicted': '', 'question': 'How long was the duration of the Securities Markets Programme to last?'}, '56cfe7f4234ae51400d9c060': {'truth': 'Guildhall', 'predicted': \"London's Guildhall\", 'question': \"Where was Chopin's last public performance?\"}, '56d375b859d6e41400146468': {'truth': 'Polish refugees.', 'predicted': 'Polish refugees', 'question': 'Who were the beneficiaries of his last public concert?'}, '56d375b859d6e41400146469': {'truth': 'terminal', 'predicted': 'his doctors were aware that his sickness was at a terminal stage', 'question': \"What was the diagnosis of Chopin's health condition at this time?\"}, '5731225ca5e9cc1400cdbc77': {'truth': 'a land bridge which connected the two continents', 'predicted': 'a land bridge which connected the two continents across what is now the Bering Strait', 'question': 'What was Beringia?'}, '5731225ca5e9cc1400cdbc79': {'truth': 'a wide range of creation myths', 'predicted': 'oral histories of many of the indigenous peoples of the Americas, they have been living there since their genesis, described by a wide range of creation myths', 'question': 'How do the indigenous peoples explain how they came to live in the Americas?'}, '5731bfaae17f3d140042238d': {'truth': 'the mid-to-late 16th century', 'predicted': '', 'question': 'When did Protestantism begin to split?'}, '5731bfaae17f3d140042238f': {'truth': 'transubstantiation', 'predicted': '', 'question': 'What Catholic belief did early Protestants not agree with?'}, '5731bfaae17f3d1400422391': {'truth': 'Christ', 'predicted': '', 'question': 'Whose body and blood is considered present in Holy Communion?'}, '5727ff7c2ca10214002d9b02': {'truth': 'the Turcophile Pope', 'predicted': 'Turcophile Pope', 'question': 'What ws he known by in the Turkish community?'}, '5a60fc3ae9e1cc001a33cdfc': {'truth': '', 'predicted': '30 November 1934', 'question': 'When was Pope Pius XI appointed Apostolic Delegate to Turkey and Greece?'}, '5a60fc3ae9e1cc001a33cdfe': {'truth': '', 'predicted': 'Turcophile Pope', 'question': 'What title is Pope Pius XI known by in Turkey?'}, '5a60fc3ae9e1cc001a33cdff': {'truth': '', 'predicted': '14 October', 'question': 'When did Pope Pius XI take his position in Turkey?'}, '56f89a0c9e9bad19000a01b2': {'truth': 'Augustus', 'predicted': 'Augustus and Aeneas', 'question': 'Who is the founder of Rome which some scholars see strong associations with Aeneas?'}, '5a7e576e70df9f001a87575f': {'truth': '', 'predicted': 'Actium', 'question': 'Where did Augustus live?'}, '5a7e576e70df9f001a875760': {'truth': '', 'predicted': 'Mark Antony and Cleopatra VII', 'question': 'How did Augustus fight at Actium?'}, '57305b0b069b5314008320a7': {'truth': 'context itself', 'predicted': 'the context itself', 'question': 'What have translators tried to preserve?'}, '57305b0b069b5314008320a9': {'truth': 'grammatical structure', 'predicted': 'word order — when necessary, reinterpreting the actual grammatical structure', 'question': 'What is it sometimes necessary to reinterpret when translating?'}, '57305b0b069b5314008320aa': {'truth': 'passive', 'predicted': '', 'question': 'What is the active voice sometimes shifted to when needed?'}, '570fc65b80d9841400ab366b': {'truth': 'consumer market', 'predicted': 'consumer', 'question': 'What market did Dell ignore at first?'}, '570fc65b80d9841400ab366c': {'truth': '1996', 'predicted': '1996 and 1997', 'question': \"When did Dell's internet site gain popularity?\"}, '570fc65b80d9841400ab366e': {'truth': '1997', 'predicted': 'early 1997', 'question': 'When did Dell create their internal marketing group?'}, '5ad3586b604f3c001a3fde25': {'truth': '', 'predicted': 'consumer', 'question': 'What market did Dell appease at first?'}, '5a6bb3444eec6b001a80a4ea': {'truth': '', 'predicted': 'viability', 'question': 'What purpose does technetium serve in terms of identifying tissue?'}, '5a6bb3444eec6b001a80a4ec': {'truth': '', 'predicted': 'normal ECG', 'question': 'A patient does not need routine imaging when they have what kind of biomarker?'}, '5726f9d3f1498d1400e8f193': {'truth': 'UN Command', 'predicted': 'the UN Command', 'question': 'Who ignored the direct participation of the Soviet Union to prevent expanding the Korean War to the Soviet Union?'}, '572792f3f1498d1400e8fc8c': {'truth': 'December 1994', 'predicted': '1994', 'question': 'In what month and year did TCM Remembers premiere?'}, '572792f3f1498d1400e8fc8e': {'truth': 'December', 'predicted': '', 'question': 'In what month of the year does a longer version of TCM Remembers appear?'}, '572792f3f1498d1400e8fc8f': {'truth': '2007', 'predicted': '', 'question': 'In what year did Badly Drawn Boy provide a soundtrack for TCM Remembers?'}, '5a826083e60761001a2eb222': {'truth': '', 'predicted': '2009', 'question': 'In what year did Steve Earle provide a soundtrack for TCM Remembers?'}, '5a826083e60761001a2eb223': {'truth': '', 'predicted': 'Steve Earle', 'question': 'Who provided the soundtrack for the longer Badly Drawn Boy episode in 2009?'}, '57269193f1498d1400e8e410': {'truth': 'in the late thirteenth century, but it declined in importance and the status of county town transferred to Taunton about 1366', 'predicted': 'late thirteenth century', 'question': 'When did Somerton take over from Ilchester as the county town '}, '57269193f1498d1400e8e411': {'truth': 'The county has two cities, Bath and Wells, and 30 towns', 'predicted': '', 'question': 'How many cities and towns in Somerset '}, '57269193f1498d1400e8e412': {'truth': 'in terms of population are Bath, Weston-super-Mare, Taunton, Yeovil and Bridgwater', 'predicted': '', 'question': 'The largest populations of the county '}, '57269193f1498d1400e8e413': {'truth': 'strategic importance in relation to geographical features, such as river crossings or valleys in ranges of hills', 'predicted': 'geographical features', 'question': 'What was the strategic purpose to settle this area '}, '57269193f1498d1400e8e414': {'truth': 'Chard is the most southerly town in Somerset, and at an altitude of 121 m (397 ft) it is also the highest', 'predicted': '', 'question': 'Most Southernly town of somerset'}, '5acf5f2877cf76001a684ca6': {'truth': '', 'predicted': 'Bath, Weston-super-Mare, Taunton, Yeovil and Bridgwater', 'question': 'What is the largest Urban area in Somerton?'}, '56ce4100aab44d1400b88615': {'truth': 'In 1642', 'predicted': '1642', 'question': 'When did the 5th Dalai Lama gain political control over Tibet?'}, '57268ac8dd62a815002e88d8': {'truth': '15.8', 'predicted': '', 'question': 'What percentage of the National GDP does the Federal District produce?'}, '57268ac8dd62a815002e88db': {'truth': '25.3', 'predicted': '25.3%', 'question': 'What percentage of service sector national GDP does Mexico City account for?'}, '5734009a4776f41900661692': {'truth': 'in a Shahmukhi script', 'predicted': 'Shahmukhi script', 'question': 'How is Punjabi written?'}, '5a68fabf8476ee001a58a965': {'truth': '', 'predicted': '89%', 'question': 'Punjabi is the mother-tongue of less than what percentage of the population?'}, '5727d9b5ff5b5019007d96d4': {'truth': 'medieval', 'predicted': 'medieval period', 'question': 'Albert Magnus studied Dionysus during what historical period?'}, '5a611ac3e9e1cc001a33cf1c': {'truth': '', 'predicted': 'prior ones of stone', 'question': 'What replaced metal objects?'}, '5a611ac3e9e1cc001a33cf20': {'truth': '', 'predicted': 'cattle', 'question': 'What diet did Egyptians begin to move away from?'}, '57344599acc1501500babd63': {'truth': 'were hunters', 'predicted': 'hunters', 'question': 'What is undisputed about early humans?'}, '57344599acc1501500babd64': {'truth': 'earlier Australopithecines', 'predicted': 'Australopithecines', 'question': 'Hunting was important for the emergence of the Homo genus from what?'}, '57344599acc1501500babd65': {'truth': 'hunting', 'predicted': 'hunting hypothesis', 'question': 'Production of stone tools and control of fire were also pushed forward by what?'}, '5735e8236c16ec1900b92882': {'truth': 'humans were hunters', 'predicted': 'hunters', 'question': 'What is undisputed about earlier humans?'}, '5ace521a32bba1001ae4a29d': {'truth': '', 'predicted': 'stone tools', 'question': 'What kind of activity would require the use of fire for early humans?'}, '5ace521a32bba1001ae4a29e': {'truth': '', 'predicted': 'Australopithecines', 'question': 'Mating behavior was important for the emergence of Homo genus from what?'}, '572b7afb34ae481900deae3f': {'truth': 'infinite', 'predicted': '', 'question': 'According to Davidson, how many gods are there?'}, '572b7afb34ae481900deae40': {'truth': 'Aristotle', 'predicted': \"Aristotle's pluralism\", 'question': 'By whose philosophy was Davidson influenced?'}, '572b7afb34ae481900deae41': {'truth': 'rational thought', 'predicted': '', 'question': 'What did Davidson believe the God of Aristotle is synonymous with?'}, '5a7ca6d9e8bc7e001a9e1f49': {'truth': '', 'predicted': 'Aristotle', 'question': \"Davidson's argument that God cannot exist apart from the world matched whose argument?\"}, '572fa925947a6a140053cb1d': {'truth': 'annual exhibition of local and national consumer products', 'predicted': 'a popular annual exhibition of local and national consumer products', 'question': 'What is Numaish?'}, '572fa925947a6a140053cb20': {'truth': \"world's largest film studio\", 'predicted': '', 'question': 'What did Guinness World Records say of Ramoji Film City was in 20015?'}, '572a34b83f37b319004787a7': {'truth': 'farmers', 'predicted': 'skilled farmers', 'question': 'What type of occupation were Neolithic people considered to be proficient at?'}, '572a34b83f37b319004787a8': {'truth': 'tending, harvesting and processing of crops', 'predicted': 'the tending, harvesting and processing of crops', 'question': 'What purposes were the production of farm tools used for?'}, '572a34b83f37b319004787aa': {'truth': 'pottery, bone implements)', 'predicted': 'pottery, bone implements', 'question': 'What types of tools did early farmers use for food production?'}, '572a34b83f37b319004787ab': {'truth': 'stone axe', 'predicted': 'the polished stone axe', 'question': 'What tool allowed early farmers to convert forest into arable land?'}, '5a7d3e5c70df9f001a87503e': {'truth': '', 'predicted': 'wood', 'question': 'What types of tools did early farmers use for shelter?'}, '5a7d3e5c70df9f001a87503f': {'truth': '', 'predicted': 'the polished stone axe', 'question': 'What tool allowed early manufacturers to convert forest into arable land?'}, '5727c7ad3acd2414000dec38': {'truth': 'Strathmore', 'predicted': 'Wurlitzer Building and Strathmore Hotel', 'question': 'Which hotel is set to be renovated?'}, '5731c971e17f3d14004223e3': {'truth': 'the 1550s', 'predicted': '1550s', 'question': 'When were French nobles converted to Protestantism?'}, '5731c971e17f3d14004223e4': {'truth': 'Henry II', 'predicted': 'Henry II of France', 'question': 'Whose death caused an increase in the French civil wars?'}, '57264e34f1498d1400e8db96': {'truth': '50 °C (122 °F)', 'predicted': '122 °F', 'question': 'At what temperature does a typical 50-hour-life projection bulb operate?'}, '57264e34f1498d1400e8db98': {'truth': 'as short as two hours', 'predicted': 'two hours', 'question': 'What is the typical life of a P1 lamp?'}, '57264e34f1498d1400e8db99': {'truth': 'higher color temperature', 'predicted': '', 'question': 'How does the color temperature differ for photographic lighting?'}, '570acf964103511400d59a24': {'truth': '145 million to 100 million years', 'predicted': '145 million to 100 million years ago', 'question': 'What is the span of the Early Cretaceous?'}, '572c99202babe914003c299d': {'truth': 'eastern', 'predicted': 'eastern third', 'question': 'Which part of Tennessee voted more Republican in the years following the Civil War?'}, '56f7d6d8aef2371900625c2e': {'truth': 'Alexander the Great', 'predicted': 'Julius Caesar, Alexander the Great', 'question': 'What was another regional leader involved in the origins of szlachta?'}, '56f7d6d8aef2371900625c2f': {'truth': \"had not mixed their bloodlines with those of 'slaves, prisoners, and aliens\", 'predicted': '', 'question': 'What was important and unique about regional leaders?'}, '56e788ab37bdd419002c40d6': {'truth': 'advent of cameras with automatic flashes', 'predicted': 'cameras with automatic flashes', 'question': 'What has made the no photography rule hard to enforce in the Rotunda for the Charters of Freedom?'}, '5a7b6b7321c2de001afe9ffe': {'truth': '', 'predicted': 'prohibited', 'question': 'Was it difficult to enforce the ban on the public entering the Rotunda before February 25, 2010?'}, '56e8f62999e8941900975f30': {'truth': 'the Celestial Organ', 'predicted': 'Celestial Organ', 'question': 'What part of the organ is not connected or playable?'}, '57304d34069b531400832024': {'truth': 'K-8 parochial schools', 'predicted': 'K-8', 'question': 'The Roman Catholic Diocese of Charleston oversees what kind of schools?'}, '57304e4b069b531400832037': {'truth': '150 years', 'predicted': '', 'question': \"How old are some of Charleston's private school?\"}, '5ad42525604f3c001a4008d5': {'truth': '', 'predicted': '150 years', 'question': 'Some of the oldest public schools in Charleston date back how long?'}, '5ad42525604f3c001a4008d9': {'truth': '', 'predicted': 'The Roman Catholic Diocese of Charleston Office of Education', 'question': 'Which organization oversees several K-9 Parochial Schools?'}, '57293b691d0469140077918f': {'truth': 'China, Germany, Spain, the United States, Italy, and Brazil', 'predicted': '', 'question': 'What  six were the top countries for investment in recent years?'}, '5ad123aa645df0001a2d0ee9': {'truth': '', 'predicted': 'dramatically lower solar prices and weakened US and EU markets', 'question': 'Why did the total investment in renewable energy go up in 2012?'}, '56e1223ecd28a01900c67633': {'truth': 'literary life', 'predicted': 'rarefied literary life', 'question': 'Boston was admired for what kind of life?'}, '5ad37aaa604f3c001a3fe3cd': {'truth': '', 'predicted': 'Age of Enlightenment', 'question': 'During which age did Thomas Hobbes advocate the principle in his writing?'}, '5ad37aaa604f3c001a3fe3cf': {'truth': '', 'predicted': 'Thomas Hobbes', 'question': 'Who was at the forefront of the opposition towards separating the branches of government?'}, '56cd7d3262d2951400fa6632': {'truth': 'third-party vendors', 'predicted': 'third-party vendors of iPod replacement batteries', 'question': 'Whose directions can be followed to interact with iPod batteries?'}, '56d131c817492d1400aabbe0': {'truth': 'batteries', 'predicted': 'batteries are not designed to be removed or replaced by the user, although some users have been able to open the case themselves, usually following instructions from third-party vendors of iPod replacement batteries. Compounding the problem, Apple initially would not replace worn-out batteries', 'question': 'Which iPod component did Apple somewhat inconveniently made non-replaceable?'}, '56d2706859d6e41400145fdc': {'truth': 'Chinese', 'predicted': 'Chinese translations of Mahāyāna texts', 'question': 'Most of the early extant evidence for the origins of Mhayana comes from what type of translations?'}, '56d2706859d6e41400145fdf': {'truth': '1st', 'predicted': '1st century BCE', 'question': 'Texts concerning Aksobhya Buddha were probably composed in what century BCE?'}, '5726b75add62a815002e8dd0': {'truth': 'they must be retained by the organization for its self-preservation, expansion, or plans', 'predicted': 'must be retained by the organization for its self-preservation, expansion, or plans', 'question': 'How does an NPO have to handle surplus money?'}, '5726b75add62a815002e8dd1': {'truth': 'controlling members or a board of directors', 'predicted': '', 'question': 'Who makes most of the decisions for an NPO?'}, '5726b75add62a815002e8dd2': {'truth': 'Many have paid staff including management, whereas others employ unpaid volunteers and even executives who work with or without compensation', 'predicted': 'paid staff including management', 'question': 'How do NPOs handle staffing?'}, '5726b75add62a815002e8dd3': {'truth': 'to meet legal requirements for establishing a contract between the executive and the organization', 'predicted': 'meet legal requirements for establishing a contract between the executive and the organization', 'question': 'What are token fees used for?'}, '5a45597219a820001a1eda2a': {'truth': '', 'predicted': 'members or a board of directors', 'question': 'What control does management have in an NPO?'}, '56dd2ff966d3e219004dac33': {'truth': 'cohabitation', 'predicted': 'political) cohabitation', 'question': 'What is the term for a situation in which the president and prime minister come from different political parties?'}, '56dd2ff966d3e219004dac34': {'truth': 'resignation of the government', 'predicted': 'force the resignation of the government', 'question': 'What can the French parliament cause in order to oust the prime minister?'}, '56e02cb57aa994140058e2f9': {'truth': 'Englishman Sir Francis Drake', 'predicted': 'Sir Francis Drake', 'question': 'Who probably located the island on their final leg of their circumnavigation trip?'}, '572a7a17be1ee31400cb802a': {'truth': 'Port of Miami', 'predicted': 'The Port of Miami', 'question': 'What is the busiest cruise port in the world?'}, '5ad3903d604f3c001a3fe64b': {'truth': '', 'predicted': 'MIA', 'question': \"What is Florida's busiest seaport?\"}, '5ad3903d604f3c001a3fe64e': {'truth': '', 'predicted': 'Brickell Avenue', 'question': 'What mountain is central to the financial district of Miami?'}, '5ad3903d604f3c001a3fe64f': {'truth': '', 'predicted': 'South America', 'question': 'Along with the Caribbean, from where does a significant amount of cargo enter MMI?'}, '572fe936b2c2fd14005685c0': {'truth': 'electrons', 'predicted': 'moving electrons', 'question': \"What particle's migration does the resonance principle rely on?\"}, '572fe936b2c2fd14005685c1': {'truth': 'reflective', 'predicted': 'reflective surface', 'question': 'What type of surface is formed by the tip of a conductor?'}, '572fe936b2c2fd14005685c2': {'truth': 'perpendicular', 'predicted': 'perpendicular to the line from the antenna to the source', 'question': 'How would you place the conductor in relation to the signal you wished to obtain?'}, '5727b0563acd2414000de9cf': {'truth': 'the Tibetan Empire', 'predicted': 'Tibetan Empire', 'question': 'Which empire ravished the Chengdu region with constant warfare and economic distress?'}, '5a512d7dce860b001aa3fbf0': {'truth': '', 'predicted': 'Emperor Xuanzong of Tang', 'question': \"Who fled from Sichuan to Chang'an?\"}, '5a68e9e48476ee001a58a87e': {'truth': '', 'predicted': 'Tang dynasty', 'question': 'During which dynasty did Sichuan regain its political and cultural prominence for which it was known during the Tibetan Empire?'}, '5a68e9e48476ee001a58a87f': {'truth': '', 'predicted': 'Du Fu', 'question': \"Who was known as China's greatest poet and lived in Tang?\"}, '5a68e9e48476ee001a58a882': {'truth': '', 'predicted': 'Tibetan Empire', 'question': 'Which empire ravished the Lushan region with constant warfare and economic distress?'}, '572ec21cdfa6aa1500f8d34d': {'truth': \"They purged monarchists and members of Idris' Senussi clan from Libya's political world and armed forces\", 'predicted': \"purged monarchists and members of Idris' Senussi clan from Libya's political world and armed forces\", 'question': 'What did the RCC do with remnants of the monarchy?'}, '572ec21cdfa6aa1500f8d34e': {'truth': 'Idris was sentenced to execution in absentia.', 'predicted': 'was sentenced to execution in absentia', 'question': 'What happened to Idris?'}, '572ec21cdfa6aa1500f8d34f': {'truth': 'Gaddafi believed this elite were opposed to the will of the Libyan people and had to be expunged.', 'predicted': 'opposed to the will of the Libyan people', 'question': 'How did Gaddafi view the elite?'}, '57317177a5e9cc1400cdbf6e': {'truth': 'monarchist politicians', 'predicted': 'monarchist politicians and journalists', 'question': \"Along with journalists, who was tried in the People's Courts?\"}, '57264c62dd62a815002e80d0': {'truth': 'rare', 'predicted': 'extremely rare', 'question': 'Are written records of Old Dutch rare or common?'}, '57264c62dd62a815002e80d1': {'truth': 'the Salic law', 'predicted': 'Salic law', 'question': 'What Frankish document contains the oldest recorded instance of Dutch?'}, '57264c62dd62a815002e80d4': {'truth': 'the Utrecht baptismal vow', 'predicted': 'Utrecht baptismal vow', 'question': 'What historically significant Dutch document begins with the phrase \"Forsachistu diobolae\"?'}, '5728b6bbff5b5019007da540': {'truth': '16.3 °C (61.3 °F)', 'predicted': '16.3 °C', 'question': 'What is the average temperature of the Baltic Islands?'}, '56cf6057aab44d1400b89178': {'truth': 'classic soul records', 'predicted': 'classic soul', 'question': 'What types of records did Kanye sample in his early career.'}, '56dd37fe66d3e219004dac77': {'truth': 'non-Commonwealth countries', 'predicted': 'non-Commonwealth', 'question': 'In what kinds of nations can the head of government attain the title of Excellency?'}, '56dd37fe66d3e219004dac7a': {'truth': 'Canada', 'predicted': 'Prime Minister of Canada', 'question': 'What is an example of a country where prime ministers can be called Right Honourable solely because of their position?'}, '5acfc9ba77cf76001a685fb7': {'truth': '', 'predicted': 'prime ministers and former prime ministers', 'question': 'Who is called Honourable in non Commonwealth countries?'}, '5728171b4b864d1900164454': {'truth': '128 code points', 'predicted': '', 'question': 'What is the ISCII standard? '}, '5acd579107355d001abf3def': {'truth': '', 'predicted': 'Chinese National Standard organization', 'question': 'Which organization successfully argued for the Tibetan script?'}, '56fdee67761e401900d28c58': {'truth': 'an assembler.', 'predicted': 'an assembler', 'question': 'Programs that convert assembly language into machine language are called what?'}, '57288d3b3acd2414000dfae3': {'truth': 'College of Engineering', 'predicted': '', 'question': 'Which BYU college was founded by former alumnus Harvey Fletcher?'}, '57288d3b3acd2414000dfae4': {'truth': 'the electronic television', 'predicted': '', 'question': 'What did alumnus Philo T. Farnsworth invent before receiving his honorary degree from the college?'}, '57288d3b3acd2414000dfae5': {'truth': 'Tracy Hall', 'predicted': 'H. Tracy Hall', 'question': 'Which notable former BYU student invented the man-made diamond?'}, '57288d3b3acd2414000dfae6': {'truth': 'a new type of diamond press, the tetrahedral press', 'predicted': '', 'question': 'What did former student Tracy Hall invent as a BYU professor of chemistry and Director of Research?'}, '5acea46832bba1001ae4aeae': {'truth': '', 'predicted': 'electronic television', 'question': 'What did Harvey Farnsworth invent?'}, '5acea46832bba1001ae4aeaf': {'truth': '', 'predicted': 'Robert Millikan', 'question': 'Who did Harvey Farnsworth carry out the oil-drop experiment with?'}, '5acea46832bba1001ae4aeb0': {'truth': '', 'predicted': 'man-made diamond', 'question': 'What did Tracy H. Hall invent?'}, '5acea46832bba1001ae4aeb1': {'truth': '', 'predicted': 'tetrahedral press', 'question': 'What type of press did Tracy H. Hall invent?'}, '5731320605b4da19006bce88': {'truth': 'the Wise', 'predicted': '\"the Wise', 'question': 'What was Yaroslav also known as?'}, '5ad0124077cf76001a6868b0': {'truth': '', 'predicted': 'Eupraxia', 'question': 'Who was the daughter of Vladimir the Great?'}, '5ad0124077cf76001a6868b2': {'truth': '', 'predicted': '1019', 'question': \"What year did Yaroslav's sons develop the Kiev Pechersk Lavra?\"}, '572ea6d5cb0c0d14000f13f0': {'truth': 'Through the force of sheer numbers, the English-speaking American settlers entering the Southwest established their language, culture, and law', 'predicted': '', 'question': \"Why isn't the southwest Spanish speaking?\"}, '572ea6d5cb0c0d14000f13f1': {'truth': 'United States never developed bilingualism as Canada did.', 'predicted': '', 'question': 'Is Canada bilingual?'}, '572ea6d5cb0c0d14000f13f2': {'truth': 'California constitutional convention of 1849 had eight Californio participants; the resulting state constitution was produced in English and Spanish,', 'predicted': 'California constitutional convention of 1849 had eight Californio participants; the resulting state constitution was produced in English and Spanish', 'question': 'Was California a bilingual state?'}, '572ea6d5cb0c0d14000f13f3': {'truth': \"the convention's English-speaking participants felt that the state's remaining minority of Spanish-speakers should simply learn English\", 'predicted': '', 'question': \"Why didn't California officially become  bilingual?\"}, '572ea6d5cb0c0d14000f13f4': {'truth': 'the convention ultimately voted 46-39 to revise the earlier clause so that all official proceedings would henceforth be published only in English.', 'predicted': 'the convention ultimately voted 46-39 to revise the earlier clause so that all official proceedings would henceforth be published only in English', 'question': 'Was there a court ruling?'}, '570c5037b3d812140066d0c0': {'truth': 'five children', 'predicted': 'five', 'question': 'How many children did John and Isabella have?'}, '56dee346c65bf219000b3ddf': {'truth': 'functionalism, anomalous monism, identity theory', 'predicted': 'identity theory', 'question': 'In regards to the mind, what are 3 theories that modern day philosophers try to harmonize?'}, '5727011b708984140094d843': {'truth': 'Heavy Brigade', 'predicted': 'the Heavy Brigade', 'question': \"Who countered the Russian cavalry's movement?\"}, '56e4cfd839bdeb14003479df': {'truth': 'solitary housing estates and suburban sprawl.', 'predicted': '', 'question': 'What kinds of buildings and building developments are the new movements not in favor of?'}, '56e4cfd839bdeb14003479e1': {'truth': 'modernist and globally uniform architecture', 'predicted': '', 'question': 'What older architectural movements do the newer movements not go along with?'}, '57301a30b2c2fd140056886e': {'truth': 'Santa Catalina Mountains', 'predicted': '', 'question': 'What mountains is Oro Valley next to?'}, '573428b44776f419006619c9': {'truth': 'in the western foothills of the Santa Catalina Mountains', 'predicted': 'western foothills of the Santa Catalina Mountains', 'question': 'Where is Oro Valley?'}, '572823804b864d190016454f': {'truth': '1897', 'predicted': '', 'question': \"In what year was Queen Victoria's Diamond Jubilee?\"}, '5acfa94777cf76001a685795': {'truth': '', 'predicted': 'English Civil War', 'question': 'What war strenthened the monarchs position?'}, '5acfa94777cf76001a685798': {'truth': '', 'predicted': 'Bill of Rights', 'question': 'What bill gave the monarch the power to establish law and impose taxes?'}, '56f72fe03d8e2e1400e37403': {'truth': 'popular styles', 'predicted': 'most popular styles', 'question': 'What is usually written in song forms?'}, '56f72fe03d8e2e1400e37404': {'truth': 'sophisticated', 'predicted': 'highly sophisticated', 'question': 'The concerto, symphony, sonata and opera are examples of what type of musical forms?'}, '572b6f49111d821400f38e9d': {'truth': 'Sense without Matter', 'predicted': '', 'question': 'What book was written by A.A. Luce?'}, '5a7c7516e8bc7e001a9e1e25': {'truth': '', 'predicted': 'vocabulary', 'question': \"What aspect of Berkeley's writing did Foster modernize?\"}, '57268c78708984140094c9bb': {'truth': 'Clifford and Angela Levin', 'predicted': 'Angela Levin', 'question': 'Who wrote Max Clifford: Read All About It?'}, '57268c78708984140094c9bc': {'truth': 'Starr', 'predicted': '', 'question': 'Who was writing a book with McCaffrey?'}, '57268c78708984140094c9be': {'truth': 'the attention helped to revive his career', 'predicted': 'helped to revive his career', 'question': \"How did attention from the story impact Starr's career?\"}, '56e0b2127aa994140058e6b0': {'truth': 'phosphorus', 'predicted': '', 'question': 'What does the symbol P represent?'}, '56dfc0ae231d4119001abd96': {'truth': 'An upstream ISP usually has a larger network than the contracting ISP', 'predicted': '', 'question': 'Why does an ISP need to pay an upstream ISP?'}, '56dfc0ae231d4119001abd97': {'truth': 'access to parts of the Internet the contracting ISP by itself has no access to', 'predicted': 'access to parts of the Internet', 'question': 'What does an upstream ISP provide for an ISP?'}, '5a10dee906e79900185c343b': {'truth': '', 'predicted': 'Internet access', 'question': 'What do ISPs pay customers for?'}, '5a10dee906e79900185c343d': {'truth': '', 'predicted': 'Internet access', 'question': 'What does an upstream ISP provide for customers?'}, '5a10dee906e79900185c343e': {'truth': '', 'predicted': 'parts of the Internet', 'question': 'What does the contracting ISP have access to that the upstream ISP does not?'}, '572ed3f503f9891900756a69': {'truth': 'Syriac', 'predicted': '', 'question': 'Which older language is thought to strongly resemble that of the Quran?'}, '56f739203d8e2e1400e3749d': {'truth': 'to create an obligation under international law', 'predicted': 'an obligation', 'question': 'North Korea and the United States have been characterized by a disagreement over one parties desire to create what with respect to security guarantees and nuclear proliferation?'}, '56d632371c85041400946fe3': {'truth': 'the face or neck.', 'predicted': 'face or neck', 'question': 'Children are often bit where by dogs?'}, '572f068ccb0c0d14000f1718': {'truth': 'most US and Canadian jurisdictions', 'predicted': 'US and Canadian', 'question': 'In what neighboring countries are passenger elevators required to adhere to Standard A17.1?'}, '56d0007f234ae51400d9c246': {'truth': '22,700 L (5,000 imp gal; 6,000 US gal) per day', 'predicted': '', 'question': 'How much water was produced by the plant?'}, '56d0007f234ae51400d9c247': {'truth': 'single-slope', 'predicted': '', 'question': 'What is an example of a solar distillation design?'}, '57303bb004bcaa1900d773f1': {'truth': 'The Northern Ireland Peace Process', 'predicted': '', 'question': 'What has caused several uncommon arrangements between the various states in the United Kingdom?'}, '57303bb004bcaa1900d773f2': {'truth': 'choice of Irish or British citizenship or both', 'predicted': 'British', 'question': 'What type of citizenship can Northern Ireland people have?'}, '57303bb004bcaa1900d773f3': {'truth': 'policies common across the island of Ireland', 'predicted': '', 'question': 'The 1998 Good Friday Agreement resulted in what arrangement?'}, '5acda46b07355d001abf48a2': {'truth': '', 'predicted': 'the Republic of Ireland, Northern Ireland and the United Kingdom', 'question': 'The Northern Atlantic Peace Process involves arrangements between which kingdoms?'}, '5733b2e14776f41900661087': {'truth': 'human behavior and cultural practices', 'predicted': 'patterns of past human behavior and cultural practices', 'question': 'What can archaeologists deduce from material remains?'}, '5733b2e14776f41900661088': {'truth': 'past human groups', 'predicted': '', 'question': 'What do Ethnoarchaeologists gain a better understanding of by studying living human groups?'}, '5733b2e14776f41900661089': {'truth': 'in similar ways', 'predicted': '', 'question': 'How are long dead human groups presumed to have lived and behaved as compared to still living populations?'}, '57318ae9e6313a140071d07f': {'truth': '30', 'predicted': 'over 30', 'question': 'About how many governments recognized the legitimacy of the NTC at a meeting on July 15, 2011?'}, '572fb5b0b2c2fd1400568396': {'truth': 'is not a reproductive process', 'predicted': 'not a reproductive process', 'question': 'Is creating endospore a reproductive process?'}, '572fb5b0b2c2fd1400568397': {'truth': 'cortex layer', 'predicted': 'a cortex layer', 'question': 'What are ribosomes in endospores are enclosed in?'}, '57278133dd62a815002e9eec': {'truth': 'form roots and shoots', 'predicted': 'to form roots and shoots by controlling the concentration of growth hormones', 'question': 'What can plant callus be coaxed into doing?'}, '572a7b02111d821400f38b53': {'truth': 'El Paso', 'predicted': '', 'question': 'What was the second poorest US city in 2004?'}, '572a7b02111d821400f38b56': {'truth': 'fifth', 'predicted': 'fifth-richest', 'question': 'In terms of purchasing power, where did Miami rank among world cities in a 2009 UBS study?'}, '5ad39216604f3c001a3fe693': {'truth': '', 'predicted': 'fifth', 'question': 'In terms of purchasing power, where did Miami rank among world cities in a 2008 UBS study?'}, '572f8622947a6a140053ca1b': {'truth': 'the rise of Nazism', 'predicted': 'Nazism', 'question': 'What did the Convulsion caused by the global depression resul in?'}, '570c27686b8089140040fb9b': {'truth': 'communist ties', 'predicted': 'communist', 'question': 'What ties did the FBI believe civil rights leaders had?'}, '570c27686b8089140040fb9d': {'truth': 'FBI', 'predicted': '', 'question': 'What agency had Dr. T.R.M. Howard criticized?'}, '5ad37d89604f3c001a3fe418': {'truth': '', 'predicted': 'communist', 'question': 'What ties did civil rights leaders believe the FBI had?'}, '5ad37d89604f3c001a3fe419': {'truth': '', 'predicted': '1956', 'question': 'When did Dr. T.R.M. Howard send an open letter denouncing Hoover?'}, '57096fa9ed30961900e84123': {'truth': 'animal emotions, animal culture and learning', 'predicted': 'personal symbolic name use, animal emotions, animal culture and learning, and even sexual conduct', 'question': 'What are some fields of knowledge concerning the animal world that have been revolutionizes in the 21st century?'}, '59fc23cca9fb160018f10da3': {'truth': '', 'predicted': 'human communication', 'question': 'Anthroposemiotics is the study of animal what?'}, '59fc23cca9fb160018f10da4': {'truth': '', 'predicted': 'animal communication', 'question': 'Zoo semiotics is the study of human what?'}, '59fc23cca9fb160018f10da6': {'truth': '', 'predicted': 'Animal communication', 'question': 'What is defined as any one behavior of all animals that affects the current or future behavior of another animal?'}, '59fc23cca9fb160018f10da7': {'truth': '', 'predicted': '21st century', 'question': 'Human and animal communication is a rapidly growing field of study in what century.'}, '5726dde0f1498d1400e8edf4': {'truth': 'at least eight', 'predicted': 'eight', 'question': 'How many strikes has Yale had since 1968?'}, '5726dde0f1498d1400e8edf5': {'truth': 'the worst record of labor tension of any university in the U.S.', 'predicted': '', 'question': \"What are The New York Times' views on Yale's labor tension?\"}, '5726dde0f1498d1400e8edf6': {'truth': 'Professor David Graeber', 'predicted': 'David Graeber', 'question': 'What professor was retired in a 2003 labor strike?'}, '5726dde0f1498d1400e8edf7': {'truth': 'he came to the defense of a student who was involved in campus labor issues.', 'predicted': 'he came to the defense of a student who was involved in campus labor issues', 'question': 'Why was Professor David Graeber retired during the strike?'}, '5726dde0f1498d1400e8edf8': {'truth': \"Yale's unusually large endowment\", 'predicted': '', 'question': 'What adds to the tensions during wage considerations?'}, '5ad3e20e604f3c001a3ff523': {'truth': '', 'predicted': 'David Graeber', 'question': 'What professor was retired in a 2013 labor strike?'}, '5ad3e20e604f3c001a3ff524': {'truth': '', 'predicted': 'he came to the defense of a student who was involved in campus labor issues', 'question': 'Why was Professor David Graeber hired during the strike?'}, '56f986ed9b226e1400dd1510': {'truth': 'the forebrain', 'predicted': 'forebrain', 'question': 'The hypothalamus is located at the base of what?'}, '56f986ed9b226e1400dd1511': {'truth': 'the hypothalamus,', 'predicted': 'hypothalamus', 'question': 'In vertebrates, the most important part of the brain is what?'}, '56f986ed9b226e1400dd1513': {'truth': 'the pituitary gland', 'predicted': 'pituitary', 'question': 'The gland directly underneath the hypothalamus is which gland?'}, '56f986ed9b226e1400dd1514': {'truth': 'the bloodstream', 'predicted': 'bloodstream', 'question': 'The pituitary gland sends hormones through what in the body?'}, '57324359b9d445190005e949': {'truth': 'Presbyterian', 'predicted': 'Presbyterian Church', 'question': 'What church did Eisenhower join in 1953?'}, '573106b7497a881900248b07': {'truth': 'only France and the United Kingdom', 'predicted': 'France and the United Kingdom', 'question': 'What 2 powers named in the 5 orignal great powers of the congress of vienna have maintained that status?'}, '573106b7497a881900248b0a': {'truth': 'European', 'predicted': 'European politics', 'question': 'Balance of power of great powers was a major influence on what continents politics?'}, '5a149f88a54d4200185292b1': {'truth': '', 'predicted': 'France and the United Kingdom', 'question': 'What two powers involved in the Franco-Prussian war have the same status today?'}, '5a149f88a54d4200185292b2': {'truth': '', 'predicted': 'British Empire', 'question': 'After WWII which country was the pre-eminent power?'}, '5a149f88a54d4200185292b3': {'truth': '', 'predicted': 'The balance of power between the Great Powers', 'question': 'What became a main influence for the Congress of Vienna?'}, '5a149f88a54d4200185292b5': {'truth': '', 'predicted': 'France', 'question': 'What pre-eminent power lost the Franco-Prussian war?'}, '5727c1123acd2414000debb5': {'truth': 'Portugal and Catalonia', 'predicted': 'Portugal and Catalonia, the Junta changed its attitude, this time due to the exhaustion of Galicia, now involved not just in naval or oversea operations, but also in an exhausting war with the Portuguese', 'question': 'War broke out with which other countries?'}, '5727c1123acd2414000debb6': {'truth': 'second half of the 17th century', 'predicted': '17th century', 'question': 'When did the Galician Junta more often stand up to requests from the monarch?'}, '5727c1123acd2414000debb7': {'truth': 'there were frequent urban mutinies', 'predicted': '', 'question': 'In what way was the tension between the monarch and Galicia similar to the wars it was fighting?'}, '572a23643f37b31900478727': {'truth': 'crop domestication and sedentary lifestyles', 'predicted': '', 'question': 'What major trends appeared in Mesoamerica during 4500 BC?'}, '572a23643f37b3190047872a': {'truth': 'bow and arrow', 'predicted': '', 'question': 'What hunting weapon was found in the Southwestern US during  500 to 1200 C.E.?'}, '5a7d1cfd70df9f001a874fad': {'truth': '', 'predicted': '500 to 1200 C.E.', 'question': 'When was there a dramatic increase in turkeys?'}, '56fa5493f34c681400b0c086': {'truth': 'wooden spoon', 'predicted': '', 'question': 'What wooden utensil could you stir a pot of soup with?'}, '56fa5493f34c681400b0c088': {'truth': 'beds', 'predicted': 'chairs and beds. It is also used for tool handles and cutlery, such as chopsticks, toothpicks, and other utensils, like the wooden spoon', 'question': 'What pieces of furniture that most people use every night can be made out of wood?'}, '570b6593ec8fbc190045b9d4': {'truth': 'Adrenalize', 'predicted': 'Hysteria with Adrenalize', 'question': \"What was the title of Def Leppard's 1992 album?\"}, '5a5a45ab9c0277001abe70e9': {'truth': '', 'predicted': '1991', 'question': 'What year did Ozzy Osbourne release Unlawful Carnal Knowledge?'}, '572849ebff5b5019007da0fc': {'truth': 'drones', 'predicted': '', 'question': 'What controversial technology did the US use in Pakistan?'}, '572849ebff5b5019007da0fd': {'truth': 'the Central Intelligence Agency', 'predicted': 'Central Intelligence Agency', 'question': 'Which US agency runs its drones in Pakistan?'}, '5a85e80ab4e223001a8e72c2': {'truth': '', 'predicted': 'Global War on Terror', 'question': 'What did the United States criticize?'}, '56cd8ffa62d2951400fa6723': {'truth': 'Japan', 'predicted': 'Japanese', 'question': 'What country does Akiko Komoto come from?'}, '5a8d914edf8bba001a0f9b0b': {'truth': '', 'predicted': 'nods and facial expressions', 'question': \"Through what can Link's verbalizations be discerned?\"}, '5a8d914edf8bba001a0f9b0f': {'truth': '', 'predicted': 'grunts', 'question': 'What does Zelda say when attacking?'}, '57096446ed30961900e8406d': {'truth': 'Lahaul Spiti', 'predicted': '', 'question': 'Who was last in population strength?'}, '5a3625b0788daf001a5f875a': {'truth': '', 'predicted': '21st', 'question': 'Where on the population chart is the Shimla district census wise, after being followed by Tripura?'}, '5a3625b0788daf001a5f875b': {'truth': '', 'predicted': '530,164', 'question': 'What place was Sirmaur district, that followed the state?'}, '5a3625b0788daf001a5f875e': {'truth': '', 'predicted': '454,293', 'question': 'If Hamirpur district is top ranked, what is its population?'}, '5727ae3e2ca10214002d9382': {'truth': 'Vedānta', 'predicted': 'Vedānta school', 'question': 'Which is the most developed and well known of the Hindu schools?'}, '5727ae3e2ca10214002d9383': {'truth': 'five or six methods', 'predicted': 'five or six', 'question': 'How many ways did the Vedantins have of gaining knowledge?'}, '5727ae3e2ca10214002d9384': {'truth': 'sub-school', 'predicted': '', 'question': 'On what was dependent for the choice of methods in gaining knowledge?'}, '57315bfaa5e9cc1400cdbf01': {'truth': 'Bastille', 'predicted': 'fall of Bastille', 'question': 'After the demise of what was the red flag linked to the French Revolution?'}, '5ad4fc2f5b96ef001a10a88f': {'truth': '', 'predicted': '\"Against us they have raised the bloody flag of tyranny', 'question': 'What did Lisle de Rouget write?'}, '572fffeb947a6a140053cf3a': {'truth': 'ball grid array', 'predicted': 'ball grid array (BGA) and naked die technologies', 'question': 'To what type of packaging is thermal expansion particularly critical?'}, '572fffeb947a6a140053cf3c': {'truth': 'FR-6', 'predicted': '', 'question': 'What dielectric is matte glass and polyester?'}, '5ace942432bba1001ae4aace': {'truth': '', 'predicted': 'Thermal expansion', 'question': 'The ball gasket array (BGA) is an important what?'}, '573035c8b2c2fd1400568a7b': {'truth': 'San Diego International Airport (SAN)', 'predicted': 'San Diego International Airport', 'question': 'What is the more popular name of Lindbergh Field?'}, '573035c8b2c2fd1400568a7d': {'truth': 'give direct access to Tijuana International Airport', 'predicted': 'direct access to Tijuana International Airport', 'question': 'What is the purpose of the Tijuana Cross-border Terminal?'}, '572f83ae947a6a140053c9fa': {'truth': 'the onset of the Great Depression', 'predicted': '', 'question': 'What changed worldwide property drasrically?'}, '572f83ae947a6a140053c9fd': {'truth': '1930s or early 1940s', 'predicted': 'the 1930s or early 1940s', 'question': 'When did the great Depression end?'}, '572f83ae947a6a140053c9fe': {'truth': '20th century', 'predicted': '', 'question': 'The great depression is the worst economic downturn of what century?'}, '57109aada58dae1900cd6ac2': {'truth': 'the Parisian lodge', 'predicted': 'Parisian', 'question': 'Which lodge that met in the mid 1720s was composed of English Jacobite exiles?'}, '57109aada58dae1900cd6ac3': {'truth': 'British', 'predicted': 'British lodges', 'question': 'Which lodges assigned themselves the duty to \"initate the unenlightened\"?'}, '5730c888aca1c71400fe5ab7': {'truth': 'replacements for incandescent and neon indicator lamps', 'predicted': 'replacements for incandescent and neon indicator lamps, and in seven-segment displays', 'question': 'What was the first commercial uses of LEDs?'}, '5730c888aca1c71400fe5aba': {'truth': 'Hewlett Packard (HP)', 'predicted': 'Hewlett Packard', 'question': 'What modern company introduced LEDs in 1968?'}, '57282ec23acd2414000df67d': {'truth': 'Ukrainian', 'predicted': '', 'question': \"What country's independence were the chain members celebrating?\"}, '57282ec23acd2414000df67f': {'truth': 'the Democratic Bloc', 'predicted': 'Democratic Bloc', 'question': 'In 1990 which party had most of the election victories?'}, '572848e94b864d19001648c6': {'truth': 'video signal-to-noise ratio and bandwidth', 'predicted': '', 'question': 'What features do LaserDiscs lack in, causing DVDs to appear sharper and clearer?'}, '5728b383ff5b5019007da4de': {'truth': 'Half the deported perished', 'predicted': 'Half', 'question': 'What percentage of Estonians died after deporation?'}, '5728b383ff5b5019007da4df': {'truth': 'the early 1960s', 'predicted': 'early 1960s', 'question': 'When were the deported Estonians allowed to return?'}, '5728b383ff5b5019007da4e0': {'truth': \"Stalin's death\", 'predicted': \"Stalin's death).[citation needed] The activities of Soviet forces\", 'question': 'What event led to the return of Estonians back home?'}, '5728b383ff5b5019007da4e1': {'truth': 'the Forest Brothers', 'predicted': 'Forest Brothers', 'question': 'Who fought a guerrilla war against the Soviets?'}, '572a3b486aef0514001553ba': {'truth': 'systematic works in biology', 'predicted': '', 'question': \"What did August von Hayek's father write?\"}, '56d109f317492d1400aab7cd': {'truth': 'Hawaii', 'predicted': 'Honolulu, Hawaii', 'question': 'In what state did Kanye West record them majority of his fourth album?'}, '56d109f317492d1400aab7ce': {'truth': '\"Love Lockdown\"', 'predicted': 'Love Lockdown', 'question': \"What was the first song released off of Kanye's fourth album?\"}, '56e08070231d4119001ac201': {'truth': '5', 'predicted': '4 to 5', 'question': 'How many digits did Saint Helena change their phone numbers to?'}, '5730878f2461fd1900a9ce8f': {'truth': 'Slavic tribes', 'predicted': '', 'question': 'Who populated the area between the Baltic Sea and the Black sea before Kievan Rus?'}, '5730878f2461fd1900a9ce90': {'truth': 'Novgorod', 'predicted': '', 'question': 'Where were the Limen Slavs located before Kievan Rus?'}, '5acff7c077cf76001a68669e': {'truth': '', 'predicted': 'between the Baltic Sea and Black Sea', 'question': 'Which lands were primarily populated by western Slavic tribes?'}, '5acff7c077cf76001a6866a0': {'truth': '', 'predicted': 'Ilmen Slavs and neighboring Krivichi', 'question': 'Who occupied the headwaters of the East Dvina?'}, '56d2581659d6e41400145edc': {'truth': 'Atticus', 'predicted': '', 'question': 'Who was the only non-abusive father mentioned?'}, '572fc229b2c2fd1400568403': {'truth': 'Dowding', 'predicted': '', 'question': 'Who was reluctant to act quickly when urgent changes need to be made?'}, '572fc229b2c2fd1400568404': {'truth': 'Air Staff', 'predicted': 'The Air Staff', 'question': \"Who thought Dowding was stubborn and didn't like to cooperate?\"}, '572fc229b2c2fd1400568405': {'truth': 'Battle of Britain Day', 'predicted': 'Battle of Britain Day and the Big Wing', 'question': 'The Air Ministry was critical of Dowding after which battle?'}, '571de3ebb64a571400c71ddb': {'truth': 'elder family members', 'predicted': '', 'question': 'Who will not reveal full ancestral data to mixed race people?'}, '5ad2b36bd7d075001a429f91': {'truth': '', 'predicted': 'Tracing the genealogy of African Americans', 'question': 'What is usually a very easy process?'}, '5ad2b36bd7d075001a429f92': {'truth': '', 'predicted': 'censuses', 'question': 'What identified slaves by name before the American Civil War?'}, '572759cb5951b619008f8890': {'truth': 'Nylon', 'predicted': '', 'question': 'What was manufactured completely from petrochemicals?'}, '5a8192e331013a001a334ccd': {'truth': '', 'predicted': 'Nylon', 'question': 'What was the first polyester fiber?'}, '5732a3dfcc179a14009dabc2': {'truth': '66 million years', 'predicted': '66 million', 'question': 'How many years long was the Cenozoic Era?'}, '5732a3dfcc179a14009dabc4': {'truth': 'Cenozoic Era', 'predicted': 'The Cenozoic Era', 'question': 'What geologic period are we in currently?'}, '5732a3dfcc179a14009dabc6': {'truth': 'the Himalayas', 'predicted': 'Himalayas', 'question': 'The collision of the Indian sub continent and the Asian plate created which mountain range?'}, '56d8dc9cdc89441400fdb353': {'truth': 'Uyghurs living in Turkey', 'predicted': 'Uyghurs', 'question': 'Who protested for their compatriots who were in Xinjiang?'}, '56db0a87e7c41114004b4cb3': {'truth': 'Taksim Square.', 'predicted': 'Taksim Square', 'question': 'Where did the torch relay finish in Turkey?'}, '571a993510f8ca140030518e': {'truth': 'modern-day Italians', 'predicted': 'Italians', 'question': 'The 2010 study found that what modern population is most closely related to Ashkenazi Jews?'}, '571a993510f8ca140030518f': {'truth': 'inter-marriage and conversions in the time of the Roman Empire', 'predicted': 'inter-marriage and conversions', 'question': 'Ashkenazi Jews and Italians may be genetically similar due to what two factors?'}, '56e168ebe3433e1400422ec7': {'truth': 'its stake in DuPont', 'predicted': 'DuPont', 'question': 'What did Seagram sell to finance their purchase of a share in MCA/Universal?'}, '5ad15e76645df0001a2d18e2': {'truth': '', 'predicted': '80%', 'question': 'What percent stake did MCA/Univeral have in Matsushita?'}, '5ad15e76645df0001a2d18e3': {'truth': '', 'predicted': 'Seagram', 'question': 'How did MCA/Universal sell their stake to?'}, '5ad15e76645df0001a2d18e5': {'truth': '', 'predicted': '1999', 'question': 'What year did PolyGram buy Seagram?'}, '56ce659aaab44d1400b88749': {'truth': 'toxic chemicals', 'predicted': 'toxic chemicals that make the water unusable', 'question': 'What is a possible negative effect of algae in water stabilization ponds?'}, '56ce451caab44d1400b88640': {'truth': 'he died', 'predicted': '', 'question': \"Why didn't Yonten Gyatso make it to Beijing?\"}, '571aeaf69499d21900609baa': {'truth': 'supported the tenets', 'predicted': 'They supported the tenets of Origenist thought and theology', 'question': 'Were Arians also Origenists?'}, '571aeaf69499d21900609bab': {'truth': 'bishops disagreed', 'predicted': '', 'question': 'Did the bishops consider themselves Arians?'}, '571aeaf69499d21900609bac': {'truth': 'a real theological ideology', 'predicted': '', 'question': 'What did the Council of Nicaea decide about Arianism?'}, '56defb12c65bf219000b3e73': {'truth': 'the Commander of the Royal Canadian Air Force', 'predicted': 'Commander', 'question': 'Who heads the Royal Canadian Air Force?'}, '56defb12c65bf219000b3e74': {'truth': 'Winnipeg', 'predicted': '', 'question': 'Where is the commander based out of?'}, '56defb12c65bf219000b3e77': {'truth': 'tactical commander', 'predicted': 'single tactical commander', 'question': 'Who reports to the operational commander about the wings?'}, '5726da10dd62a815002e929d': {'truth': 'fall of 1939 till spring of 1940', 'predicted': '', 'question': 'How long did liquidation occur?'}, '5726da10dd62a815002e929e': {'truth': '16,000 members', 'predicted': '', 'question': ' How many intelligentia were killed during operation AB-Akiton?'}, '56cbd2356d243a140015ed66': {'truth': 'Polish and French', 'predicted': 'Polish', 'question': \"What was Frédéric's nationalities?\"}, '56cbd2356d243a140015ed67': {'truth': 'Romantic era', 'predicted': 'Romantic', 'question': 'In what era was Frédéric active in?'}, '56cbd2356d243a140015ed69': {'truth': 'Duchy of Warsaw', 'predicted': 'the Duchy of Warsaw', 'question': 'In what area was Frédéric born in?'}, '56ce0a3762d2951400fa69d7': {'truth': 'Romantic era', 'predicted': 'Romantic', 'question': 'What era was Chopin active during?'}, '56cf54a2aab44d1400b89008': {'truth': 'Fryderyk Franciszek Chopin', 'predicted': 'Fryderyk Franciszek', 'question': \"What was Chopin's full name?\"}, '56cf54a2aab44d1400b8900a': {'truth': 'Romantic era', 'predicted': 'Romantic', 'question': 'Chopin was active during what era?'}, '56d1ca30e7d4791d009021a8': {'truth': 'Warsaw', 'predicted': 'Duchy of Warsaw', 'question': 'In what city was Chopin born and raised?'}, '5727a8874b864d19001639bb': {'truth': 'has since been ratified', 'predicted': 'ratified', 'question': 'What has since happened to the second series of bilateral agreements?'}, '5733fa9a4776f41900661625': {'truth': 'European Commission, European Central Bank and International Monetary Fund', 'predicted': 'the European Commission, European Central Bank and International Monetary Fund', 'question': 'By what entities was the Portuguese economy bailed out?'}, '570a5e0e6d058f1900182dba': {'truth': 'one third', 'predicted': '', 'question': 'What portion of females reported that they were held back by managers?'}, '570a5e0e6d058f1900182dbc': {'truth': '2007', 'predicted': '', 'question': 'After the 2003 incident, what was the next major year in which conerns were raised?'}, '5a4875a284b8a4001a7e7890': {'truth': '', 'predicted': 'Alice Gast', 'question': 'Who is the Vice President of Imperial?'}, '56df631296943c1400a5d4ab': {'truth': '39', 'predicted': '', 'question': 'About how many inches of rain fall on Plymouth every year?'}, '572e7f0adfa6aa1500f8d047': {'truth': 'late Bronze Age', 'predicted': 'Bronze Age', 'question': 'During what Age did Cyprus experience two waves of Greek settlement?'}, '572e7f0adfa6aa1500f8d049': {'truth': '1400 BC', 'predicted': '', 'question': 'What year did Turkish Republic of Northern Cyprus begin visiting Cyprus?'}, '57285eda3acd2414000df96f': {'truth': 'much like people', 'predicted': 'behave much like people', 'question': 'How does Pascal Boyer believe that gods and other supernatural beings behave?'}, '57285eda3acd2414000df971': {'truth': \"god concepts are projections of one's father\", 'predicted': '', 'question': 'What did Frued believe about the belief in God?'}, '572c0480dfb02c14005c6b60': {'truth': 'much like people', 'predicted': 'behave much like people', 'question': 'How do supernatural entities act?'}, '570b2117ec8fbc190045b843': {'truth': 'financing', 'predicted': 'lacking the legislation or the financing for large-scale VRS services, and to provide the necessary telecommunication equipment to deaf users', 'question': 'What is one of the reasons why VRS services are not in most European countries?'}, '56e180f5e3433e1400422f96': {'truth': 'uniformity', 'predicted': 'relative uniformity', 'question': 'What do the dialects of Catalan feature?'}, '56e180f5e3433e1400422f97': {'truth': 'other Romance languages', 'predicted': 'Romance languages', 'question': 'In comparison to what are the dialects uniform?'}, '56e180f5e3433e1400422f98': {'truth': 'intelligibility', 'predicted': 'Mutual intelligibility', 'question': 'What is high among dialects?'}, '56e0cfce7aa994140058e737': {'truth': 'Chrome', 'predicted': 'Google Chrome', 'question': 'What other browser has Google as the default search engine?'}, '56e0cfce7aa994140058e739': {'truth': 'Google Chrome', 'predicted': 'development at Google and of Google Chrome', 'question': 'The increased revenue funds what, in addition to Google?'}, '5a4d40ce7a6c4c001a2bbc84': {'truth': '', 'predicted': 'make their engine default', 'question': 'Why do web browsers pay search engine companies?'}, '5726ee155951b619008f82ae': {'truth': 'carrying capacity', 'predicted': '', 'question': 'Prey that is eaten is simply replaced by anohter when the population is close to what?'}, '5a6be2214eec6b001a80a5f4': {'truth': '', 'predicted': 'populations of predator and prey species also interact', 'question': 'What do different species of predators do when they encounter each other?'}, '5a6be2214eec6b001a80a5f5': {'truth': '', 'predicted': 'survival', 'question': 'What do different predators sometimes need each other for?'}, '5728d8e3ff5b5019007da80f': {'truth': 'University of Detroit Mercy', 'predicted': 'Wayne State University, a national research university with medical and law schools in the Midtown area offering hundreds of academic degrees and programs. The University of Detroit Mercy', 'question': 'What Catholic university is in Detroit?'}, '5728d8e3ff5b5019007da811': {'truth': 'Society of Jesus', 'predicted': 'the Society of Jesus (the Jesuits) and the Sisters of Mercy', 'question': 'Which Catholic society is Detroit Mercy affiliated with?'}, '5ad269fad7d075001a4292fc': {'truth': '', 'predicted': 'Synthesizing: Ten Ragas to a Disco Beat', 'question': \"What was the name of Charanjit Singh's 1981 album?\"}, '5ad269fad7d075001a4292fd': {'truth': '', 'predicted': 'Indian ragas', 'question': \"What did Beat's album contains?\"}, '5ad269fad7d075001a4292ff': {'truth': '', 'predicted': 'electronic instrumentation and minimal', 'question': 'What sort of arrangement did Charanjit Singh use on his 1981 album?'}, '572b8fff111d821400f38f1b': {'truth': 'a Less Commonly Taught Language', 'predicted': 'Less Commonly Taught Language', 'question': 'What kind of language is Czech in U.S. schools?'}, '5ad13a0b645df0001a2d12a6': {'truth': '', 'predicted': 'returned to opposition', 'question': 'What did Labour do after winning the 1970 general election?'}, '5ad13a0b645df0001a2d12a7': {'truth': '', 'predicted': 'Harold Wilson', 'question': 'Who won the 1970 general election?'}, '5ad13a0b645df0001a2d12aa': {'truth': '', 'predicted': 'they had fewer seats despite receiving more votes numerically', 'question': 'Why were the Conservatives able to form a government alone?'}, '57266a17708984140094c531': {'truth': 'Federal law and treaties', 'predicted': '', 'question': 'What comes before state and territorial laws in the 50 U.S states?'}, '57266a17708984140094c535': {'truth': 'vary greatly from one state to the next.', 'predicted': '', 'question': 'Does every state have the same laws?'}, '5726d83d708984140094d331': {'truth': 'conflicting state and territorial laws', 'predicted': '', 'question': 'Federal law overrides what laws?'}, '572c9ab3dfb02c14005c6bab': {'truth': 'Federal law and treaties', 'predicted': '', 'question': 'Is there anything that trumps state law?'}, '572c9ab3dfb02c14005c6bad': {'truth': 'as long as they do not infringe on any federal constitutional rights', 'predicted': '', 'question': 'Can states grant rights to citizens that are not defined by the constitution?'}, '572c9ab3dfb02c14005c6bae': {'truth': '\"living law\"', 'predicted': '', 'question': 'What is day-to-day, operational law considered?'}, '5a79baa717ab25001a8a0001': {'truth': '', 'predicted': 'conflicting', 'question': 'State and territorial laws preempt what kind of laws and treaties?'}, '5a79baa717ab25001a8a0003': {'truth': '', 'predicted': 'dual-sovereign', 'question': 'What type of government system do Indian reservations have? '}, '5728b8862ca10214002da658': {'truth': 'Nawab of Bengal Siraj Ud Daulah', 'predicted': 'The Nawab of Bengal Siraj Ud Daulah', 'question': 'What ruler opposed the British use of permits and ended up at war?'}, '5728b8862ca10214002da659': {'truth': 'East India Company', 'predicted': 'Bengal Army of the East India Company', 'question': \"What British company was heavily involved in the defeat of the Nawab's forces?\"}, '5728b8862ca10214002da65a': {'truth': 'Robert Clive', 'predicted': 'Clive', 'question': 'Who did the East India Company appoint as Governor of Bengal?'}, '57283abb4b864d19001647b0': {'truth': 'St. George Cathedral.', 'predicted': 'St. George Cathedral', 'question': 'Where was the August 9 liturgy held?'}, '57283abb4b864d19001647b1': {'truth': '1933', 'predicted': '', 'question': 'Prior to the September 8th rally when was the last Youth for Christ rally held?'}, '57342bbc4776f419006619f1': {'truth': 'Artur Pizarro, Maria João Pires, Sequeira Costa', 'predicted': 'Artur Pizarro', 'question': 'What are some examples of classical pianists from Portugal?'}, '57342bbc4776f419006619f3': {'truth': 'José Vianna da Motta, Carlos Seixas, João Domingos Bomtempo, João de Sousa Carvalho, Luís de Freitas Branco and his student Joly Braga Santos', 'predicted': 'José Vianna da Motta, Carlos Seixas', 'question': 'Who are some notable musical composers from Portugal?'}, '5726d331f1498d1400e8ec6e': {'truth': 'the Berlin Conference', 'predicted': 'Berlin Conference', 'question': \"Where was Britain's claim to West Africa recognized in 1885?\"}, '570b09a96b8089140040f701': {'truth': 'in 1985', 'predicted': '1985', 'question': 'When was Admiral Flota Sovetskovo Soyuza Kuznetsov first launched?'}, '570b09a96b8089140040f702': {'truth': 'Tbilisi', 'predicted': '', 'question': 'What was Admiral Flota Sovetskovo Soyuza Kuznetsov renamed?'}, '570b09a96b8089140040f704': {'truth': 'The P-700 systems', 'predicted': 'P-700 systems', 'question': 'What will be removed from Tbilisi in order to enlarge her below decks aviation facilities?'}, '5acd884007355d001abf4604': {'truth': '', 'predicted': '55,000 tonne', 'question': 'What type of airplane is Admiral Flota Sovetskovo Soyuza Kuznetsov?'}, '5acd884007355d001abf4608': {'truth': '', 'predicted': 'P-700 systems', 'question': 'What will be added from Tbilisi in order to enlarge her below decks aviation facilities?'}, '5a836aeee60761001a2eb68f': {'truth': '', 'predicted': 'early 20th century', 'question': 'When did commercial production of copper end?'}, '56e163afe3433e1400422e63': {'truth': '1960', 'predicted': '', 'question': 'What year were the new England patriots founded in?'}, '56e163afe3433e1400422e65': {'truth': 'after relocating', 'predicted': \"after relocating. The team won the Super Bowl after the 2001, 2003, 2004, and 2014 seasons. They share Gillette Stadium with the New England Revolution of Major League Soccer. The Boston Breakers of Women's Professional Soccer, which formed in 2009\", 'question': 'When dod the Boston patriots change their name?'}, '56e163afe3433e1400422e66': {'truth': 'Gillette Stadium', 'predicted': '', 'question': 'What stadium do the patriots play in?'}, '570d410afed7b91900d45db4': {'truth': 'for anti-aircraft fire', 'predicted': 'anti-aircraft fire', 'question': 'Why was Britain mainly interested in solid fuel rockets?'}, '56f8d3179b226e1400dd1095': {'truth': 'post-World War I', 'predicted': 'post-World War I period', 'question': 'When were ski-lifts built in Swiss and Austrian towns?'}, '56f8d3179b226e1400dd1096': {'truth': 'the 1970s', 'predicted': '1970s', 'question': 'When were several new villages built in France almost exclusively for skiing?'}, '57303e5ea23a5019007fcffe': {'truth': 'through a Control Panel applet called \"Windows 7 File Recovery\"', 'predicted': '', 'question': 'How is Backup and Restore opened?'}, '57303e5ea23a5019007fcfff': {'truth': 'access previous versions of shared files stored on a Windows Server computer', 'predicted': '', 'question': 'What does :76 Shadow Copy do?'}, '57303e5ea23a5019007fd000': {'truth': 'other software', 'predicted': '', 'question': 'What is :74 used for? '}, '5726adf0dd62a815002e8cc6': {'truth': 'the ogival', 'predicted': 'ogival', 'question': 'What is another name for the pointed arch?'}, '5726adf0dd62a815002e8cc7': {'truth': 'the ribbed vault', 'predicted': 'the ogival or pointed arch, the ribbed vault, and the buttress', 'question': 'What is an example of architectural technology that is seen in Gothic construction?'}, '5726adf0dd62a815002e8cc8': {'truth': 'the buttress', 'predicted': 'the ogival or pointed arch, the ribbed vault, and the buttress', 'question': 'What is another example of architectural technology that is seen in Gothic construction?'}, '56df778a5ca0a614008f9adf': {'truth': '1898', 'predicted': '1898 until 1903', 'question': 'What year did Bell become President of the National Geographic magazine?'}, '5731dd950fdd8d15006c65af': {'truth': 'Portuguese', 'predicted': 'Portuguese language', 'question': \"What is Brazil's official language?\"}, '5a2eeb0aa83784001a7d256b': {'truth': '', 'predicted': '2016', 'question': 'In what year will the official curriculum start?'}, '56f8cf869b226e1400dd1053': {'truth': 'Access courses', 'predicted': 'Access', 'question': 'What courses does Southampton City College offer to adult students?'}, '56f8cf869b226e1400dd1055': {'truth': 'Richard Taunton Sixth Form College', 'predicted': 'Itchen College and Richard Taunton Sixth Form College', 'question': \"What's the sixth form college named after a person?\"}, '572846473acd2414000df84d': {'truth': 'felt that, if freedom and civilization were to survive, it would have to be because the US would triumph over totalitarianism', 'predicted': 'because he felt that, if freedom and civilization were to survive, it would have to be because the US would triumph over totalitarianism from Nazism, Fascism and Soviet Communism', 'question': 'Why did von Neumann join government work?'}, '572846473acd2414000df84e': {'truth': 'violently anti-communist, and much more militaristic than the norm', 'predicted': 'violently anti-communist', 'question': 'How did von Neumaan describe his political ideology?'}, '56dfb89e7aa994140058e06f': {'truth': 'accommodation', 'predicted': '', 'question': 'What amenity does an inn offer that pubs, alehouses and taverns usually do not?'}, '56dfb89e7aa994140058e070': {'truth': 'the UK', 'predicted': '', 'question': \"In what nation's pubs is food often served?\"}, '5731eaa7e17f3d140042254a': {'truth': 'shipbuilding', 'predicted': '', 'question': 'What type of industrial activity was evident in Greece in the period researched?'}, '5731eaa7e17f3d140042254b': {'truth': 'defaulted on its external loans', 'predicted': '', 'question': 'What did Greece do in 1826, 1843, 1860 and 1894?'}, '5a7b2ba921c2de001afe9d82': {'truth': '', 'predicted': 'slightly lower', 'question': \"What was Greece's GDP decline between 1833 and 1911 compared with other Western European nations?\"}, '572786b5dd62a815002e9f88': {'truth': \"Darwin's contemporaries thought the time he took was reasonable\", 'predicted': 'reasonable', 'question': \"What did Darwin's contemporaries think of the long delays on his publishing?\"}, '5ad27318d7d075001a4294b1': {'truth': '', 'predicted': 'North Germanic', 'question': 'According to this few, what language is Old Norse a dialect of?'}, '56e09c507aa994140058e64d': {'truth': 'electrons', 'predicted': 'electron', 'question': 'When hydrogen oxidates, what is it removing?'}, '56e09c507aa994140058e651': {'truth': 'Bronsted-Lowry', 'predicted': 'Bronsted-Lowry theory', 'question': 'What theory suggests that acids are proton donors?'}, '56e790e237bdd419002c414d': {'truth': 'U.S. News & World Report', 'predicted': '', 'question': \"Who published America's Best Colleges in 2016?\"}, '56e790e237bdd419002c414f': {'truth': 'national universities', 'predicted': '', 'question': \"Against what other kinds of institutions was KU's engineering school compared?\"}, '5acf5c9477cf76001a684c39': {'truth': '', 'predicted': '90th', 'question': 'In what place did the engineering school at KU appear in 2015?'}, '57284dd0ff5b5019007da13f': {'truth': 'First Amendment rights, Fourth Amendment rights, and right to due process', 'predicted': 'First Amendment', 'question': 'What rights did the ACLU say the Patriot Act violated?'}, '5a86027fb4e223001a8e7383': {'truth': '', 'predicted': 'American Civil Liberties Union', 'question': 'Who sued the ACLU?'}, '5726e302f1498d1400e8eea8': {'truth': 'Philosophy', 'predicted': '', 'question': 'What classic area of study is now mostly reserved for academic consideration?'}, '5726e302f1498d1400e8eea9': {'truth': 'academic journals', 'predicted': '', 'question': 'Most serious studies in philosophy are segregated to what publications?'}, '5726e302f1498d1400e8eeaa': {'truth': 'Plato, Aristotle, Socrates, Augustine, Descartes, Kierkegaard, Nietzsche', 'predicted': 'Plato, Aristotle', 'question': 'Who are some of the most important philosophers in history?'}, '5726e302f1498d1400e8eeab': {'truth': 'logic', 'predicted': '', 'question': 'What aspect of modern academic philosophy is less literary than technical in nature?'}, '5726e302f1498d1400e8eeac': {'truth': 'mathematics', 'predicted': '', 'question': 'Serious studies in logic tend to resemble what discipline, moreso than literature?'}, '5a7a553c21c2de001afe9b73': {'truth': '', 'predicted': 'academic journals', 'question': 'Most new pychologic work appears where?'}, '5a7a553c21c2de001afe9b74': {'truth': '', 'predicted': 'Aristotle', 'question': 'Besides Plato, Achilles, Socrates, and Augustine, who are other noted major philosophers in history?'}, '5a7e59cc48f7d9001a063515': {'truth': '', 'predicted': 'Plato, Aristotle, Socrates, Augustine, Descartes, Kierkegaard, Nietzsche', 'question': 'Who are the major practitioners through history? '}, '5a7e59cc48f7d9001a063517': {'truth': '', 'predicted': 'mathematics', 'question': 'Logic is technical and similar to what field of study?'}, '5726e48b708984140094d4ff': {'truth': 'glass', 'predicted': '', 'question': 'Besides porcelain, paper and mica, what other non conductive material was used as an insulator? '}, '5726e48b708984140094d501': {'truth': 'strip of impregnated paper', 'predicted': 'impregnated paper', 'question': 'What was layered between strips of metal in order to create paper capacitors?'}, '5726e48b708984140094d502': {'truth': 'in 1876', 'predicted': '1876', 'question': 'When were paper capacitors first manufactured?'}, '5acf588577cf76001a684bd6': {'truth': '', 'predicted': 'telephony', 'question': ' What other use did metal capacitors serve in the telecommunications industry?'}, '57304c5e8ab72b1400f9c3fe': {'truth': 'seven', 'predicted': 'seven of its 12,000', 'question': 'How many houses were spared damage in Glasgow?'}, '57304c5e8ab72b1400f9c3ff': {'truth': '40,000 people', 'predicted': '40,000', 'question': 'How many people per week were losing housing?'}, '57304c5e8ab72b1400f9c400': {'truth': 'because of its vulnerable position on the south coast', 'predicted': 'vulnerable position on the south coast', 'question': 'Why was Plymouth targeted the most?'}, '57304c5e8ab72b1400f9c402': {'truth': 'Over 2,000', 'predicted': '2,000', 'question': 'How many AAA shells were fired?'}, '5727b735ff5b5019007d9340': {'truth': 'a full Latin alphabet that is separate from the main Latin alphabet', 'predicted': 'a full Latin alphabet that is separate from the main Latin alphabet section', 'question': 'What does the \"fullwidth forms\" section of code points encompass?'}, '5727b735ff5b5019007d9341': {'truth': 'Chinese, Japanese, and Korean', 'predicted': 'Chinese, Japanese, and Korean (CJK) fonts', 'question': 'What are the CJK languages referenced?'}, '5acd10bc07355d001abf32f6': {'truth': '', 'predicted': 'CJK ideographs', 'question': 'What are Latin characters called when they are half width?'}, '5acd10bc07355d001abf32f8': {'truth': '', 'predicted': 'western', 'question': 'The 256 initial points make it difficult to translate what kind of text?'}, '57322fcce17f3d14004226ef': {'truth': 'Democratic Congressional Campaign Committee', 'predicted': '', 'question': 'According to democratic rules of the 106th congress what campaign membership do they have?'}, '57322fcce17f3d14004226f0': {'truth': 'Democratic Leadership Council', 'predicted': '', 'question': 'According to democratic rules of the 106th congress what leadership members do they appoint?'}, '57317c50a5e9cc1400cdbfc2': {'truth': 'Transfiguration', 'predicted': 'the Transfiguration', 'question': \"What was the name of the painting used to represent raphael in St. Peter's?\"}, '5726e7d3f1498d1400e8ef84': {'truth': 'Britain', 'predicted': 'Britain was concerned about Russian activity and Sir John Burgoyne', 'question': 'Who was concerned with Russia  capturing Constantinople?'}, '5726e7d3f1498d1400e8ef87': {'truth': 'British Ambassador and the French Emperor', 'predicted': 'the British Ambassador and the French Emperor', 'question': 'Who was Burgoyne visiting in Paris?'}, '5726e7d3f1498d1400e8ef88': {'truth': 'Lord Cowley', 'predicted': 'The Lord Cowley', 'question': 'Who wrote to Burgoyne on February 8th?'}, '56fad599f34c681400b0c147': {'truth': '2006', 'predicted': '', 'question': 'When did Richards publish his mtDNA research?'}, '56fad599f34c681400b0c148': {'truth': 'Ethiopians', 'predicted': 'Ethiopians and North Africans', 'question': 'Along with Egyptians, Algerians and Somalis, what people commonly possess the M1 haplogroup?'}, '5733e8ccd058e614000b6572': {'truth': 'communist sympathies.', 'predicted': 'communist sympathies', 'question': 'Why are several anthropologists dismissed from their jobs, according to David H. Price?'}, '572a3a0b6aef0514001553a4': {'truth': 'the A-series', 'predicted': 'A-series', 'question': \"What was McTaggart's first series called?\"}, '572a3a0b6aef0514001553a5': {'truth': 'each other', 'predicted': '', 'question': 'The A-Series orders events according to their being in the past, present or future and in comparison to what else?'}, '5a42d2aa4a4859001aac734b': {'truth': '', 'predicted': 'J. M. E. McTaggart', 'question': 'Who created a problem with the flow of time?'}, '5a42d2aa4a4859001aac734c': {'truth': '', 'predicted': 'A-series', 'question': 'What orders events according to their existance in the past, present or future?'}, '56e79b2d00c9c71400d77384': {'truth': 'Nanjing–Beijing railway', 'predicted': 'Nanjing–Beijing', 'question': 'What is the railway that runs South to North called?'}, '56e79b2d00c9c71400d77385': {'truth': 'Zhong Mountain', 'predicted': 'Loong-like Zhong Mountain', 'question': 'What mountain is in the East are of Nanjing?'}, '5730ef4105b4da19006bcc60': {'truth': 'St Bernard of Clairvaux', 'predicted': '', 'question': 'Who began to query the position of the conception of Mary following the 11th century ?'}, '5730ef4205b4da19006bcc61': {'truth': 'St Bernard blames the canons of the metropolitan church of Lyon for instituting such a festival without the permission of the Holy See.', 'predicted': 'the canons of the metropolitan church of Lyon', 'question': \"Who did the query starter lay blame upon for the festivals that surrounded Mary's inception ?\"}, '5730ef4205b4da19006bcc63': {'truth': \"conception in the active sense of the mother's cooperation\", 'predicted': '', 'question': \"What did the query starter believe had been done by Mary's direct  maternal line that contradict the conception theory of immaculate for Mary ?\"}, '5730ef4205b4da19006bcc64': {'truth': 'instituting such a festival without the permission of the Holy See', 'predicted': 'without the permission of the Holy See', 'question': \"Did the query starter believe that the festival for Mary's conception had authorization to be held ?\"}, '5a273563c93d92001a400426': {'truth': '', 'predicted': 'A feast of the Conception of the Blessed Virgin', 'question': 'What was already being celebrated in some churches of the East?'}, '5a273563c93d92001a400427': {'truth': '', 'predicted': 'St Bernard blames the canons of the metropolitan church of Lyon', 'question': 'Who instituted a feast of the conception of the Blessed Virgin with the permission of the Holy See?'}, '5a273563c93d92001a400428': {'truth': '', 'predicted': 'Mary', 'question': 'Whose conception does St. Bernard say was sinless?'}, '5a273563c93d92001a400429': {'truth': '', 'predicted': 'Pope Pius IX', 'question': \"What Pope's definition of conception agreed with St. Bernard's?\"}, '5a6bb40d4eec6b001a80a4fc': {'truth': '', 'predicted': '300', 'question': \"How many mg of fat is suggested for a healthy person's diet?\"}, '56ce81bdaab44d1400b88817': {'truth': 'bass', 'predicted': 'low-frequency bass', 'question': 'What part of audio output was substandard on 3rd generation iPods?'}, '56ce81bdaab44d1400b88818': {'truth': 'undersized DC-blocking capacitors', 'predicted': 'DC-blocking capacitors', 'question': 'What component was to blame for the weak bass of the 3rd generation iPod?'}, '5726638e708984140094c48f': {'truth': '1990', 'predicted': 'after 1990', 'question': 'Since when has environmental damage in Thuringia been reduced?'}, '5726638e708984140094c490': {'truth': 'modernizing factories', 'predicted': 'modernizing factories, houses (decline of coal heating) and cars', 'question': 'What is one thing that helped to improve condition of forests, rivers and air?'}, '5726638e708984140094c491': {'truth': 'Uranium surface mines around Ronneburg have been remediated', 'predicted': 'remediated', 'question': 'What has been done to former Uranium surface mines around Ronneburg?'}, '5726638e708984140094c493': {'truth': 'discharges of K+S salt mines around Unterbreizbach', 'predicted': 'discharges of K+S salt mines', 'question': 'What is causing the salination of the Werra river?'}, '5a7cc42be8bc7e001a9e1fd9': {'truth': '', 'predicted': 'after 1990', 'question': 'When has environmental damage in Thuringia been increased since?'}, '56e19df2e3433e1400423035': {'truth': 'Standard Catalan', 'predicted': '', 'question': 'What form is excepted by most speakers?'}, '56e19df2e3433e1400423037': {'truth': 'traditional ones', 'predicted': 'traditional', 'question': 'What language forms are not now used in eastern Catalonia?'}, '570c6506fed7b91900d45981': {'truth': 'dictatorships', 'predicted': 'the dictatorships of Miguel Primo de Rivera', 'question': 'What caused the suppression of regional cultures?'}, '570c6506fed7b91900d45982': {'truth': 'Spanish (Castilian)', 'predicted': 'Spanish (Castilian', 'question': 'Of the languages of the are, what was the only approved language?'}, '570c6506fed7b91900d45983': {'truth': 'joining Barça', 'predicted': 'by joining Barça', 'question': 'How did the Catalans show their identity during the dictatorships?'}, '570c6506fed7b91900d45984': {'truth': 'blaugrana', 'predicted': 'blaugrana team', 'question': 'What team was awarded by Franco for having a good relationship?'}, '5726086889a1e219009ac16f': {'truth': 'By careful selection of which electron energy level transitions are used, and fluorescent coatings which modify the spectral distribution', 'predicted': '', 'question': 'How can luminescent light sources be modified to resemble the appearance of incandescents?'}, '5726086889a1e219009ac170': {'truth': 'Due to the discrete spectral lines rather than a continuous spectrum', 'predicted': 'discrete spectral lines rather than a continuous spectrum', 'question': 'Why are luminescent light sources not ideal for photography?'}, '572844c5ff5b5019007da066': {'truth': 'unitary state', 'predicted': 'a unitary state', 'question': 'How has United Kingdom been governed?'}, '572844c5ff5b5019007da067': {'truth': 'UK has relied on gradual devolution to decentralise political power', 'predicted': 'gradual devolution to decentralise political power', 'question': 'Instead of the UK adopting the federalist model, what did they do?'}, '572844c5ff5b5019007da068': {'truth': '1914', 'predicted': 'Government of Ireland Act 1914', 'question': 'When did devolution in the UK begin?'}, '572844c5ff5b5019007da06a': {'truth': 'eventually evolved into the modern day Republic of Ireland', 'predicted': 'Republic of Ireland', 'question': 'What is Irish Free State?'}, '5acfbe5777cf76001a685c00': {'truth': '', 'predicted': 'Republic of Ireland', 'question': 'What is Irish Cost State?'}, '572ea043c246551400ce4425': {'truth': 'English', 'predicted': '', 'question': 'Are there any Western languages spoken in Cyprus?'}, '572ea043c246551400ce4426': {'truth': 'Russian', 'predicted': '', 'question': 'Are there any Eastern languages spoken in Cyprus?'}, '56fdc60e19033b140034cd67': {'truth': 'a form of tally stick', 'predicted': 'tally stick', 'question': 'The earliest device to help count was what?'}, '56cfef3c234ae51400d9c10d': {'truth': 'the Revolutionary Étude', 'predicted': 'Revolutionary Étude', 'question': 'What is another title Op. 10, No. 12 has garnered? '}, '56cfef3c234ae51400d9c10f': {'truth': 'Sonata No. 2', 'predicted': 'Sonata No. 2 (Op. 35), the one case where he did give a title, was written before the rest of the sonata', 'question': 'The Funeral March was written as part of what piece?'}, '570d6de5fed7b91900d460b7': {'truth': 'Charles I', 'predicted': 'Habsburg king Charles I', 'question': 'Whose government did the guilds rebel against?'}, '570d6de5fed7b91900d460b9': {'truth': 'plague', 'predicted': 'epidemic of plague', 'question': 'What caused the nobility to leave Valencia?'}, '570d6de5fed7b91900d460bb': {'truth': 'the Germanies', 'predicted': 'Germanies', 'question': 'What were the artisan guilds called?'}, '5acec3f332bba1001ae4b339': {'truth': '', 'predicted': 'Domestic Organization', 'question': 'What organization did Brigham Young create?'}, '5a4745ff5fd40d001a27dd9d': {'truth': '', 'predicted': 'nine', 'question': 'When South Baden was formed in 1952, how many states were there?'}, '5a4745ff5fd40d001a27dda1': {'truth': '', 'predicted': 'special status', 'question': 'What kind of status did Saarland have when integrated with West Germany?'}, '5a514b63ce860b001aa3fcbd': {'truth': '', 'predicted': 'nine', 'question': \"How many states did West Germany add after it's founding in 1952?\"}, '5a514b63ce860b001aa3fcbf': {'truth': '', 'predicted': '1957', 'question': 'When was the Saar Protectorate given to the French?'}, '5a514b63ce860b001aa3fcc1': {'truth': '', 'predicted': 'Western Allies', 'question': 'What German state did West Berlin fall under?'}, '5728f761af94a219006a9e85': {'truth': 'akin to a marriage', 'predicted': 'marriage', 'question': 'How did the samurai treat concubines?'}, '5728f761af94a219006a9e86': {'truth': 'shameful, if not criminal', 'predicted': 'shameful', 'question': 'How did the samurai view kidnapping concubines?'}, '5728f761af94a219006a9e88': {'truth': \"her family's money erased the samurai's debts\", 'predicted': '', 'question': 'Why did merchants prefer that their daughters not marry samurai?'}, '5728f761af94a219006a9e89': {'truth': \"the son could inherit his father's social status\", 'predicted': \"inherit his father's social status\", 'question': 'What happened if a commoner concubine had a son?'}, '5730ed3ea5e9cc1400cdbaf5': {'truth': '1:18', 'predicted': '1:18 for all schools with the exception of Nauti School, which has a teacher-student ratio of 1:27', 'question': 'What si the teacher-student ratio for Tuvalu schools?'}, '572a518ab8ce0319002e2a94': {'truth': 'interests of the state', 'predicted': 'the state', 'question': 'What dominated all economic and political interests?'}, '572a518ab8ce0319002e2a95': {'truth': 'capitalist and mercantile economies', 'predicted': 'capitalist and mercantile', 'question': 'What were the types of economies that were being developed in western Europe?'}, '572a518ab8ce0319002e2a96': {'truth': 'developing commercial centres and routes', 'predicted': 'By developing commercial centres and routes, encouraging people to extend the area of cultivated land in the country and international trade through its dominions', 'question': 'The expansion of international trade through the Empire was the result of what?'}, '56f8c9d29e9bad19000a04f1': {'truth': 'identical daughter cells', 'predicted': 'daughter cells', 'question': 'In cell division, what two cells are created? '}, '5726e07a5951b619008f8105': {'truth': 'destroy the guerrillas and their sympathizer citizens in Southern Korea', 'predicted': 'to destroy the guerrillas and their sympathizer citizens in Southern Korea', 'question': 'What to be achieved by the Sancheong-Hamyang and Geochang massacres in South Korea?'}, '57266c865951b619008f7251': {'truth': \"Hispanic or Latino ancestry ancestry accounted for 22.5% (4,223,806) of Florida's population\", 'predicted': '22.5%', 'question': 'What percentage of the Florida population in 2010 was Hispanic '}, '57266c865951b619008f7253': {'truth': 'Nearly 80% of Cuban Americans live in Florida, especially South Florida', 'predicted': '80%', 'question': 'What percentage of Cuban Americans live in Florida '}, '5acd969f07355d001abf47b2': {'truth': '', 'predicted': '1.6%', 'question': 'What percentage of the Florida population in 2001 was Colombian?'}, '56e0cdc37aa994140058e721': {'truth': 'Internet Explorer', 'predicted': '', 'question': 'What was bundled for free with the Windows OS?'}, '56e0cdc37aa994140058e722': {'truth': 'sales of Windows to computer manufacturers and direct to users', 'predicted': 'the sales of Windows to computer manufacturers and direct to users', 'question': 'Internet Explorer was partially funded in what two ways?'}, '57302bbda23a5019007fcee4': {'truth': 'were believed to be funding rebels in Sierra Leone.', 'predicted': 'funding rebels in Sierra Leone', 'question': 'Why were sanctions place on Liberian timber exports?'}, '57302bbda23a5019007fcee5': {'truth': 'in 2006', 'predicted': '2006', 'question': 'When were the timber export sanctions lifted for Liberia?'}, '5a62be8af8d794001af1c1fc': {'truth': '', 'predicted': '2006', 'question': 'In what year did UN sanctions prohibit membership in the World Trade Organization?'}, '5a62be8af8d794001af1c1fe': {'truth': '', 'predicted': 'Due in large part to foreign aid and investment inflow following the end of the war, Liberia maintains a large account deficit, which peaked at nearly 60%', 'question': \"What happened to Liberia's large export deficit?\"}, '5a62be8af8d794001af1c1ff': {'truth': '', 'predicted': '2006', 'question': 'In what year were World Trade Organization sanctions lifted?'}, '5733b195d058e614000b6083': {'truth': 'that Islamic militarism in the east of the country was on the rise following the escape of 25 militants from a Tajik prison in August', 'predicted': 'Islamic militarism in the east of the country', 'question': 'Why was there concerns in 2010?'}, '5733b195d058e614000b6084': {'truth': '28 Tajik soldiers', 'predicted': '', 'question': 'How many solider were killed in September when Islamic militants escaped?'}, '5733b195d058e614000b6085': {'truth': 'November 2010', 'predicted': '', 'question': 'When did the military operation end in Rasht Valley?'}, '57278cb9f1498d1400e8fbb2': {'truth': 'Supreme Federal Tribunal (Supremo Tribunal Federal)', 'predicted': 'Supreme Federal Tribunal', 'question': 'What is the highest court in Brazil?'}, '57278cb9f1498d1400e8fbb3': {'truth': 'cases that may be unconstitutional or final habeas corpus pleads for criminal cases', 'predicted': '', 'question': 'What are the two areas this court has supremacy over?'}, '57278cb9f1498d1400e8fbb5': {'truth': 'ministers of state, members of the high courts and the President and Vice-President of the Republic', 'predicted': 'President and Vice-President of the Republic', 'question': \"What other government officials are subject to judgments of Brazil's highest court?\"}, '57278cb9f1498d1400e8fbb6': {'truth': 'The Superior Labour Tribunal (Tribunal Superior do Trabalho)', 'predicted': 'Superior Labour Tribunal (Tribunal Superior do Trabalho', 'question': \"What is Brazil's high court for labor law?\"}, '5a7fb0f18f0597001ac0007b': {'truth': '', 'predicted': 'general', 'question': 'The Superior Labour Tribunal oversees what type of elections?'}, '5a7fb0f18f0597001ac0007c': {'truth': '', 'predicted': 'criminal cases', 'question': 'What two types of cases does the Superior Military Tribunal only review?'}, '5a7fb0f18f0597001ac0007e': {'truth': '', 'predicted': 'Supremo Tribunal Federal', 'question': 'What is the Brazilian name for the Supreme Court?'}, '56d9ddf4dc89441400fdb86f': {'truth': 'anxiety', 'predicted': '', 'question': 'What is lessened when people are with their pet dogs?'}, '5731450de6313a140071cda0': {'truth': 'deny the opponent an advantage in the EMS', 'predicted': 'to deny the opponent an advantage in the EMS', 'question': 'What is the purpose of electronic warfare?'}, '5731450de6313a140071cda2': {'truth': 'to keep airspaces friendly, and send critical information to anyone who needs it', 'predicted': 'keep airspaces friendly', 'question': 'What does the USAF use Electronic warfare aircraft for?'}, '5731450de6313a140071cda4': {'truth': 'Airborne Command Post', 'predicted': '', 'question': 'What does the USAF use the E-4B aircraft for?'}, '5726f50add62a815002e9631': {'truth': 'in Western calendars', 'predicted': 'Western calendars', 'question': 'Where are Mesopotamian astronomical periods still used?'}, '5726f50add62a815002e9632': {'truth': 'a Chaldean astronomer and mathematician', 'predicted': 'Chaldean astronomer and mathematician', 'question': 'Who was Kidinnu?'}, '571021d7a58dae1900cd68cf': {'truth': 'firing squad', 'predicted': '', 'question': 'What method of execution was used on James W. Rodgers?'}, '5ad3f93e604f3c001a3ffaa7': {'truth': '', 'predicted': 'Utah', 'question': 'In what state was Rodgers born?'}, '5ad3f93e604f3c001a3ffaa9': {'truth': '', 'predicted': 'Oklahoma', 'question': ' Which state freed James French?'}, '5733e5704776f41900661453': {'truth': 'number of other disciplines', 'predicted': '', 'question': 'What does the field of anthrozoology overlap with?'}, '5ad2f672604f3c001a3fda6b': {'truth': '', 'predicted': 'Anthrozoology', 'question': 'What is the study of animals?'}, '5ad2f672604f3c001a3fda6d': {'truth': '', 'predicted': 'anthropology, ethology, medicine, psychology, veterinary medicine and zoology', 'question': 'What fields developed from Anthrozoology?'}, '57302c9aa23a5019007fcefd': {'truth': 'imaginary plane', 'predicted': '', 'question': 'What are electrical fields projected on to?'}, '57302c9aa23a5019007fcefe': {'truth': 'radio wave', 'predicted': '', 'question': 'What is the imagenary plane perpindicular to?'}, '572845f7ff5b5019007da0a0': {'truth': 'French', 'predicted': 'French-flagged warships', 'question': \"What country's ships did some witnesses say were involved in the Baarawe attack?\"}, '5a839cf3e60761001a2eb82a': {'truth': '', 'predicted': 'sheik commander', 'question': 'What was the title for Saleh Nabhan Saleh Ali?'}, '5a839cf3e60761001a2eb82b': {'truth': '', 'predicted': 'Nabhan', 'question': 'Who was wanted in connection with the 2002 Kenyan attacks?'}, '5a839cf3e60761001a2eb82c': {'truth': '', 'predicted': 'Baarawe', 'question': 'Witnesses from what village claim they saw French-flagged warships?'}, '5a85b7e3b4e223001a8e71b9': {'truth': '', 'predicted': 'two', 'question': 'How many US Special Forces were killed on 14 September 2009?'}, '5a85b7e3b4e223001a8e71bd': {'truth': '', 'predicted': 'Al-Shabaab', 'question': 'What was the name of the Kenyan al-Qaeda group?'}, '57266a2c5951b619008f7206': {'truth': 'salt', 'predicted': 'the tax on salt', 'question': 'One of the governor of the company said that he would rather have saltpetre then____ in its raw form?'}, '5a8464dd7cf838001a46a7da': {'truth': '', 'predicted': '1673', 'question': 'What year did Banks negotiate between the princess and the North India company for 600 tons of salt?'}, '570b00026b8089140040f69e': {'truth': 'telemedicine', 'predicted': 'telemedicine, distance education, and business meetings', 'question': 'What is one area where teleconferencing could not be used?'}, '570b00026b8089140040f6a0': {'truth': 'the 1950s', 'predicted': '1950s', 'question': 'When was the first slow-scan video systems researched?'}, '570b00026b8089140040f6a1': {'truth': '6 Mbit/s', 'predicted': '1 MHz bandwidth and 6 Mbit/s', 'question': \"What was the bit rate of AT&T's Picturephone?\"}, '5a1f22043de3f40018b2650e': {'truth': '', 'predicted': 'AT&T Corporation', 'question': 'Who developed the first system to transmit distance education?'}, '5a1f22043de3f40018b2650f': {'truth': '', 'predicted': 'telemedicine, distance education', 'question': 'In what area could business meetings not be used?'}, '5a1f22043de3f40018b26510': {'truth': '', 'predicted': 'poor picture quality and the lack of efficient video compression techniques', 'question': 'Why did using business meetings to transmit slow-scan video fail?'}, '572a8012be1ee31400cb8045': {'truth': 'Baghdad', 'predicted': '', 'question': 'Where did Shuhda attend school?'}, '5ace804f32bba1001ae4a87b': {'truth': '', 'predicted': 'Baghdad', 'question': ' Where did Shuhda work?'}, '5ace804f32bba1001ae4a87c': {'truth': '', 'predicted': 'around 750, during the Abbasid Caliphate, women “became renowned for their brains as well as their beauty”. In particular, many well known women of the time were trained from childhood', 'question': ' When was formal education for Islamic women encouraged?'}, '572b883cf75d5e190021fe32': {'truth': 'teaching, research, and social services activities,', 'predicted': 'teaching, research, and social services activities', 'question': 'What does University education include?'}, '572b883cf75d5e190021fe34': {'truth': 'independent', 'predicted': 'private and independent', 'question': 'What type of University would Yale fall under?'}, '5731a21de17f3d1400422297': {'truth': 'an alternative to traditional universities', 'predicted': '', 'question': ' Governments created universities to serve as what?'}, '5a77ae98b73996001af5a4fa': {'truth': '', 'predicted': 'free education', 'question': 'What type of education did scientists benefactors hope to provide to the public?'}, '5a77ae98b73996001af5a4fc': {'truth': '', 'predicted': 'to provide a knowledge hungry populace with an alternative to traditional universities', 'question': 'Why did local governments establish research?'}, '5a77ae98b73996001af5a4fd': {'truth': '', 'predicted': 'the resources available through private benefactors', 'question': 'What could scientists not compete with?'}, '5ad37275604f3c001a3fe2b5': {'truth': '', 'predicted': 'Primary Reserve, Supplementary Reserve, Cadet Organizations Administration and Training Service, and the Canadian Rangers', 'question': 'What four components make up the Force Reserve?'}, '5ad37275604f3c001a3fe2b6': {'truth': '', 'predicted': 'the Department of National Defence', 'question': 'What is the Canadian Armed Forces associated with under the National Defence Act?'}, '5709b165ed30961900e84426': {'truth': 'deceased', 'predicted': '', 'question': 'What condition does a person have to meet to  be allowed by law on a coin?'}, '5709b165ed30961900e84428': {'truth': '20th century', 'predicted': '', 'question': 'When did modern day currency start getting the faces that they have?'}, '5709b165ed30961900e84429': {'truth': 'composite Native Americans', 'predicted': 'Native Americans', 'question': 'Other than Greek and Roman mythology, who else was featured on the \"heads\" side of past coins?'}, '5a8cd753fd22b3001a8d8f31': {'truth': '', 'predicted': 'Native Americans', 'question': 'Other than Greek and Roman mythology, who else was featured on the \"heads\" of past monarchs?'}, '56e14538e3433e1400422d20': {'truth': 'in public life and education', 'predicted': 'public life and education', 'question': 'Where does the General Council want to promote Catalan?'}, '56e14538e3433e1400422d21': {'truth': 'General Council of the Pyrénées-Orientales', 'predicted': 'the General Council of the Pyrénées-Orientales', 'question': 'Who recognized Catalan as a departmental language?'}, '56d296f259d6e414001460fb': {'truth': 'core', 'predicted': '', 'question': 'Some scholars say there is no universally accepted common what?'}, '56dfdbee7aa994140058e1c7': {'truth': 'March 2006', 'predicted': '2006', 'question': 'In what month and year was smoking banned in public places in Scotland?'}, '56dfdbee7aa994140058e1cb': {'truth': 'Wetherspoon', 'predicted': 'The Wetherspoon', 'question': 'What chain of pubs reported favorable profits in June 2009?'}, '572827843acd2414000df5ae': {'truth': 'hair-forming', 'predicted': '', 'question': 'What do chetoblast cells do?'}, '5728b201ff5b5019007da4a3': {'truth': '40 or 50 members', 'predicted': '40 or 50', 'question': 'How many people compose the Japanese mandolin orchestras?'}, '5728b201ff5b5019007da4a4': {'truth': 'include woodwind, percussion, and brass sections.', 'predicted': 'woodwind, percussion, and brass sections', 'question': 'What other instruments do the Japanese madnolin orchestras play?'}, '5728b201ff5b5019007da4a5': {'truth': \"20th Century mandolin music from Europe and one of the most complete collections of mandolin magazines from mandolin's golden age\", 'predicted': '20th Century mandolin music', 'question': 'Japan hold and extensive collection of what? '}, '5728b201ff5b5019007da4a6': {'truth': 'Morishige Takei.', 'predicted': 'Morishige Takei', 'question': 'Who purhcased one of the collections of mandolin magazines? '}, '5ad22e93d7d075001a42866e': {'truth': '', 'predicted': 'Morishige Takei', 'question': ' Who sold one of the collections of mandolin magazines?'}, '5ace3ada32bba1001ae49f8d': {'truth': '', 'predicted': '19', 'question': \"How many men's cross country NCAA Division III championships have they won?\"}, '56e7ac6c37bdd419002c430f': {'truth': '85 routes', 'predicted': '85', 'question': \"How many routes does Nanjing's airport run?\"}, '56e7ac6c37bdd419002c4310': {'truth': '15,011,792 passengers', 'predicted': '15,011,792', 'question': 'How many passengers did the airport service in 2013?'}, '56e7ac6c37bdd419002c4312': {'truth': 'Nanjing Dajiaochang Airport', 'predicted': '', 'question': 'What airport was the primary airport before Lukou?'}, '5727a1eeff5b5019007d9160': {'truth': 'Guaranda (Bolivar province) and Ambato (Tungurahua province', 'predicted': 'Guaranda', 'question': 'Where are the most famed Carnival festivities?'}, '5727a1eeff5b5019007d9162': {'truth': 'Imbabura', 'predicted': 'northern part of the Sierra in the Chota Valley in Imbabura', 'question': 'Where has a celebration recently gained acclaim?'}, '5727a1eeff5b5019007d9163': {'truth': 'afro-Ecuadorian', 'predicted': '', 'question': 'What is there a large population of in the Chota Valley?'}, '57276683dd62a815002e9c36': {'truth': 'paid by the piece', 'predicted': 'workers were paid by the piece', 'question': 'Were the boys in glass making industry paid by the hour?'}, '56e032247aa994140058e34b': {'truth': 'Ninth Art', 'predicted': '', 'question': 'Comics for adults began to be called what?'}, '56e032247aa994140058e34d': {'truth': 'Adventures of Asterix', 'predicted': 'The Adventures of Asterix', 'question': 'What became a best-seller comic in the French language?'}, '5acf864b77cf76001a6850d3': {'truth': '', 'predicted': 'The Adventures of Asterix', 'question': 'What became a best-seller comic in the English language?'}, '57321963e99e3014001e650d': {'truth': 'auspex', 'predicted': '\"auspex', 'question': 'The word auspicious is derived from which word?'}, '57321963e99e3014001e650e': {'truth': 'mythical chullumpi bird', 'predicted': 'chullumpi bird', 'question': 'What is said to mark the existence of a portal between such worlds, and to transform itself into a llama?'}, '572e9c96cb0c0d14000f135c': {'truth': 'After the Mexican War of Independence from Spain', 'predicted': 'After the Mexican War of Independence', 'question': 'When did other states become part of Mexico?'}, '572e9c96cb0c0d14000f135d': {'truth': 'California, Nevada, Arizona, Utah, western Colorado and southwestern Wyoming became part of the Mexican territory of Alta California', 'predicted': '', 'question': 'What states made up of Alta California'}, '572e9c96cb0c0d14000f135e': {'truth': 'most of New Mexico, western Texas, southern Colorado, southwestern Kansas, and Oklahoma panhandle were part of the territory of Santa Fe de Nuevo México', 'predicted': 'New Mexico, western Texas, southern Colorado, southwestern Kansas, and Oklahoma panhandle', 'question': 'What states were part of Santa Fe de Nuevo'}, '572e9c96cb0c0d14000f135f': {'truth': 'The geographical isolation and unique political history of this territory', 'predicted': '', 'question': 'Why is  there still Bilingual  spoken in these states?'}, '5730a9732461fd1900a9cf63': {'truth': 'Ubaid', 'predicted': 'The Ubaid period', 'question': 'Fine quality painted pottery is a distinctive style of what period in Sumerian history?'}, '5a650f65c2b11c001a425bc7': {'truth': '', 'predicted': 'Hadji Muhammed culture', 'question': 'What culture adopted and improved on irrigation agriculture?'}, '5a650f65c2b11c001a425bc8': {'truth': '', 'predicted': 'Uruk', 'question': 'What city became the new religious center after Eridu?'}, '57310f4ae6313a140071cbca': {'truth': 'United Kingdom, France, Italy, and Japan', 'predicted': 'the United Kingdom, France, Italy, and Japan', 'question': 'Who were the four permanent members of the League of Nations Council?'}, '57310f4ae6313a140071cbcb': {'truth': 'against the ratification of the Treaty of Versailles, thus preventing American participation in the League', 'predicted': '', 'question': 'Why was United Stated excluded from League of Nations Council?'}, '5a14a7c7a54d4200185292fd': {'truth': '', 'predicted': '19 March 1920', 'question': 'When did the League of Nations council vote against the Treaty of Versailles ratification?'}, '5a14a7c7a54d420018529300': {'truth': '', 'predicted': 'US Senate', 'question': 'What council voted against ratification of the Treaty of Versailles?'}, '5732ae23cc179a14009dabfa': {'truth': 'the Isthmus of Panama', 'predicted': 'Isthmus of Panama', 'question': 'What is the link between North and South America called?'}, '5732ae23cc179a14009dabfc': {'truth': 'marsupial faunas', 'predicted': 'marsupial', 'question': 'The Pliocene saw the end of what fauna in South America?'}, '5732ae23cc179a14009dabfd': {'truth': 'Africa', 'predicted': '', 'question': 'The Mediterranean was created by the collision of Europe and what?'}, '5732ae23cc179a14009dabfe': {'truth': 'the Quaternary Period', 'predicted': '', 'question': 'What period came after the Pliocene?'}, '5a4ebec8af0d07001ae8cc27': {'truth': '', 'predicted': 'Isthmus of Panama', 'question': 'What formed to linked North and South America in the Quaternary Period?'}, '572bc28a111d821400f38f76': {'truth': 'poor countries should grow faster than rich countries', 'predicted': 'theoretical prediction that poor countries should grow faster than rich countries', 'question': 'What is the theory behind Empirical analyses?'}, '57337cc94776f41900660ba7': {'truth': 'Claremont and a select number of liberal graduate-level theology and philosophy programs', 'predicted': 'Claremont', 'question': \"Where are Whitehead's works primarily studied in English-speaking countries?\"}, '57337cc94776f41900660ba8': {'truth': 'through the work of his students and admirers rather', 'predicted': 'the work of his students and admirers', 'question': 'Where has interest outside of those areas mainly come from?'}, '57337cc94776f41900660ba9': {'truth': 'Bertrand Russell, and he also taught and supervised the dissertation of Willard Van Orman Quine', 'predicted': 'Willard Van Orman Quine', 'question': \"Who are two of Whitehead's students that have gone on to become renowned in the field of analytic philosophy?\"}, '57337cc94776f41900660baa': {'truth': '\"he stands provisionally as the last great Anglo-American philosopher before Wittgenstein\\'s disciples spread their misty confusion, sufficiency, and terror.\"', 'predicted': '\"he stands provisionally as the last great Anglo-American philosopher before Wittgenstein\\'s disciples spread their misty confusion, sufficiency, and terror.', 'question': 'What did Gilles Deleuze say about Whitehead?'}, '5ad3d82d604f3c001a3ff36f': {'truth': '', 'predicted': 'Claremont', 'question': \"Where are Whitehead's works was not primarily studied in English-speaking countries?\"}, '5723fc250dadf01500fa1fe5': {'truth': 'Germany', 'predicted': '', 'question': 'Where did princess Victoria move to after she was married?'}, '572501720ba9f01400d97c29': {'truth': 'last German Kaiser', 'predicted': 'the last German Kaiser', 'question': 'What position did Wilhelm later hold in Germany? '}, '57266b70708984140094c567': {'truth': \"Eleven days after Orsini's assassination attempt\", 'predicted': 'Eleven days', 'question': \"When was Victoria's oldest daughter married?\"}, '57266b70708984140094c56b': {'truth': 'the last German Kaiser', 'predicted': 'Wilhelm, who would become the last German Kaiser', 'question': 'What future awaited the first grandson of Queen Victoria?'}, '5ad17607645df0001a2d1cee': {'truth': '', 'predicted': 'Prince Frederick William of Prussia', 'question': 'Who did Victorias youngest daughter marry?'}, '5ad17607645df0001a2d1cef': {'truth': '', 'predicted': 'London', 'question': \"Where did Victoria's youngest daughter get married?\"}, '5ad17607645df0001a2d1cf0': {'truth': '', 'predicted': '14', 'question': 'How old was Princess Victoria when she agreed to divorce the Prince?'}, '5ad17607645df0001a2d1cf1': {'truth': '', 'predicted': '14', 'question': \"How old was Princess Victoria when she wasn't married?\"}, '56e7b1d437bdd419002c4378': {'truth': 'May', 'predicted': 'May to February', 'question': 'In what month did the AFL season originally begin?'}, '56e7b1d437bdd419002c4379': {'truth': 'February', 'predicted': 'May to February', 'question': 'After the TV deal, when was the start of the AFL season moved to?'}, '572fd6a6b2c2fd14005684f3': {'truth': 'One Laptopschool Per child', 'predicted': '', 'question': 'What does OLPC stand for?'}, '572fd6a6b2c2fd14005684f4': {'truth': 'American University of Armenia and the QSI International School of Yerevan', 'predicted': 'American University of Armenia', 'question': 'What are the names of some of the other higher education organizations in Armenia?'}, '5706b5fa0eeca41400aa0d6f': {'truth': 'Los Angeles', 'predicted': \"Los Angeles saw a huge explosion of underground raves and DJs, notably DJs Marques Wyatt and Billy Long, who spun at Jewel's Catch One\", 'question': 'where did DJs marques wyatt and billy long become successful?'}, '5ad2932ad7d075001a429adc': {'truth': '', 'predicted': 'Los Angeles', 'question': 'Where did DJs Marques, Wyatt, and Mike Wilson become successful?'}, '5ad2932ad7d075001a429add': {'truth': '', 'predicted': \"Jewel's Catch One\", 'question': 'What is the oldest dance club in San Deigo?'}, '5ad2932ad7d075001a429ade': {'truth': '', 'predicted': 'One Voice Records', 'question': 'What label did Mike Wilson start in 1989?'}, '5ad2932ad7d075001a429ae0': {'truth': '', 'predicted': 'Dada Nada', 'question': \"David Morales was the moniker for what artist's solo act?\"}, '56f8ba089e9bad19000a03cb': {'truth': 'the Dolphin', 'predicted': 'Platform tavern, the Dolphin', 'question': 'What small music venue in Southampton is named after an aquatic mammal?'}, '56f96a3c9e9bad19000a08ed': {'truth': 'US$57.7 million', 'predicted': '$57.7 million', 'question': 'How much money did the Marshall Islands receive yearly from the United States until 2013?'}, '56f96a3c9e9bad19000a08ef': {'truth': 'the Amended Compact of Free Association', 'predicted': 'Amended Compact of Free Association', 'question': 'What document defines how much money is transferred from the United States to the Marshall Islands?'}, '572966e11d046914007793a5': {'truth': 'A unique green', 'predicted': '', 'question': 'What is produced on a computer display when light from the green primary is mixed with some light from the blue primary?'}, '572966e11d046914007793a6': {'truth': '~550 nm', 'predicted': '550 nm', 'question': 'At what wavelength is green on computer displays?'}, '56e0c956231d4119001ac391': {'truth': 'the World Wide Web', 'predicted': 'World Wide Web', 'question': 'The primary function of a browser is to use what?'}, '5a4d2f747a6c4c001a2bbc18': {'truth': '', 'predicted': 'World Wide Web', 'question': 'Where are browsers located?'}, '5a4d2f747a6c4c001a2bbc19': {'truth': '', 'predicted': 'World Wide Web', 'question': 'Where is a browser so you are able to find it later?'}, '57294fa6af94a219006aa286': {'truth': 'refugees, who were captured in 2001 in Pakistan', 'predicted': 'refugees', 'question': 'What were the Uyghurs claiming to be?'}, '57294fa6af94a219006aa287': {'truth': \"training to assist the Taliban's military.\", 'predicted': \"training to assist the Taliban's military\", 'question': 'What were the Ugyhurs accused of?'}, '57294fa6af94a219006aa288': {'truth': 'the US government determined that China was likely to violate their human rights.', 'predicted': '', 'question': \"Why weren't the Ugyhurs deported back to China?\"}, '5ad417d2604f3c001a4003b9': {'truth': '', 'predicted': '2001', 'question': 'What year were the Uyghurs captured fleeing America?'}, '5726b8addd62a815002e8e28': {'truth': '29 September', 'predicted': '29 September 1963', 'question': 'On what date was Vatican II re convened?'}, '5729145a3f37b31900477fff': {'truth': 'delimitation of the federal territory', 'predicted': '', 'question': 'What keeps being debated in Germany?'}, '5729145a3f37b31900478000': {'truth': 'pay for it from own source revenues', 'predicted': 'administrative and fiscal capacity to implement legislation and pay for it from own source revenues', 'question': 'What does Gunlick remark that the German System of  dual federalism requires strong Länder to have other than the capacity to implement legislation?'}, '56dfe86b7aa994140058e251': {'truth': 'John Manners, Marquess of Granby', 'predicted': 'John Manners', 'question': 'After whom was the Marquis of Granby pub named?'}, '56dfe86b7aa994140058e254': {'truth': '18th', 'predicted': '', 'question': 'In what century did the 3rd Duke of Rutland live?'}, '56dfe86b7aa994140058e255': {'truth': 'the Royal George', 'predicted': 'Royal George', 'question': 'What were pubs licensed in 1780 named?'}, '572fa6bca23a5019007fc834': {'truth': 'Russian Army', 'predicted': 'Imperial Russian Army', 'question': 'Which army had a group of Armenian volunteers fighting for them?'}, '57290895af94a219006a9fae': {'truth': 'Agence', 'predicted': 'Agence France-Presse', 'question': \"What is France's oldest operating news agency?\"}, '56e162a3e3433e1400422e44': {'truth': 'the FleetCenter', 'predicted': 'FleetCenter', 'question': 'What is the TD Gardens former name?'}, '5726b8be708984140094cf15': {'truth': 'Anglican', 'predicted': 'Anglican Church', 'question': 'Which church did Burke most defend?'}, '5726b8be708984140094cf19': {'truth': 'political arrangements', 'predicted': '', 'question': 'Burke thought religion was beneficial to what besides souls?'}, '5ad11411645df0001a2d0c94': {'truth': '', 'predicted': 'civil society', 'question': 'What did Burke believe was the foundation of religion?'}, '5731bcdc0fdd8d15006c64c5': {'truth': 'because of the experience under the British crown', 'predicted': 'the experience under the British crown', 'question': 'Why was the word \"national\" a cause for alarm to both Federalists and Anti-Federalists?'}, '5731bcdc0fdd8d15006c64c6': {'truth': 'Elbridge Gerry', 'predicted': 'Rep. Elbridge Gerry', 'question': \"Who took issue with Madison's language during the debate over the establishment clause?\"}, '5ad1423f645df0001a2d1420': {'truth': '', 'predicted': 'legal scholars', 'question': \"What is John Baker of LLU's profession?\"}, '5ad1423f645df0001a2d1422': {'truth': '', 'predicted': 'the experience under the British crown', 'question': 'Why was the word \"national\" a cause for alarm to neither Federalists and Anti-Federalists?'}, '5728116c3acd2414000df3a4': {'truth': 'The European Parliament', 'predicted': '', 'question': 'Who gets to interview and question prospective board members?'}, '5728116c3acd2414000df3a5': {'truth': 'bound by the treaties', 'predicted': 'the treaties', 'question': 'Why do EU institutions and national governments have to respect the independence of the ECB?'}, '5728116c3acd2414000df3a6': {'truth': 'bound to publish reports on its activities and has to address its annual report to the European Parliament', 'predicted': 'publish reports on its activities', 'question': \"How is the ECB held accountable for it's actions?\"}, '5726134f89a1e219009ac200': {'truth': 'much less', 'predicted': 'less', 'question': 'Do CFLs consume more, less, or the same amount of energy as incandescent bulbs?'}, '5730a045069b5314008321cf': {'truth': 'Greek surnames are most commonly patronymics', 'predicted': '', 'question': 'What names are used that are typically from  a father and have usually added a suffix or prefix ?'}, '5730a045069b5314008321d0': {'truth': 'Greek male surnames end in -s, which is the common ending', 'predicted': '', 'question': 'What letter of the alphabet do most of the last names the men of Greecs end with ?'}, '5730a045069b5314008321d1': {'truth': 'many have Latin, Turkish and Italian origin.', 'predicted': '', 'question': 'What other beginnings of origination do some of the last names of the Greeks share ?'}, '5730a045069b5314008321d2': {'truth': 'some end in -ou, indicating the genitive case of this proper noun for patronymic reasons.', 'predicted': '', 'question': 'What does it mean to have the letters OU added to the ending of a males last name ?'}, '56e6df336fe0821900b8ec13': {'truth': 'soft rock', 'predicted': 'dance-pop (such as upbeat songs by Madonna, Cher, Gloria Estefan and Kylie Minogue), power pops (mainly by boybands such as Backstreet Boys and Westlife), and adult-oriented soft rock', 'question': 'What genre of music is played by The Eagles?'}, '5710f431b654c5140001fa3f': {'truth': 'Thomas Jefferson', 'predicted': 'Jefferson', 'question': 'John Locke, Francis Bacon, and Isaac Newton where considered the the greatest men who ever lived by which American colonist?'}, '5710f431b654c5140001fa40': {'truth': 'the United States Constitution', 'predicted': 'United States Constitution', 'question': 'Religious tolerance and the importance of individual conscience was particularly influential in the drafting of which American document?'}, '570a5d534103511400d59674': {'truth': 'Imperial College Healthcare NHS Trust', 'predicted': '', 'question': 'What was formed on the 1st October 2007?'}, '570a5d534103511400d59676': {'truth': 'an academic health science centre', 'predicted': 'academic health science centre', 'question': 'What is it considered to be?'}, '5a48747484b8a4001a7e7888': {'truth': '', 'predicted': \"Hammersmith Hospitals NHS Trust (Charing Cross Hospital, Hammersmith Hospital and Queen Charlotte's and Chelsea Hospital\", 'question': 'what  hospitals were formed from Imperial College Healthcare?'}, '56defd9bc65bf219000b3e9b': {'truth': 'The Canadian Special Operations Forces Command', 'predicted': 'Canadian Special Operations Forces Command', 'question': 'what does CANSOFCOM stand for?'}, '56defd9bc65bf219000b3e9c': {'truth': 'generating special operations forces', 'predicted': 'generating special operations forces (SOF) elements to support CJOC', 'question': 'What is the CANSOFCOM focussed on?'}, '5ad3ec3b604f3c001a3ff759': {'truth': '', 'predicted': 'Canadian Special Operations Forces Command', 'question': 'what does CANSOFCORN stand for?'}, '570b28ab6b8089140040f7a4': {'truth': 'system of weeks', 'predicted': '', 'question': 'What other system of calculations are inherent in the Gregorian calendar?'}, '57301c4fb2c2fd140056888f': {'truth': 'a military coup led by Master Sergeant Samuel Doe of the Krahn ethnic group', 'predicted': 'Master Sergeant Samuel Doe', 'question': 'Who was responsible for the death of William R. Tolbert?'}, '57301c4fb2c2fd1400568890': {'truth': 'April 12, 1980', 'predicted': '', 'question': 'On what date was William R. Tolbert killed?'}, '57301c4fb2c2fd1400568891': {'truth': \"majority of Tolbert's cabinet and other Americo-Liberian government officials and True Whig Party members.\", 'predicted': '', 'question': \"Who was also executed on the day of William R. Tolbert's death?\"}, '5a62a976f8d794001af1c198': {'truth': '', 'predicted': 'Master Sergeant Samuel Doe', 'question': 'Who led a military coup that killed the American ambassador?'}, '5a62a976f8d794001af1c19a': {'truth': '', 'predicted': 'Master Sergeant Samuel Doe', 'question': \"Who tried to stop President Tolbert's assasination?\"}, '5a62a976f8d794001af1c19b': {'truth': '', 'predicted': 'corruption and political repression', 'question': 'What did the United States criticize the PRC for?'}, '56beb67d3aeaaa14008c929a': {'truth': 'beats', 'predicted': 'formulate beats', 'question': 'Beyonce does not create which aspect of her music?'}, '56bfb8dca10cfb1400551279': {'truth': 'female-empowerment', 'predicted': 'personally driven and female-empowerment', 'question': \"What theme was Beyonce's early music?\"}, '56bfb8dca10cfb140055127d': {'truth': 'melodies', 'predicted': 'comes up with melodies and ideas during production', 'question': 'What part of production does she do?'}, '56ddd49566d3e219004dad07': {'truth': 'Quito Astronomical Observatory', 'predicted': 'The Quito Astronomical Observatory', 'question': 'What observatory is the National Observatory of Ecuador?'}, '572a982b34ae481900deaba6': {'truth': 'the Silver Star Medal, Bronze Star Medal, and three Purple Heart Medals', 'predicted': 'Silver Star Medal', 'question': 'What medals did Kerry win?'}, '572956496aef051400154d14': {'truth': 'Brigadier Harvey', 'predicted': 'Harvey', 'question': 'Who was the youngest Royal Marine Brigadier?'}, '5ad423a1604f3c001a40084c': {'truth': '', 'predicted': 'developing the Bermuda Regiment', 'question': 'What was Major-General Charles Glyn Gilbert Anglim instrumental in?'}, '5ad423a1604f3c001a40084e': {'truth': '', 'predicted': 'Harvey', 'question': 'Who became the youngest-ever Marine Royal Brigadier?'}, '572b7eb8be1ee31400cb83ef': {'truth': 'anti-corrosion agent', 'predicted': '', 'question': 'Why is zinc oxide used in nuclear reactors?'}, '5acfc84c77cf76001a685f43': {'truth': '', 'predicted': \"radioactivity of the weapon's fallout\", 'question': 'What does 65Zn reduce?'}, '5acfc84c77cf76001a685f45': {'truth': '', 'predicted': '65Zn', 'question': 'What is used to study how alloys containing zinc melt?'}, '5a6246a3f8d794001af1bf2c': {'truth': '', 'predicted': 'headhunting', 'question': 'What ritual did Robert Lewis Stevenson say that the Samoans engaged in?'}, '5a6246a3f8d794001af1bf2d': {'truth': '', 'predicted': '1894', 'question': 'What year did John Williams die?'}, '570e38eb0dc6ce1900204ea7': {'truth': 'Italy', 'predicted': '', 'question': 'In what county was glass with uranium oxide content found?'}, '570e38eb0dc6ce1900204ea8': {'truth': '1912', 'predicted': '', 'question': 'In what year was yellow uranium oxide glass discovered?'}, '570e38eb0dc6ce1900204ea9': {'truth': 'the University of Oxford', 'predicted': 'University of Oxford', 'question': 'What institution did R.T. Gunther belong to?'}, '5ad1167a645df0001a2d0d14': {'truth': '', 'predicted': '79 CE', 'question': 'What was the latest year in recorded history that uranium oxide was used?'}, '5ad1167a645df0001a2d0d18': {'truth': '', 'predicted': 'University of Oxford', 'question': 'What institution did T.T. Gunther belong to?'}, '570b26c7ec8fbc190045b888': {'truth': 'the Xbox Guide button on the gamepad', 'predicted': 'Xbox Guide button', 'question': 'The simple dashboard could be accessed by pressing what controller button?'}, '570b26c7ec8fbc190045b889': {'truth': 'five', 'predicted': '', 'question': 'How many tabs were on the 360 dashboard interface?'}, '5a70c2358abb0b001a676177': {'truth': '', 'predicted': 'four', 'question': 'How many blades did the simplified version on the guide have?'}, '56dfa2414a1a83140091ebde': {'truth': 'sexual division', 'predicted': '', 'question': 'How is labor often divided in these groups?'}, '5acd54e907355d001abf3d74': {'truth': '', 'predicted': 'the same kind of quarry', 'question': 'In a majority of cases, women hunt what?'}, '5acd54e907355d001abf3d76': {'truth': '', 'predicted': 'dogs', 'question': \"The Ju'/hoansi women typically hunt in groups and with what?\"}, '5acd54e907355d001abf3d77': {'truth': '', 'predicted': 'quarry', 'question': \"Among the Ju'/hoansi women of China, women help men track down what?\"}, '5acd54e907355d001abf3d78': {'truth': '', 'predicted': 'lizards', 'question': 'Men in the Austrailian Martu primarily hunt small animals like what?'}, '57264dcd708984140094c1d8': {'truth': 'central part of the Aegean Sea', 'predicted': 'the central part of the Aegean Sea', 'question': 'The Cyclade islands are located where?'}, '57264dcd708984140094c1d9': {'truth': 'west coast of Turkey', 'predicted': 'off the west coast of Turkey', 'question': 'The North Aegean islands are located where?'}, '571ae53e9499d21900609b98': {'truth': 'severe penance', 'predicted': '', 'question': 'What happened to Bishop leaders who did not agree with the doctrine?'}, '56dfc2b77aa994140058e154': {'truth': 'cutting or burning undesirable plants', 'predicted': 'consciously manipulate the landscape through cutting or burning undesirable plants', 'question': 'How do they manage the landscape?'}, '56dfc2b77aa994140058e157': {'truth': 'domesticated food', 'predicted': 'domesticated food sources', 'question': 'What do modern hunter-gatherers depend on at least somewhat?'}, '5acd607f07355d001abf3fbc': {'truth': '', 'predicted': 'the landscape', 'question': 'Many hunter-gatherers unconsciously manipulate what?'}, '5acd607f07355d001abf3fbe': {'truth': '', 'predicted': 'game animals', 'question': 'Which group uses a slash-and-burn technique to create habitats for humans?'}, '5acd607f07355d001abf3fbf': {'truth': '', 'predicted': 'hunter-gatherers', 'question': 'Which group burns desirable plants while encouraging undesirable ones?'}, '572fffb8a23a5019007fcc29': {'truth': 'Second Triumvirate of Octavian', 'predicted': 'Second Triumvirate of Octavian, Lepidus and Mark Antony', 'question': 'What failure caused the the flares of civil war to spark up again?'}, '573367eed058e614000b5a5b': {'truth': 'Colombier Bay', 'predicted': '', 'question': 'What is the name of the deepest bay at St Barts?'}, '573367eed058e614000b5a5c': {'truth': 'small', 'predicted': 'small vessels', 'question': 'Grande Saline Bay provides docking for what kind of boats?'}, '573367eed058e614000b5a5d': {'truth': 'visible coral reef', 'predicted': '', 'question': 'The North and East sides of St. Barts are fringed by what?'}, '573367eed058e614000b5a5e': {'truth': 'shallow', 'predicted': 'shallow waters', 'question': 'Reefs are almost always in what type of water?'}, '5a39963a2f14dd001ac72435': {'truth': '', 'predicted': 'shallow waters', 'question': 'How deep do most of the coral reefs lie?'}, '5a39963a2f14dd001ac72436': {'truth': '', 'predicted': 'northwest', 'question': 'In what direction does St. Jean Bay lie from Colombier Bay?'}, '5a39963a2f14dd001ac72437': {'truth': '', 'predicted': 'northwest', 'question': 'In what direction does Grande Saline Bay lie from St. Jean Bay?'}, '56e75e1200c9c71400d77019': {'truth': 'Laidlines', 'predicted': '', 'question': 'What are small regular lines left on paper when handmade in a mould?'}, '56e75e1200c9c71400d7701b': {'truth': 'chainlines', 'predicted': '', 'question': 'What runs perpendicular to the laidlines?'}, '56e75e1200c9c71400d7701c': {'truth': 'Laidlines', 'predicted': '', 'question': 'Which is lines are commonly higher in density, laidlines or chainlines?'}, '5ad504cb5b96ef001a10a9d2': {'truth': '', 'predicted': 'Laidlines', 'question': 'What type of lines run parallel to chainlines?'}, '5731c1260fdd8d15006c6503': {'truth': 'mechanism of action', 'predicted': 'their mechanism of action, chemical structure, or spectrum of activity', 'question': 'Besides sprectrum of activity and chemical structure, how can antibacterial antibiotics classified?'}, '5733b4cf4776f419006610c9': {'truth': 'mechanism of action, chemical structure, or spectrum of activity', 'predicted': 'their mechanism of action, chemical structure, or spectrum of activity', 'question': 'What three ways are antibiotics classified?'}, '5733b4cf4776f419006610cd': {'truth': '(macrolides, lincosamides and tetracyclines', 'predicted': '', 'question': 'What 3 types go after protein synthesis?'}, '5a65cfcfc2b11c001a425d57': {'truth': '', 'predicted': 'bacteriostatic', 'question': 'Besides spectrum of activity and chemical structure, how can protein synthesis be classified?'}, '56e79ed037bdd419002c4268': {'truth': 'when DST rules change', 'predicted': 'DST rules change', 'question': 'When does the table of rule sets the Windows operating system uses have to be updated?'}, '56e79ed037bdd419002c426a': {'truth': '1987', 'predicted': '1987–2006 and post-2006', 'question': 'If located in Canada somewhere where DST is observed, a system running Vista might mishandle time stamps that are older than what year?'}, '56e79ed037bdd419002c426b': {'truth': '2006', 'predicted': 'post-2006', 'question': 'On a system running Windows older than Vista, locations in Canada observing DST would only reliably support time stamps from after what year?'}, '5725f1dc271a42140099d354': {'truth': 'the Royal Mews', 'predicted': 'Royal Mews', 'question': 'Where is the Gold State Coach housed?'}, '57261ed8ec44d21400f3d925': {'truth': 'horses', 'predicted': 'coach horses', 'question': 'What animals are kept in the mews?'}, '5a7a4c3d17ab25001a8a0493': {'truth': '', 'predicted': 'the Golden Jubilee of Elizabeth II', 'question': 'When was the Gold State Coach last used?'}, '572d3b0d8351f81400e9d37d': {'truth': 'music that emerged from the cultural milieu of punk rock in the late 1970s', 'predicted': 'music that emerged from the cultural milieu of punk rock in the late 1970s, although many groups now categorized as post-punk were initially subsumed under the broad umbrella of punk or new wave music', 'question': 'What is post-punk?'}, '572d3b0d8351f81400e9d37e': {'truth': 'new wave music', 'predicted': '', 'question': 'What else was music incorrectly catagorized into before post-punk?'}, '572d3b0d8351f81400e9d37f': {'truth': 'various groups commonly labeled post-punk in fact predate the punk rock movement', 'predicted': 'the accuracy of the term\\'s chronological prefix \"post\" has been disputed, as various groups commonly labeled post-punk in fact predate the punk rock movement', 'question': 'Why is the term post-punk sometimes disputed?'}, '572e6bacc246551400ce422d': {'truth': 'punk or new wave music', 'predicted': 'new wave music', 'question': 'What were many groups now labeled as post-punk initially categorized as?'}, '572e6bacc246551400ce422e': {'truth': 'predate the punk rock movement', 'predicted': 'the accuracy of the term\\'s chronological prefix \"post\" has been disputed, as various groups commonly labeled post-punk in fact predate the punk rock movement', 'question': \"Why has the prefix 'post' caused a bit of dispute as it relates to various post-punk groups?\"}, '5a270daac93d92001a4003a2': {'truth': '', 'predicted': '1978 and 1984', 'question': 'Between what years did the punk movement occur?'}, '5a270daac93d92001a4003a3': {'truth': '', 'predicted': 'new wave music', 'question': 'What were groups categorized as post-punk later changed to?'}, '5a282e9bd1a287001a6d0ac7': {'truth': '', 'predicted': 'new wave music', 'question': 'What else was music correctly catagorized into before post-punk?'}, '5a282e9bd1a287001a6d0ac8': {'truth': '', 'predicted': '1978 and 1984', 'question': 'Which years was the punk era between?'}, '5a4e8143755ab9001a10f4a1': {'truth': '', 'predicted': 'railway trunk routes', 'question': 'What are many cities located close to?'}, '5a4e8143755ab9001a10f4a5': {'truth': '', 'predicted': 'controlled-access highways', 'question': 'What does Seattle allow access to the AirTrain through?'}, '57271739f1498d1400e8f387': {'truth': 'type 2', 'predicted': 'type 2 diabetes', 'question': 'If someone struggles with insulin resistance, what kind of diabetes can develop as a result?'}, '57271739f1498d1400e8f388': {'truth': 'marked insulin resistance', 'predicted': 'insulin resistance', 'question': 'Almost all individuals who suffer from type 2 diabetes and/or obesity are found to have which trait?'}, '57301d1da23a5019007fcda7': {'truth': 'the San Ysidro neighborhood at the San Ysidro Port of Entry', 'predicted': 'San Ysidro neighborhood at the San Ysidro Port of Entry', 'question': \"Where is San Diego's border crossing?\"}, '57301d1da23a5019007fcda8': {'truth': '15-mile (24 km)', 'predicted': '15-mile', 'question': 'How long is the border that San Diego shares with Mexico?'}, '57301d1da23a5019007fcda9': {'truth': 'Otay Mesa', 'predicted': 'Otay Mesa area', 'question': 'Where is the next nearest commercial crossing at the border?'}, '57301d1da23a5019007fcdaa': {'truth': 'the third-highest volume of trucks and dollar value of trade among all United States-Mexico land crossings.', 'predicted': 'third-highest', 'question': 'What is the volumte of trucks handled at the Otay Mesa crossing?'}, '57295a38af94a219006aa308': {'truth': 'trance', 'predicted': 'trance music', 'question': 'What type of music was pioneered in Germany?'}, '57295a38af94a219006aa309': {'truth': 'electronic', 'predicted': '', 'question': 'What type of music does Kraftwerk make?'}, '57295a38af94a219006aa30a': {'truth': 'Rock am Ring', 'predicted': 'Rock am Ring festival', 'question': 'What is the largest music festival in Germany?'}, '5a63e5537f3c80001a150b86': {'truth': '', 'predicted': 'trance music', 'question': 'What type of dance was pioneered in Germany?'}, '5a63e5537f3c80001a150b87': {'truth': '', 'predicted': 'Rock am Ring festival', 'question': 'What is the largest music festival in Europe?'}, '57317628e6313a140071cf61': {'truth': 'inseparable from', 'predicted': '', 'question': 'What role did music play in the religious festivities? '}, '57317628e6313a140071cf62': {'truth': 'large variety of percussion and wind', 'predicted': '', 'question': 'What instruments were used to make music by the Central Americans?'}, '57317628e6313a140071cf63': {'truth': 'a jar in Guatemala', 'predicted': 'Guatemala', 'question': 'Where did archaeologists find a depiction of a Mayan stringed instrument?'}, '56cee23caab44d1400b88be6': {'truth': 'that China formally requested the support of the international community', 'predicted': 'that China formally requested the support of the international community to respond to the needs of affected families', 'question': 'What did UNICEF report?'}, '56d660e91c850414009470d6': {'truth': 'magnitude of the quake', 'predicted': 'Because of the magnitude of the quake, and the media attention on China', 'question': 'Why did the world community notice the need for help?'}, '5727865cf1498d1400e8fad1': {'truth': '63', 'predicted': '', 'question': 'How many students attended Eton free of charge in 2014?'}, '5727865cf1498d1400e8fad2': {'truth': 'raise pupil achievement, improve pupil self-esteem, raise pupil aspirations and improve professional practice across the schools', 'predicted': '', 'question': 'What are the goals of the Independent and State School Partnership?'}, '5ad2043fd7d075001a4281fb': {'truth': '', 'predicted': 'Tony Little', 'question': 'Who was the Head Master of Eton in 1982?'}, '5ad2043fd7d075001a4281fe': {'truth': '', 'predicted': 'Tony Little', 'question': 'Who was the Head Master of the Eton, Slough, Windsor and Hounslow Independent and State School Partnership in 2008?'}, '5ad3d325604f3c001a3ff26c': {'truth': '', 'predicted': 'Edinburgh of the Seven Seas', 'question': 'What is the name for the settlement located in the south-west coast?'}, '5ad3d325604f3c001a3ff26f': {'truth': '', 'predicted': 'Gough Island', 'question': 'Where is the weather station with a staff of 2,062 currently located?'}, '56e0a2a3231d4119001ac2f3': {'truth': 'the Chechen-Ingush ASSR', 'predicted': '', 'question': 'What state was dissolved on March 3, 1944?'}, '5727657f708984140094dcf9': {'truth': 'Tuscan', 'predicted': 'Tuscan dialect', 'question': 'Which dialect became the norm for the modern Italian language?'}, '5ad02a8077cf76001a686c56': {'truth': '', 'predicted': 'Tuscan dialect', 'question': 'Which dialect became the norm for the ancient Italian language?'}, '5726083a89a1e219009ac167': {'truth': 'Premier League title', 'predicted': '', 'question': 'What competition sparked the rivalry with Manchester?'}, '5726083a89a1e219009ac168': {'truth': '2008', 'predicted': '', 'question': \"What year's poll of supporters said that the Tottenham rivalry was greatest?\"}, '5acd176407355d001abf3442': {'truth': '', 'predicted': 'Manchester United', 'question': \"According to a 2003 online poll who is Fulham's biggest rival club?\"}, '5acd176407355d001abf3444': {'truth': '', 'predicted': 'North London derbies', 'question': 'What are matches between Chelsea and Manchester United called?'}, '56e10dbdcd28a01900c674e1': {'truth': 'the CDMA principle', 'predicted': 'CDMA principle', 'question': 'What are the ranging signals of the BeiDou system based on?'}, '56e10dbdcd28a01900c674e2': {'truth': 'open and restricted (military)', 'predicted': '', 'question': 'What positioning levels will the BeiDou system offer?'}, '56e10dbdcd28a01900c674e5': {'truth': '75+ satellites', 'predicted': '75+', 'question': 'How many satellites will the COMPASS navigation system use?'}, '5acd455207355d001abf3b89': {'truth': '', 'predicted': 'CDMA principle', 'question': 'The ranging signals have a simple structure and are based on which principle? '}, '5acd455207355d001abf3b8a': {'truth': '', 'predicted': 'complex structure', 'question': 'The ranging signals, based on the Galileo principle, have what type of structure?'}, '5acd455207355d001abf3b8c': {'truth': '', 'predicted': 'BeiDou navigation system', 'question': 'Which system is the predecessor of the GNSS system?'}, '572e9d6d03f9891900756837': {'truth': 'road, sea and air', 'predicted': 'by road, sea and air', 'question': 'What are the three modes of transport available on Cyprus?'}, '572e9d6d03f9891900756838': {'truth': '6,249 km (3,883 mi)', 'predicted': '6,249', 'question': 'How many miles of roads are paved on Cyprus?'}, '572e9d6d03f9891900756839': {'truth': '4,414 km (2,743 mi)', 'predicted': '4,414 km', 'question': 'How many miles of roads are unpaved on Cyprus?'}, '572e9d6d03f989190075683a': {'truth': 'left-hand', 'predicted': 'left-hand side', 'question': 'Which side of the road do vehicles on Cyprus drive on?'}, '5727b808ff5b5019007d935a': {'truth': 'Law should govern', 'predicted': '', 'question': 'What phrase Ariostle also use which is closely related to \"the rule of law\"?'}, '5727b808ff5b5019007d935b': {'truth': 'against the divine right of kings', 'predicted': 'divine right of kings', 'question': 'Samuel Rutherford used the principle of the rule of law to argue what point?'}, '5a3af1c53ff257001ab84354': {'truth': '', 'predicted': 'Samuel Rutherford', 'question': 'What British theologian used the rule of law in his argument against the divine rights of kings?'}, '5ad26126d7d075001a429005': {'truth': '', 'predicted': 'changes in pulse rate', 'question': ' Aside from decreased perspiration, what is a physiological change related to emotions?'}, '5ad4005e604f3c001a3ffccc': {'truth': '', 'predicted': 'Spanish', 'question': 'What language did the first blacks to arrive in Bermuda speak?'}, '56e19b21e3433e140042300a': {'truth': 'use of carbon monoxide through the water gas shift reaction', 'predicted': 'carbon monoxide', 'question': 'How can it be recovered through steam?'}, '5726cb515951b619008f7e4d': {'truth': '50', 'predicted': '50%', 'question': 'What percent of scientific research is done at UNAM?'}, '572f9c99a23a5019007fc7d5': {'truth': 'a 2, 3 or 4-digit sequential number with no significance as to device properties', 'predicted': '', 'question': 'What follows the 2N in a JEDEC EIA370?'}, '5a7b83a521c2de001afea0e4': {'truth': '', 'predicted': 'germanium', 'question': 'What were most early devices made from?'}, '5a7b83a521c2de001afea0e5': {'truth': '', 'predicted': '2, 3 or 4', 'question': 'What is the most common amount of extra digits in a device number?'}, '5a7b83a521c2de001afea0e8': {'truth': '', 'predicted': 'A', 'question': 'What is the most common letter suffix?'}, '56f9409c9b226e1400dd12c9': {'truth': 'Bellevue', 'predicted': 'Bellevue Hospital Center', 'question': 'Which hospital is located at the end if 27th Street?'}, '56f9409c9b226e1400dd12ca': {'truth': 'Chelsea', 'predicted': 'Chelsea Park', 'question': 'Which park does 27th Street pass through between Ninth and Tenth Avenues?'}, '5731628fe6313a140071ceb3': {'truth': 'wax or resin', 'predicted': 'wax or resin on a wooden panel', 'question': 'What were the tesserae usually set in for miniature mosaic icons?'}, '5731628fe6313a140071ceb4': {'truth': 'in the 12th century', 'predicted': '12th century', 'question': 'The more humanistic conception of Christ appeared when?'}, '56f756c6a6d7ea1400e171d4': {'truth': 'the score', 'predicted': '', 'question': 'Improvisation is integral before what took a high significance?'}, '56f756c6a6d7ea1400e171d5': {'truth': 'the 20th century', 'predicted': '20th century', 'question': 'When did oral tradition disappear?'}, '56f756c6a6d7ea1400e171d7': {'truth': 'block-rhythms', 'predicted': 'metrically strict block-rhythms', 'question': 'A score-centric approach strictly emphasizes what?'}, '57268a4cdd62a815002e88ac': {'truth': '\"snobs\" who want to \"impose their tastes on everyone else\"', 'predicted': '\"snobs', 'question': 'How did Murdoch describe critics of his newspaper?'}, '57268a4cdd62a815002e88ad': {'truth': 'they are \"giving the public what they want\"', 'predicted': '', 'question': 'What did Murdoch and Mackenzie say in defense of The Sun?'}, '57268a4cdd62a815002e88ae': {'truth': 'John Pilger', 'predicted': '', 'question': 'Who is one critic of The Sun?'}, '57268a4cdd62a815002e88af': {'truth': \"the genocide in Pol Pot's Cambodia\", 'predicted': \"genocide in Pol Pot's Cambodia\", 'question': \"What was the focus of Pilger's reporting in one issue of The Daily Mirror?\"}, '57109988a58dae1900cd6aba': {'truth': 'upper', 'predicted': 'upper classes', 'question': 'Did natural history in particular become increasingly popular amoung the upper or lower classes?'}, '570969eaed30961900e840cf': {'truth': 'published in more than one language, and their reach extends to almost all the Hindi-speaking states', 'predicted': '', 'question': 'What are the newspapers famous for?'}, '5a3636d5788daf001a5f8790': {'truth': '', 'predicted': 'Aapka Faisla, Amar Ujala, Panjab Kesari, Divya Himachal', 'question': 'What are the names of newspapers read widely in English?'}, '5732321ce17f3d1400422717': {'truth': 'edict of Milan', 'predicted': 'Milan', 'question': 'What edict defined imperial ideas as being those of toleration?'}, '5732321ce17f3d1400422718': {'truth': 'officially', 'predicted': 'officially embraced along with traditional religions and from his new Eastern capital, Constantine could be seen to embody both Christian and Hellenic religious interests', 'question': 'How did Constantine accept Christianity?'}, '5727faefff5b5019007d99d0': {'truth': 'Unicode Roadmap Committee', 'predicted': 'The Unicode Roadmap Committee', 'question': 'Michael Everson, Rick McGowan, and Ken Whistler make up what group?'}, '5727faefff5b5019007d99d2': {'truth': 'no proposal has yet been made', 'predicted': '', 'question': 'What proposal has been made for the Mayan script? '}, '5727faefff5b5019007d99d3': {'truth': 'Unicode Consortium Web site', 'predicted': 'Unicode Roadmap page of the Unicode Consortium Web site', 'question': 'Where does the Unicode Roadmap Committee post information on these scripts?'}, '5acd1d2407355d001abf3588': {'truth': '', 'predicted': 'Michael Everson, Rick McGowan, and Ken Whistler) maintain the list of scripts', 'question': 'Who are the potential candidates for encoding?'}, '571aeca132177014007e9ff1': {'truth': 'Bristol-Myers Squibb, Eli Lilly, Pfizer, AstraZeneca and Johnson & Johnson', 'predicted': '', 'question': 'What companies have been involved with health care fraud cases?'}, '571aeca132177014007e9ff2': {'truth': 'False Claims Act', 'predicted': '', 'question': 'What do illegal marketing cases fall under?'}, '5ad3b079604f3c001a3feca8': {'truth': '', 'predicted': \"Eli Lilly's antipsychotic Zyprexa\", 'question': 'What drugs were involved in cases of the largest Johnson & Johnson fines?'}, '570c56adfed7b91900d458d7': {'truth': 'acquiring the large sums of money needed', 'predicted': 'acquiring the large sums of money needed for his proposed campaigns to reclaim Normandy', 'question': \"What was one of John's principal challenges?\"}, '57300a06b2c2fd140056878a': {'truth': 'North Charleston', 'predicted': '', 'question': 'What is the third largest city in South Carolina?'}, '57300a06b2c2fd140056878c': {'truth': 'Dorchester', 'predicted': '', 'question': 'Charleston and Berkeley is combined with what other county to form a metropolitan statistical area?'}, '570b221b6b8089140040f768': {'truth': '2005', 'predicted': '', 'question': 'What year was the first HD video conferencing system displayed?'}, '570b221b6b8089140040f76a': {'truth': 'Las Vegas, Nevada', 'predicted': '', 'question': 'Where was the first HD video conferencing system displayed?'}, '570b221b6b8089140040f76b': {'truth': 'Polycom', 'predicted': '', 'question': 'What company introduced the first HD video conferencing system to the general market?'}, '570b221b6b8089140040f76c': {'truth': '1280 by 720', 'predicted': '', 'question': 'What was the resolution of the first HD video conferencing system?'}, '5a1f2bc43de3f40018b26537': {'truth': '', 'predicted': 'Interop trade show in Las Vegas, Nevada', 'question': 'Where was Polycom displayed in May 2005?'}, '5726ad725951b619008f79f0': {'truth': 'The providence petrel', 'predicted': 'providence petrel', 'question': 'What near extinct bird of Norfolk Island has shown signs of population increase?'}, '5726ad725951b619008f79f2': {'truth': 'the white-necked petrel, Kermadec petrel', 'predicted': '', 'question': 'What other types of petrels breed on Phillip Island?'}, '5726ad725951b619008f79f3': {'truth': 'the whale bird', 'predicted': 'whale bird', 'question': 'What is the sooty tern known as on Norfolk Island?'}, '5a81b79c31013a001a334dd1': {'truth': '', 'predicted': 'Nepean Island', 'question': 'What island in the Norfolk Island Group is home to bioluminescent seabirds?'}, '5727fb814b864d1900164148': {'truth': 'Northwestern', 'predicted': '', 'question': 'Where is the home of the Center for Catalysis and Surface Science?'}, '5727fb814b864d1900164149': {'truth': 'Northwestern', 'predicted': '', 'question': 'Where is the home of the International Institute for Nanotechnology?'}, '5727fb814b864d190016414a': {'truth': 'Northwestern', 'predicted': '', 'question': 'Where is the home of the Materials Research Center?'}, '5727fb814b864d190016414b': {'truth': 'Northwestern', 'predicted': '', 'question': 'Where is the home for the Institute for Policy Research?'}, '5727fb814b864d190016414c': {'truth': 'Northwestern', 'predicted': '', 'question': 'Where is the home of the Buffet Center for International and Comparative Studies?'}, '572f90e2a23a5019007fc768': {'truth': 'the 1890s', 'predicted': '1890s', 'question': 'When did Washington University begin to expand west?'}, '572f90e2a23a5019007fc769': {'truth': 'Olmsted, Olmsted & Eliot of Boston', 'predicted': 'Olmsted, Olmsted & Eliot', 'question': 'What architecture firm was hired by the Board of Directors at Washington University? '}, '572f90e2a23a5019007fc76b': {'truth': '\"Hilltop\" campus', 'predicted': 'Hilltop', 'question': 'What nickname was given to the new campus site?'}, '5ace156432bba1001ae49a6c': {'truth': '', 'predicted': 'Olmsted, Olmsted & Eliot', 'question': \"Who was one of the university's Board of Directors members in the 1890s?\"}, '5ace156432bba1001ae49a6e': {'truth': '', 'predicted': 'Robert Brookings', 'question': 'Who designed the Manual School?'}, '5ace156432bba1001ae49a6f': {'truth': '', 'predicted': 'Washington Ave., Lucas Place, and Locust Street', 'question': 'What is one of the streets that the Danforth campus now lies next to?'}, '57313831497a881900248c71': {'truth': 'low elevation', 'predicted': '', 'question': 'What geological situation makes Tuvalu prone to storm damage?'}, '57313831497a881900248c73': {'truth': '4.6 metres', 'predicted': '4.6 metres (15 ft) above sea level', 'question': 'What is the highest elevation on Tuvalu?'}, '57313831497a881900248c75': {'truth': 'second-lowest', 'predicted': 'second', 'question': 'Where does Tuvalu rank among other countries as to lowest elevation?'}, '572a470efed8de19000d5b67': {'truth': 'New Haven Museum and Historical Society', 'predicted': \"Eli Whitney Museum (across the town line in Hamden, Connecticut, on Whitney Avenue); the Yale Center for British Art, which houses the largest collection of British art outside the U.K., and the Yale University Art Gallery, the nation's oldest college art museum.[citation needed] New Haven is also home to the New Haven Museum and Historical Society\", 'question': \"There is a museum on Whitney Avenue that contain a variety of historical treasure, what is its' name?\"}, '572a470efed8de19000d5b68': {'truth': 'the Eli Whitney Museum', 'predicted': 'Eli Whitney Museum', 'question': 'What is the name of the museum that specifically focus on a single inventor located the city?'}, '5730048aa23a5019007fcc4d': {'truth': 'the Islamic Golden Age', 'predicted': 'Islamic Golden Age', 'question': 'What period was known for an era where Iranian civilization blossomed and peaked?'}, '5730048aa23a5019007fcc4e': {'truth': 'by the 10th and 11th centuries', 'predicted': '10th and 11th centuries', 'question': 'When did the Islamic Golden Age reach its zenith?'}, '5730048aa23a5019007fcc50': {'truth': 'scientific writing', 'predicted': 'scientific', 'question': 'Prominent Iranian writers during this time of the Islamic Golden Age contributed to what area of writing?'}, '59fb34d4ee36d60018400d68': {'truth': '', 'predicted': 'sixteen teams remained', 'question': 'What was the score when England played Italy in the 2014 FIFA World Cup?'}, '572b749abe1ee31400cb83ae': {'truth': 'exercise of reason and intellect', 'predicted': '', 'question': 'How did Hegel believe historical reality to be knowable to a philosopher?'}, '5a7c8ba9e8bc7e001a9e1eb2': {'truth': '', 'predicted': 'the finite and a dialectical philosophy of history', 'question': 'Kant and Fichte critiqued what?'}, '5a7c8ba9e8bc7e001a9e1eb5': {'truth': '', 'predicted': 'absolute\" idealism', 'question': 'How did Berkeley label his own idealism?'}, '572955e1af94a219006aa2cd': {'truth': 'requires the applicant to show their ability to test software', 'predicted': '', 'question': 'With several certifications out there that can be aquired, what is the one trait they all share?'}, '572955e1af94a219006aa2ce': {'truth': 'testing field is not ready for certification', 'predicted': '', 'question': 'What has the inability for the applicant to show how well they test led to?'}, '572955e1af94a219006aa2cf': {'truth': \"individual's productivity, their skill, or practical knowledge\", 'predicted': '', 'question': 'What four traits can a certification not measure?'}, '56cbedde6d243a140015edf4': {'truth': 'August', 'predicted': '11 August', 'question': 'During what month did Frédéric make his first appearance in Vienna?'}, '56cf6af94df3c31400b0d763': {'truth': 'Piano Concerto No. 2 in F minor, Op. 21', 'predicted': 'Piano Concerto No. 2', 'question': 'What piece did Chopin debut after returning to Warsaw?'}, '56cf6af94df3c31400b0d765': {'truth': 'accustomed to the piano-bashing of local artists', 'predicted': 'those accustomed to the piano-bashing of local artists', 'question': 'Why did some critics say that Chopin was too delicate?'}, '56d315d159d6e41400146224': {'truth': 'two', 'predicted': 'two piano concerts', 'question': 'How many public performances did Chopin do where he made his debut after completing his education?'}, '56d315d159d6e41400146225': {'truth': 'three', 'predicted': 'three weeks', 'question': 'How many weeks after completing school was it before Chopin made his public debut?'}, '57279c2edd62a815002ea1ee': {'truth': 'several', 'predicted': '', 'question': 'How many ways does the Mimamsa separate into subschools?'}, '57279c2edd62a815002ea1f0': {'truth': 'perception', 'predicted': '', 'question': 'How is pratyaksa defined in the Prabhakara subschool?'}, '5a5e51b25bc9f4001a75aee4': {'truth': '', 'predicted': 'comparison and analogy', 'question': 'What does Mimamsa mean?'}, '573254ece99e3014001e66c5': {'truth': 'Chairman of the Joint Chiefs of Staff', 'predicted': 'Chairman of the Joint Chiefs of Staff in Washington', 'question': 'What position did Eisenhower informally hold?'}, '572726a2f1498d1400e8f41c': {'truth': '1877', 'predicted': '', 'question': 'When did the Reconstruction Era end?'}, '5ad40ef3604f3c001a400145': {'truth': '', 'predicted': 'General William Tecumseh Sherman', 'question': 'Who proposed that land be divided and split up among white families?'}, '5727d1683acd2414000ded2f': {'truth': 'Christians', 'predicted': 'early Christians', 'question': 'Who shattered the fragments of a grand Mithraeum?'}, '5acd583707355d001abf3e0b': {'truth': '', 'predicted': '1988', 'question': ' In what year was the Bishopric of Strasbourg established?'}, '56f75c11a6d7ea1400e17202': {'truth': 'the Baroque and early romantic eras', 'predicted': 'during both the Baroque and early romantic eras', 'question': 'When was improvisation in classical music performance common?'}, '56f75c11a6d7ea1400e17203': {'truth': 'the second half of the 19th and in the 20th centuries', 'predicted': 'during the second half of the 19th and in the 20th centuries', 'question': 'When did improvisation begin to lessened strongly?'}, '56f75c11a6d7ea1400e17204': {'truth': 'the cadenzas to their piano concertos', 'predicted': 'piano concertos', 'question': 'What part did Mozart and Beethoven often improvise?'}, '56f75c11a6d7ea1400e17206': {'truth': 'soprano Maria Callas', 'predicted': 'Maria Callas', 'question': 'Who strongly supposed ome scritto?'}, '5734296dd058e614000b6a6e': {'truth': '40,000-plus', 'predicted': 'Another 40,000-plus', 'question': 'How many Montanans entered the miltary in the first year of the war?'}, '5734296dd058e614000b6a6f': {'truth': 'over 57,000', 'predicted': '57,000', 'question': 'How many Montanans joined the military in the war total?'}, '5734296dd058e614000b6a70': {'truth': 'At least 1500', 'predicted': '1500', 'question': 'About how many Montanans  died in the war?'}, '5734296dd058e614000b6a71': {'truth': 'First Special Service Force or \"Devil\\'s Brigade,\"', 'predicted': 'First Special Service Force or \"Devil\\'s Brigade', 'question': 'Who trained at the military grounds in Montana?'}, '56d0e42e17492d1400aab68a': {'truth': 'the Buddhacarita', 'predicted': 'Buddhacarita', 'question': 'What is one of the earlier biographies on Buddhism?'}, '56d0e42e17492d1400aab68c': {'truth': 'Buddha', 'predicted': '', 'question': 'Who founded a monastic order in his life?'}, '56d1c2d2e7d4791d0090211f': {'truth': 'Buddha', 'predicted': \"the Buddha's life\", 'question': \"Scholars do not make claims without evidence about who's life?\"}, '56d1c2d2e7d4791d00902120': {'truth': 'monastic', 'predicted': 'monastic order', 'question': 'Most accept that Buddha lived and taught in what type of order?'}, '56d1c2d2e7d4791d00902121': {'truth': '5th ce', 'predicted': '5th century CE', 'question': 'The Jataka tales of the Theravada happened in what century?'}, '570f887880d9841400ab35a1': {'truth': 'Elizabeth', 'predicted': '', 'question': \"Who is the world's oldest reigning monarch?\"}, '570f887880d9841400ab35a3': {'truth': 'great-great-grandmother', 'predicted': '', 'question': 'How is Victoria related to Elizabeth?'}, '570f887880d9841400ab35a4': {'truth': '2015', 'predicted': '', 'question': \"In what year did Elizabeth pass Victoria's length of rule?\"}, '5ad354ab604f3c001a3fdd8b': {'truth': '', 'predicted': 'Queen Victoria', 'question': 'Who had the shortest reign on the British throne?'}, '56d1335f17492d1400aabc16': {'truth': 'Amiibo', 'predicted': 'enhanced graphics and Amiibo functionality', 'question': 'What kind of functionality will the remaster feature?'}, '5a8db847df8bba001a0f9ba1': {'truth': '', 'predicted': 'Tantalus Media', 'question': 'Which company is responsible for the HD version of Nintendo Direct?'}, '5a8db847df8bba001a0f9ba4': {'truth': '', 'predicted': 'November 12, 2015', 'question': 'When were plans for Nintendo Direct revealed?'}, '572804792ca10214002d9b9e': {'truth': 'the Critique of Pure Reason', 'predicted': 'Critique of Pure Reason', 'question': 'In what did Immanuel Kant describe time as a priori intuition that allows humankind to understand sense experience?'}, '572804792ca10214002d9ba0': {'truth': 'an abstract conceptual framework', 'predicted': 'abstract conceptual framework', 'question': 'Kant thought of time as a fundamental part of what?'}, '572804792ca10214002d9ba2': {'truth': 'Spatial measurements', 'predicted': 'Spatial', 'question': 'What type of measurements are used to quantify the distances between objects?'}, '5a7e003d70df9f001a8753fe': {'truth': '', 'predicted': 'comprehend sense experience', 'question': 'What does the mind allow us to do according to Immanuel Kant?'}, '5a8103d68f0597001ac0022d': {'truth': '', 'predicted': 'Spatial', 'question': 'What type of measurements are used to quantify the distance between measurements?'}, '570d5aabfed7b91900d45f05': {'truth': 'Moriscos', 'predicted': 'Jews and the Moriscos', 'question': 'What people group was descended from Muslim converts to Christianity?'}, '56f8a3aa9b226e1400dd0d24': {'truth': 'snowmaking in the ski resorts', 'predicted': 'snowmaking', 'question': 'Water is diverted from rivers for what purpose?'}, '570b69c1ec8fbc190045ba00': {'truth': 'the New Wave of British Heavy Metal', 'predicted': 'New Wave of British Heavy Metal', 'question': 'What sub-genre of hard rock does Def Leppard belong to?'}, '5a5a3dd89c0277001abe70c0': {'truth': '', 'predicted': 'Def Leppard', 'question': 'Who is often categorized with the New Wave of American Heavy Metal?'}, '5a5a3dd89c0277001abe70c1': {'truth': '', 'predicted': \"High 'n' Dry\", 'question': 'What is the first album released by Def Leppard?'}, '5a5a3dd89c0277001abe70c3': {'truth': '', 'predicted': '1981', 'question': 'When did the band Ratt release their Too Fast for Love album?'}, '5ad02ad977cf76001a686c72': {'truth': '', 'predicted': 'Islam', 'question': 'What religion is notable due to an influx of foreign workers in the rural areas?'}, '5ad02ad977cf76001a686c76': {'truth': '', 'predicted': '19.9%', 'question': 'What is the percentage of Protestants in South Africa?'}, '570e6b020b85d914000d7eb5': {'truth': 'early second millennium BCE', 'predicted': 'second millennium BCE', 'question': 'When is it thought that early speakers of Sanskrit came to India?'}, '570e6b020b85d914000d7eb6': {'truth': 'close', 'predicted': 'close relationship', 'question': 'What is the relationship between Indo-Iranian and Baltic languages?'}, '5a2994e803c0e7001a3e17ed': {'truth': '', 'predicted': 'early second millennium BCE', 'question': 'When did the original speakers of Sanskrit migrate to the north-west?'}, '5a2ab45c5b078a001a2f06c1': {'truth': '', 'predicted': 'Indo-Aryan migration theory', 'question': 'What theory explains the different features of Sanskrit and other Indo-European languages?'}, '5a2ab45c5b078a001a2f06c2': {'truth': '', 'predicted': 'India and Pakistan', 'question': 'To where did early speakers from the north-east bring Sanskrit?'}, '572ebac003f98919007569ad': {'truth': 'Canada', 'predicted': 'Canada and stretching nearly to Mexico', 'question': 'Which northern country do the Rocky Mountains begin at?'}, '5a0f25d8d7c85000188645bd': {'truth': '', 'predicted': 'Rocky Mountains', 'question': 'Where did the Great Plains come to a gradual and at?'}, '5a0f25d8d7c85000188645be': {'truth': '', 'predicted': 'the United States', 'question': 'The appellation Mountains are the highest region of what?'}, '5a0f25d8d7c85000188645c0': {'truth': '', 'predicted': 'Colorado', 'question': 'The Lowes peak of the Rockies is found in what state?'}, '5726128a89a1e219009ac1ec': {'truth': 'the right to cover their upper body', 'predicted': 'cover their upper body', 'question': 'What right were lower caste women required to pay a tax to acquire?'}, '56fb2e3bf34c681400b0c1f5': {'truth': '1350', 'predicted': '', 'question': 'When did the Black Death end?'}, '56fb2e3bf34c681400b0c1f6': {'truth': 'Late', 'predicted': 'Late Middle Ages was marked by difficulties and calamities including famine, plague, and war, which significantly diminished the population of Europe; between 1347 and 1350', 'question': 'In what period of the Middle Ages did the Black Death occur?'}, '56fb2e3bf34c681400b0c1f7': {'truth': 'a third', 'predicted': 'about a third', 'question': 'What portion of the European population died in the Black Death?'}, '56fb2e3bf34c681400b0c1f8': {'truth': 'the early modern period', 'predicted': 'early modern period', 'question': 'What era occurred after the Late Middle Ages?'}, '572f54dba23a5019007fc554': {'truth': \"agreed to Army's demand that China be united under a Beijing government.\", 'predicted': '', 'question': 'What did China agree to avoid the undermining of the Republic?'}, '572f54dba23a5019007fc555': {'truth': 'Beijing, Shikai', 'predicted': 'Shikai', 'question': 'Who was sworn in as the second provisional president of the republic of China?'}, '572fdf3904bcaa1900d76e1f': {'truth': 'FC Ararat Yerevan team', 'predicted': 'FC Ararat Yerevan', 'question': 'What team won the Soviet football Cup in 1973 and 1975?'}, '572fdf3904bcaa1900d76e22': {'truth': 'eight teams', 'predicted': 'eight', 'question': 'How many teams does the Armenian Premier League have?'}, '56cbd8c66d243a140015ed86': {'truth': 'his love life and his early death', 'predicted': 'love life and his early death', 'question': \"What parts of Frédéric's personal life influenced his legacy as a leading symbol of the era?\"}, '56ce1138aab44d1400b8842a': {'truth': 'political insurrection', 'predicted': '', 'question': 'He had a non-direct association with what?'}, '56ce1138aab44d1400b8842b': {'truth': 'Romantic era', 'predicted': 'Romantic', 'question': 'Chopin is closely associated with what era?'}, '56de708f4396321400ee28e2': {'truth': 'the ecumenical councils', 'predicted': 'ecumenical councils', 'question': 'What were the meetings called that were hosted by Constantine that helped enforce orthodoxy by Imperial authority?'}, '5a5abce09c0277001abe7166': {'truth': '', 'predicted': 'ecumenical councils', 'question': 'What council was first held by Arius?'}, '57325b9fe99e3014001e670a': {'truth': \"Orwell's Nineteen Eighty-four\", 'predicted': '', 'question': \"What do former Jehovah's Witnesses members Heath and Gary Botting compare the culture of the religion to?\"}, '57325b9fe99e3014001e670c': {'truth': 'disparaging individual decision-making', 'predicted': \"by disparaging individual decision-making, the religion's leaders cultivate a system of unquestioning obedience in which Witnesses abrogate all responsibility and rights over their personal lives\", 'question': \"How do the leaders of the Jehovah's Witnesses cultivate a system of unquestioning obedience?\"}, '5733f165d058e614000b663e': {'truth': 'the Public Ministry.', 'predicted': 'the Public Ministry', 'question': 'Which entity oversees the Judicial Police?'}, '5725d36038643c19005acdb1': {'truth': '1948', 'predicted': 'Since the establishment of the State in 1948, and particularly since the late 1970s', 'question': 'When was Israeli fusion cuisine first developed?'}, '5728162d4b864d1900164442': {'truth': 'Call Of Duty 3', 'predicted': '', 'question': 'Which Call of Duty title does Sony include in their low-end price range?'}, '5728162d4b864d1900164443': {'truth': '2009', 'predicted': '', 'question': 'In what year was Devil May Cry 4 added to the budget game offerings for PS3?'}, '5728162d4b864d1900164444': {'truth': 'Greatest Hits', 'predicted': '', 'question': 'What words would you see in the United States or Canada on a PS3 game that would signify its lower price?'}, '5ad33f9a604f3c001a3fdbcb': {'truth': '', 'predicted': 'The Best', 'question': \"What's Sony's budget line of PS2 games called in Japan?\"}, '5ad33f9a604f3c001a3fdbcc': {'truth': '', 'predicted': 'Platinum', 'question': 'If you live in Australia and want affordable PlayStation 4 games, what range would you shop for?'}, '5ad33f9a604f3c001a3fdbcd': {'truth': '', 'predicted': \"Assassin's Creed and Ninja Gaiden Sigma\", 'question': 'Which Call of Duty title does Sony include in their high-end price range?'}, '5722ccb20dadf01500fa1ef4': {'truth': \"Through reading her mother's papers\", 'predicted': \"Through reading her mother's papers, Victoria discovered that her mother had loved her deeply; she was heart-broken\", 'question': 'How did Victoria realize that her mother deeply loved her?'}, '5722ccb20dadf01500fa1ef7': {'truth': \"Prince of Wales's philandering\", 'predicted': \"worry over the Prince of Wales's philandering\", 'question': \"What did Victoria blame Albert's death on?\"}, '5724d5ba0a492a190043563a': {'truth': 'an actress in Ireland', 'predicted': '', 'question': 'Who was the Prince of Wales suspected to be having an affair with? '}, '57257e8fcc50291900b28535': {'truth': '1861', 'predicted': 'March 1861', 'question': 'When did the Duchess die?'}, '57257e8fcc50291900b28539': {'truth': 'had slept with an actress in Ireland', 'predicted': 'slept with an actress in Ireland', 'question': 'What gossip did Prince Albert hear about their son?'}, '5ad176e7645df0001a2d1d21': {'truth': '', 'predicted': \"Through reading her mother's papers, Victoria discovered that her mother had loved her deeply\", 'question': 'How did Victoria realize that her mother slightly loved her?'}, '56f95c439b226e1400dd13a9': {'truth': 'the Gilbert and Marshall Islands campaign', 'predicted': 'Gilbert and Marshall Islands campaign', 'question': 'What was the name of the campaign in which the US occupied the Marshalls?'}, '56f95c439b226e1400dd13ab': {'truth': 'one', 'predicted': 'one month', 'question': 'How many months did it take for the US to occupy Kwajalein Atoll, Majuro and Enewetak?'}, '5726ed3ddd62a815002e9572': {'truth': 'Hanja', 'predicted': '', 'question': 'What is still being used according to experts?'}, '5726ed3ddd62a815002e9574': {'truth': 'weddings', 'predicted': '', 'question': 'What is considered a location with a high level of ambiguity?'}, '572a268f6aef051400155312': {'truth': '1772', 'predicted': '', 'question': 'In what year was Royal Prussia annexed?'}, '572a268f6aef051400155313': {'truth': '31 January 1773', 'predicted': '1773', 'question': 'In what year was the territory of Warmia incorporated? '}, '572a268f6aef051400155314': {'truth': 'The Polish Partition Sejm', 'predicted': '', 'question': 'What was ratified in 1773 in Prussia?'}, '5a3bf219cc5d22001a521c3f': {'truth': '', 'predicted': 'Frederick the Great', 'question': 'Who was the king of Poland? '}, '56fb85aab28b3419009f1df8': {'truth': '11th', 'predicted': 'late 11th century', 'question': 'During what century did the Investiture Controversy occur?'}, '56fb85aab28b3419009f1df9': {'truth': '1049', 'predicted': '1049–1054', 'question': 'When did the reign of Pope Leo IX begin?'}, '56fb85aab28b3419009f1dfc': {'truth': 'German princes', 'predicted': 'German princes at the expense of the German emperors', 'question': 'What secular rulers did the Concordat of Worms increase the power of?'}, '5726bd86f1498d1400e8e9b0': {'truth': 'Semantic-phonetic compounds', 'predicted': 'Semantic-phonetic compounds or pictophonetic compounds', 'question': 'What are the most numerous characters?'}, '57280c3f3acd2414000df30f': {'truth': 'Teddington Lock to the sea', 'predicted': 'Teddington Lock', 'question': \"What area of the River Thames does the Port of London Authority's jurisdiction cover?\"}, '57280c3f3acd2414000df312': {'truth': 'the London Ambulance Service (LAS) NHS Trust', 'predicted': 'London Ambulance Service (LAS) NHS Trust', 'question': 'The world\\'s largest \"free-at-the-point-of-use\" ambulance service is known as what?'}, '57280c3f3acd2414000df313': {'truth': 'The London Air Ambulance charity', 'predicted': 'London Air Ambulance charity', 'question': 'What agency operates in conjunction with the LAS as needed?'}, '572913111d0469140077901c': {'truth': 'Staaten', 'predicted': 'Staaten (States)', 'question': 'Before 1919 what were the German states called?'}, '572913111d0469140077901d': {'truth': 'Freistaaten', 'predicted': 'Freistaaten (Free States', 'question': 'What does Bavaria refer to itself as?'}, '5a4736e95fd40d001a27dd78': {'truth': '', 'predicted': 'Hamburg and Bremen', 'question': 'What two city-states are in Bavaria?'}, '5a513e8ece860b001aa3fc7e': {'truth': '', 'predicted': 'Staaten (States)', 'question': 'What were the constituent states of the German Empire called after 1919?'}, '5725e48589a1e219009ac05b': {'truth': 'Parthian', 'predicted': 'Parthian counterattack', 'question': 'Who was Antiochus VII Sidetes killed by which army?'}, '5725e48589a1e219009ac05d': {'truth': 'Greek drachmas', 'predicted': '', 'question': 'What was the currency in the Parthian Empire?'}, '56e0bc7b231d4119001ac364': {'truth': 'December 6, 195', 'predicted': 'December 6, 1957', 'question': 'Project Vanguard launch failed on what date?'}, '572ac792111d821400f38d60': {'truth': 'Russia', 'predicted': 'Russian Foreign Minister Sergey Lavrov said Russia', 'question': 'Which country convinced Syria to actually give up its chemical weapons?'}, '572ac792111d821400f38d61': {'truth': 'Russian Foreign Minister', 'predicted': 'Foreign Minister', 'question': \"What was Sergey Lavrov's position?\"}, '57307352069b5314008320eb': {'truth': 'total number of Greeks living outside Greece and Cyprus today is a contentious issue', 'predicted': '', 'question': 'How many people that are of Greek ascendancy live elsewhere than Greece ?'}, '57307352069b5314008320ec': {'truth': 'World Council of Hellenes Abroad', 'predicted': 'SAE - World Council of Hellenes Abroad', 'question': 'Who provided the contradictory  population numbers for Greeks abroad ?'}, '57307352069b5314008320ef': {'truth': 'George Prevelakis of Sorbonne University, the number is closer to just below 5 million', 'predicted': '', 'question': 'Who has presented the contradictory number to the census groups ?'}, '56df64a68bc80c19004e4bb3': {'truth': 'The University of St Mark & St John', 'predicted': 'University of St Mark & St John', 'question': 'What institution of higher  education is colloquially known as Marjons?'}, '572cb837750c471900ed4cf3': {'truth': 'involve claims and defenses under state laws', 'predicted': 'Most cases are litigated in state courts and involve claims and defenses under state laws', 'question': 'What types of cases are argued in the state courts?'}, '572cb837750c471900ed4cf5': {'truth': '272,795', 'predicted': '', 'question': 'How many cases did appellate courts receice in 2010?'}, '572cb837750c471900ed4cf6': {'truth': '282,000 new civil cases, 77,000 new criminal cases, and 1.5 million bankruptcy cases', 'predicted': '', 'question': 'What types of cases did federal district courts receive in 2010?'}, '5a79f25b17ab25001a8a0200': {'truth': '', 'predicted': '5.9 million', 'question': 'How many trial courts received domestic relations cases in 2010?'}, '5a79f25b17ab25001a8a0201': {'truth': '', 'predicted': \"National Center for State Courts' Court Statistics Project\", 'question': 'What organization started in 2012?'}, '5a591bbb3e1742001a15cf91': {'truth': '', 'predicted': 'DNA', 'question': 'What does a molecule contain?'}, '5a591bbb3e1742001a15cf93': {'truth': '', 'predicted': 'DNA', 'question': 'What is included in virus RNA?'}, '57283f892ca10214002da183': {'truth': 'at least 14', 'predicted': '14', 'question': 'How many parts does a VHS tape have that must be manufactured?'}, '57283f892ca10214002da185': {'truth': 'as little as $1.00', 'predicted': '$1.00', 'question': 'How much did VHS cost to produce by the 1990s?'}, '5728ea364b864d1900165084': {'truth': 'translator', 'predicted': '', 'question': \"What was William Scott Wilson's occupation?\"}, '5728ea364b864d1900165085': {'truth': 'bushi (武士?, [bu.ɕi]) or buke (武家?)', 'predicted': 'bushi', 'question': 'What are samurai usually called in Japanse?'}, '56f8ec5d9e9bad19000a0704': {'truth': 'regions in their inventory', 'predicted': '', 'question': 'Assyria, Chaldea, Mesopotamia, Persia, Armenia, Egypt, Arabia, Syria, Palestine, Ethiopia, Caucasus, Libya, Anatolia, and Abyssinia were all what?'}, '5729355b1d0469140077916d': {'truth': 'forensic', 'predicted': 'forensic anthropology', 'question': 'What type of anthropology is \"race\" sometimes still used within?'}, '5729355b1d0469140077916f': {'truth': 'specific region', 'predicted': '', 'question': 'What can forensic anthropologists determine about the ancestors of someone from their skeletal remains?'}, '5729355b1d04691400779170': {'truth': 'particular context', 'predicted': '', 'question': 'What does Brace feel the term \"black\" in meaningful in?'}, '5729355b1d04691400779171': {'truth': 'is not itself scientifically valid', 'predicted': '', 'question': 'Why is it bad that a category is merely socially constructed?'}, '5726baf5f1498d1400e8e92c': {'truth': 'Charles Wide', 'predicted': 'Judge Charles Wide', 'question': 'Who was appointed presiding judge over the retrial in 2015?'}, '5726baf5f1498d1400e8e92f': {'truth': 'why Marks was being replaced by Wide', 'predicted': '', 'question': 'What did Rumfit state that the defendants should have been informed about?'}, '5726baf5f1498d1400e8e930': {'truth': 'take the decision to judicial review', 'predicted': '', 'question': 'What did the lawyers for the defendants threaten to do?'}, '56fb69e38ddada1400cd63f6': {'truth': 'peasants', 'predicted': 'peasants settled on small farms', 'question': 'Of what class were most inhabitants of the Carolingian Empire?'}, '5729fcffaf94a219006aa726': {'truth': 'certain amount of energy', 'predicted': 'a certain amount of energy', 'question': 'Mass is also equivalent to what?'}, '5acd46d507355d001abf3bce': {'truth': '', 'predicted': 'Albert Einstein', 'question': 'Who created the formula E = nc2?'}, '56dcdbe566d3e219004dab35': {'truth': 'Pygmy', 'predicted': 'the Pygmy people', 'question': 'What group of people were living in the area that would become the Congo prior to the arrival of Bantu tribes?'}, '56dcdbe566d3e219004dab37': {'truth': 'Bantu', 'predicted': 'Bantu ethnic group that also occupied parts of present-day Angola, Gabon, and Democratic Republic of the Congo', 'question': 'The Bakongo were a group derived from which tribes?'}, '5acff76f77cf76001a686698': {'truth': '', 'predicted': 'Bantu kingdoms—notably those of the Kongo, the Loango, and the Teke', 'question': 'What Pygmy kingdom built trade links with the Congo River basin?'}, '5726f52bdd62a815002e9642': {'truth': 'Nigeria', 'predicted': '', 'question': 'What was the last African country to still have significant Polio problems?'}, '5726f52bdd62a815002e9645': {'truth': 'the University of Nigeria', 'predicted': 'University of Nigeria', 'question': \"Which entity runs Nigeria's bone marrow donation program?\"}, '572b72e9be1ee31400cb83a3': {'truth': 'state of Great Moravia', 'predicted': 'the state of Great Moravia', 'question': 'What did the ninth century bring?'}, '572b72e9be1ee31400cb83a4': {'truth': 'East Francia', 'predicted': 'Byzantine ruler Michael III to send missionaries in an attempt to reduce the influence of East Francia', 'question': 'Whose influence was Rastislav eager to reduce, when he invited  Michael III to send missionaries? '}, '572a2b821d04691400779803': {'truth': 'physically possible process', 'predicted': 'a physically possible process', 'question': 'If if a cinematographic film were taken by means of physical  laws and then played backwards, it would still portray what?'}, '572a2b821d04691400779804': {'truth': 'not time-reversal invariant', 'predicted': '', 'question': 'How is our experience of time at the macro level?'}, '572a2b821d04691400779805': {'truth': 'the future', 'predicted': '', 'question': 'What do we not have memories of?'}, '5a42cd804a4859001aac7327': {'truth': '', 'predicted': 'fundamental physical laws', 'question': 'What laws are time-reversal variant?'}, '57277595708984140094de38': {'truth': 'their areas of origin', 'predicted': 'areas of origin', 'question': 'What does the head regalia of the bell-ringers represent?'}, '5727b1c13acd2414000de9eb': {'truth': 'Advaita', 'predicted': '', 'question': 'What means '}, '5a5e5b755bc9f4001a75af3b': {'truth': '', 'predicted': 'Adi Shankara', 'question': 'Who coined the term Advaita?'}, '5a5e5b755bc9f4001a75af3c': {'truth': '', 'predicted': 'Adi Shankara', 'question': \"Who was Gaudapada's teacher?\"}, '57095defed30961900e84008': {'truth': 'corrosion-resistant', 'predicted': 'structural and corrosion-resistant', 'question': 'Name a property that makes copper a good material to use in marine environments?'}, '57279c33708984140094e235': {'truth': 'remove profanity, but he also made stylistic revisions', 'predicted': 'stylistic revisions', 'question': 'What alterations did Crane make to secure commercial publication?'}, '57279c33708984140094e236': {'truth': 'to preserve the stylistic and literary changes of 1896', 'predicted': '', 'question': \"What was Bower's first step in editing multiple works into a single product?\"}, '57279c33708984140094e237': {'truth': \"to revert to the 1893 readings where he believed that Crane was fulfilling the publisher's intention rather than his own\", 'predicted': '', 'question': \"What was Bower's second step in editing multiple works into a single product?\"}, '57279d21ff5b5019007d910e': {'truth': '1896', 'predicted': '', 'question': 'I what year was Maggie printed for commercial use?'}, '57279d21ff5b5019007d910f': {'truth': 'to preserve the stylistic and literary changes of 1896,', 'predicted': '', 'question': 'What was the first step Bowers took in editing a single work with two versions?'}, '57279d21ff5b5019007d9110': {'truth': \"to revert to the 1893 readings where he believed that Crane was fulfilling the publisher's intention\", 'predicted': '', 'question': 'What was the second step Bowers took in editing a single work with two versions?'}, '57279d21ff5b5019007d9111': {'truth': 'to remove profanity, but he also made stylistic revisions', 'predicted': '', 'question': 'What changes were made for the commercial publication of Maggie?'}, '570a6c176d058f1900182e4c': {'truth': 'Kurt Kortschal', 'predicted': 'Kurt Kortschal 2013', 'question': 'Who researched the role of emotional phenotype temperaments on social connectedness?'}, '570a6c176d058f1900182e4d': {'truth': 'their parental germ cells', 'predicted': 'parental germ cells', 'question': 'From where do zygotes derive their genetic information?'}, '570a6c176d058f1900182e50': {'truth': '200,000 years', 'predicted': '', 'question': 'About how long ago did modern human beings first come into existence?'}, '5ad255afd7d075001a428d48': {'truth': '', 'predicted': '1.2%', 'question': ' What percentage of similarity is there between the genetic material of humans and the genetic material of chimpanzees?'}, '572a22256aef0514001552f8': {'truth': 'Sultans', 'predicted': 'weak Sultans', 'question': 'Poor rule by what class of people strained the empire?'}, '572a22256aef0514001552f9': {'truth': 'military technology', 'predicted': 'military', 'question': 'Europeans gained on the Ottoman empire in what type of technology? '}, '572a22256aef0514001552fa': {'truth': 'religious and intellectual', 'predicted': 'religious and intellectual conservatism', 'question': 'What types of conservative beliefs slowed the expansion of the empire?'}, '57261a8e38643c19005acff5': {'truth': 'Empiric', 'predicted': 'Empiric school', 'question': 'Which school of medicine was based on strict observation?'}, '570d9a31df2f5219002ed00e': {'truth': 'the Kerrison Predictor', 'predicted': 'Kerrison Predictor', 'question': 'What was the name of the mechanical computer that used automation?'}, '570d9a31df2f5219002ed00f': {'truth': 'the proper aim point automatically', 'predicted': 'proper aim point', 'question': 'What did the Predictor calculate after it was pointed at a target?'}, '570d9a31df2f5219002ed010': {'truth': 'as a pointer mounted on the gun', 'predicted': 'a pointer mounted on the gun', 'question': 'How did the Predictor display the information needed?'}, '57316532a5e9cc1400cdbf19': {'truth': 'Strange Stories', 'predicted': 'Strange Stories from a Chinese Studio', 'question': \"What was the name of Pu Songling's collection of short stories?\"}, '572668e2708984140094c518': {'truth': 'rebelled', 'predicted': 'rebelled against', 'question': 'How did the Ciccone siblings behaved towards anyone brought to their home to replace their beloved mother?'}, '572668e2708984140094c51a': {'truth': 'unable to sleep unless she was near him', 'predicted': 'unable to sleep', 'question': 'Afraid that Tony would be taken from her, what does she do?'}, '57300da0947a6a140053cffa': {'truth': 'Pictish (northern Britain)', 'predicted': 'Hiberni (Ireland), Pictish (northern Britain) and Britons', 'question': 'Which is one of the tribes that spoke Insular Celtic?'}, '57300da0947a6a140053cffb': {'truth': 'beginning of the 1st millennium AD', 'predicted': '1st millennium AD', 'question': 'When did the Pictish tribe start to inhabit the islands?'}, '57300da0947a6a140053cffe': {'truth': 'Northern Ireland', 'predicted': '', 'question': 'What are the six counties in Ireland called that are still part of the United Kingdom?'}, '5acd1c9c07355d001abf3582': {'truth': '', 'predicted': 'six', 'question': 'How many counties fought with the UK during the Irish War of Independence?'}, '5acd6a7807355d001abf4126': {'truth': '', 'predicted': 'Insular Celtic', 'question': 'The Hiberni tribes of northern Britain speak which language?'}, '5acd6a7807355d001abf4127': {'truth': '', 'predicted': 'at the beginning of the 1st millennium AD', 'question': 'The Pictish tribe of Southern Ireland inhabited the islands when? '}, '5acd6a7807355d001abf4128': {'truth': '', 'predicted': 'AD 43', 'question': 'The Anglo-Saxon empire was conquered by the Roman Empire from when? '}, '5acd6a7807355d001abf4129': {'truth': '', 'predicted': 'England', 'question': 'The Anglo Saxons arrived in the 3rd century and would end up dominated what is modernly known as what country?'}, '5acd6a7807355d001abf412a': {'truth': '', 'predicted': '1919–1922', 'question': 'The Anglo-Viking treaty is associated with which time frame? '}, '5ad2de1cd7d075001a42a57e': {'truth': '', 'predicted': 'Isa Masih', 'question': 'Which term means followers of Jesus?'}, '5ad2de1cd7d075001a42a57f': {'truth': '', 'predicted': 'Nasrani', 'question': 'Which Christian term is attached to Saint Thomas Christians?'}, '5ad2de1cd7d075001a42a580': {'truth': '', 'predicted': 'Isa Masih', 'question': 'What do people in Nasrani call Jesus?'}, '5730b1488ab72b1400f9c6a5': {'truth': 'Super Advantage', 'predicted': 'Super Advantage is an arcade-style joystick', 'question': 'What game accessory was similar to the NES Advantage?'}, '5a42e7df4a4859001aac7394': {'truth': '', 'predicted': 'turbo settings', 'question': 'What adjustable settings did the Super Scope have on its joystick?'}, '572a5f33b8ce0319002e2ae5': {'truth': 'traditional Byzantine art', 'predicted': 'Byzantine art', 'question': 'Ottoman artists mixed Chinese art with the art of what else?'}, '572a5f33b8ce0319002e2ae6': {'truth': 'fountains and schools', 'predicted': 'mosques, bridges, fountains and schools', 'question': 'The ottoman empire built structures in Romania that included Mosques, and Bridges, what else was built?'}, '570b609e6b8089140040f8eb': {'truth': 'Rush, Fly by Night and Caress of Steel', 'predicted': 'Fly by Night and Caress of Steel', 'question': 'What are the first three Rush albums?'}, '570b609e6b8089140040f8ed': {'truth': '\"The Boys Are Back in Town\"', 'predicted': 'The Boys Are Back in Town', 'question': \"What was Thin Lizzy's hit single?\"}, '5a5a32199c0277001abe70a3': {'truth': '', 'predicted': 'more progressive', 'question': 'What sound did the band Rush move toward in the 1976 album Caress of Steel?'}, '5a5a32199c0277001abe70a5': {'truth': '', 'predicted': 'The Boys Are Back in Town', 'question': 'Which song by Lizzy reached number 8 on the US charts in 1976?'}, '572a8990f75d5e190021fb59': {'truth': '2015', 'predicted': '', 'question': 'In what year did the Indian government begin to stop recognizing madaris as schools?'}, '57277e4edd62a815002e9eb5': {'truth': 'Ash', 'predicted': 'Ash Wednesday', 'question': 'Dutch Carnaval is celebrated until which Wednesday?'}, '57277e4edd62a815002e9eb7': {'truth': 'herring', 'predicted': 'eating herring', 'question': 'What is consumed on Ash Wednesday?'}, '570e432f0dc6ce1900204ee2': {'truth': '1–2%', 'predicted': '', 'question': 'What percentage of high-density penetrators is not made up of depleted uranium?'}, '570e432f0dc6ce1900204ee3': {'truth': 'molybdenum', 'predicted': '', 'question': 'Along with titanium, what element often makes up the portion of high-density penetrators not made of depleted uranium?'}, '5ad113dc645df0001a2d0c8d': {'truth': '', 'predicted': 'Persian Gulf', 'question': 'Along with the Balkans, in what geographical location did a war take place where the UN used depleted uranium munitions?'}, '5732549b0fdd8d15006c69c9': {'truth': 'professional', 'predicted': 'professional and governmental', 'question': 'Along with business and government, what leaders did Eisenhower see meeting at the Council on Foreign Relations?'}, '5732549b0fdd8d15006c69ca': {'truth': 'economic', 'predicted': 'economic analysis', 'question': 'What sort of analysis did Eisenhower first experience with the Council on Foreign Relations?'}, '572ba616111d821400f38f36': {'truth': 'genitive', 'predicted': 'genitive case', 'question': 'What case are numbers over five in?'}, '572ba616111d821400f38f37': {'truth': 'when the entire expression is in nominative or accusative case', 'predicted': '', 'question': 'When is the genitive case used?'}, '5727972a708984140094e1a9': {'truth': 'Bangladesh, Brazil, China, Egypt', 'predicted': '', 'question': 'What are some of the target countries?'}, '5727972a708984140094e1aa': {'truth': 'universal primary school', 'predicted': '', 'question': 'What do they want to do with regards to schooling of young children?'}, '5ad014c177cf76001a686917': {'truth': '', 'predicted': 'December 30, 2010', 'question': 'When did the Congolese parliament strike down a law to protect indigenous people?'}, '5ad014c177cf76001a686919': {'truth': '', 'predicted': 'property', 'question': 'What does the Congolese Human Rights Observatory say Bantus are treated as?'}, '56de45d84396321400ee2754': {'truth': 'being a non-UN member or unable or unwilling to provide the necessary data at the time of publication', 'predicted': '', 'question': 'What three reasons were mentioned for countries being excluded?'}, '5acd71d807355d001abf4292': {'truth': '', 'predicted': 'Missionaries and scholars', 'question': 'Who delivered new ideas to other civilizations?'}, '56dff3c2231d4119001abef0': {'truth': 'ITV', 'predicted': 'BBC One and the Woolpack in ITV', 'question': 'What channel is home to the soap opera Emmerdale?'}, '5726a3fc5951b619008f78b5': {'truth': 'French and Germans', 'predicted': 'European nations, particularly the French and Germans', 'question': 'Which two nations has The Sun been very antagonistic towards?'}, '5726a3fc5951b619008f78b6': {'truth': '\"frogs\", \"krauts\" or \"hun\"', 'predicted': '', 'question': 'What names were used by The Sun to characterize the French and Germans?'}, '5726872c5951b619008f75c9': {'truth': \"one of the world's longest-running ongoing civil wars.\", 'predicted': 'civil wars', 'question': 'What major conflict is Myanmar known for?'}, '5726872c5951b619008f75cb': {'truth': 'a nominally civilian government', 'predicted': 'civilian', 'question': 'What type of government is now established in Myanmar?'}, '5726872c5951b619008f75cc': {'truth': 'former military leaders still wield enormous power in the country', 'predicted': 'former military leaders still wield enormous power', 'question': 'Are previous leaders a hendrence to the current government?'}, '57279c1aff5b5019007d90ea': {'truth': 'some point between 915 and 922', 'predicted': '915 and 922', 'question': 'When did Viking travelers establish a trading post in Cork?'}, '57279c1aff5b5019007d90ec': {'truth': 'otherwise unobtainable trade goods', 'predicted': 'unobtainable trade goods', 'question': 'What did the Norsemen provide to the monastery?'}, '5a5d0b835e8782001a9d5e73': {'truth': '', 'predicted': 'the Norsemen', 'question': 'Who did the monastery provide religious services for?'}, '56de71b2cffd8e1900b4b8fd': {'truth': 'Henry VIII', 'predicted': '', 'question': \"During what king's reign did 60 Protestants die for heresy?\"}, '5a5ad1eb9c0277001abe71b1': {'truth': '', 'predicted': 'Henry VIII', 'question': \"During who's reign were Catholics executed as heretics?\"}, '56f8dbf69e9bad19000a0611': {'truth': 'RMS Queen Elizabeth 2', 'predicted': 'Queen Elizabeth 2', 'question': 'What was the name of the liner that left Southampton on its final journey on November 11, 2008?'}, '572f2ce3a23a5019007fc4b5': {'truth': 'Western Railroad', 'predicted': 'Delaware, Lackawanna & Western Railroad', 'question': 'What was New Jersey Transit called before?'}, '572f2ce3a23a5019007fc4b6': {'truth': 'AC', 'predicted': '', 'question': 'What does the railway system of US use DC or AC?'}, '5acd72be07355d001abf42c4': {'truth': '', 'predicted': '25 kV 50 Hz AC', 'question': 'The Kolkata suburban railway in India converted from what?'}, '570a55836d058f1900182d68': {'truth': '1950s', 'predicted': '1950s & 1960s', 'question': 'In which decade did the expansion of the South Kensington campus being?'}, '5a4860a984b8a4001a7e7858': {'truth': '', 'predicted': 'Natural History Museum', 'question': \"What are some of the institutions are across from Imperial's main campus?\"}, '5a4860a984b8a4001a7e785a': {'truth': '', 'predicted': 'South Kensington', 'question': 'What campus was absorbed by the Imperial Institute?'}, '5723df4df6b826140030fccf': {'truth': 'amity', 'predicted': '', 'question': 'What must two lodges be in, in order to inter-visit?'}, '5723df4df6b826140030fcd1': {'truth': 'Exclusive Jurisdiction and Regularity', 'predicted': '', 'question': 'What can be causes of one Grand Lodge withdrawing Recognition from another?'}, '5723df4df6b826140030fcd2': {'truth': 'brethren', 'predicted': '', 'question': 'What are the members of a Grand Lodge called?'}, '5726225d271a42140099d4c3': {'truth': 'amity', 'predicted': '', 'question': 'Gran Lodges are in what when they are in Masonic Communication with each other?'}, '5726225d271a42140099d4c5': {'truth': 'a list', 'predicted': 'a list of other Grand Lodges that it recognises', 'question': 'What does each Grand Lodge maintain?'}, '5acf801b77cf76001a684fe8': {'truth': '', 'predicted': 'inter-visitation', 'question': 'What, besides Recognition, must never happen between two Grand Lodges in order for them to be considered in amity?'}, '5acf801b77cf76001a684fea': {'truth': '', 'predicted': 'inter-visitation', 'question': 'What is not allowed when Grand Lodges are empty?'}, '56beabab3aeaaa14008c91dc': {'truth': 'Ban Bossy campaign', 'predicted': 'Ban Bossy', 'question': 'Beyonce supported which campaign that encourages leadership in girls?'}, '570a6f996d058f1900182e60': {'truth': 'Baruch Spinoza', 'predicted': 'Niccolò Machiavelli, Baruch Spinoza', 'question': 'Along with Descartes, Machiavelli and Hume, what notable philosopher developed a theory of emotions?'}, '572f6c2cb2c2fd14005680f9': {'truth': 'the Kakatiya dynasty was annexed', 'predicted': 'annexed', 'question': 'What did the Malik Kafur do to the Kakatiya dynasty in 1321?'}, '570d6cf1fed7b91900d460a7': {'truth': 'lethal French gunfire', 'predicted': 'French gunfire', 'question': 'What factor immobilised the Prussian Guard?'}, '571ae71a32177014007e9fc7': {'truth': 'opportunity cost of investing capital', 'predicted': '', 'question': 'What accounts for nearly half of the costs to develop drugs?'}, '571ae71a32177014007e9fc8': {'truth': 'increasingly outsource risks related to fundamental research', 'predicted': '', 'question': 'What is the consequence in the value chain?'}, '571ae71a32177014007e9fc9': {'truth': 'somewhat reshapes the industry ecosystem with biotechnology companies', 'predicted': '', 'question': 'What happens when companies outsource?'}, '571d1d495efbb31900334ea8': {'truth': 'nearly half the total expense', 'predicted': 'nearly half', 'question': 'Investing capital can increase drug development costs by how much?'}, '5ad399f2604f3c001a3fe843': {'truth': '', 'predicted': 'approved drugs', 'question': 'What drugs are the least fundamental to develop?'}, '5ad399f2604f3c001a3fe847': {'truth': '', 'predicted': 'Line-extensions', 'question': 'What is re-formulations of approved drugs referred to?'}, '57286d4b4b864d19001649dd': {'truth': 'a single vote', 'predicted': 'single', 'question': 'How many votes did the vote of confidence lose by in 1979?'}, '5ad13b7e645df0001a2d1302': {'truth': '', 'predicted': 'Labour government', 'question': 'Who pushed ahead with setting up a Scottish Assembly?'}, '5ad13b7e645df0001a2d1303': {'truth': '', 'predicted': 'Labour', 'question': 'What party withdrew its support for the SNP?'}, '5ad13b7e645df0001a2d1304': {'truth': '', 'predicted': 'a single vote', 'question': \"How much did Callaghan's government win a vote of confidence by?\"}, '5a567d306349e2001acdcdc5': {'truth': '', 'predicted': 'bottom', 'question': 'How many layers of polycarbonate are there?'}, '5735b062dc94161900571f25': {'truth': 'three-star', 'predicted': 'The Garden Hotel, Hotel Ambassador, and Aloha Inn are among the three-star hotels', 'question': 'What type of hotel is Aloha Inn?'}, '5732aeedcc179a14009dac05': {'truth': '35 metres (115 ft)', 'predicted': '', 'question': 'By what height did sea levels rise at the end of the last glacial period?'}, '5732aeedcc179a14009dac06': {'truth': 'Holocene', 'predicted': '', 'question': 'During what period did sea levels rice 115 feet?'}, '5732aeedcc179a14009dac07': {'truth': 'Pleistocene', 'predicted': '', 'question': 'Glaciars from what period depressed the height of northern lands by 591 feet?'}, '5a4ebffaaf0d07001ae8cc2e': {'truth': '', 'predicted': 'about 10,000 years ago', 'question': 'When did the last glacial period begin?'}, '572f21e6b2c2fd1400567f3d': {'truth': 'Donald Wills Douglas, Sr', 'predicted': 'Donald Wills Douglas, Sr.', 'question': 'Who built a plant in Clover field?'}, '572f21e6b2c2fd1400567f3e': {'truth': 'Douglas Aircraft', 'predicted': 'Douglas Aircraft Company', 'question': 'What company was the structure at Clover Field for?'}, '572f21e6b2c2fd1400567f41': {'truth': 'Two', 'predicted': '', 'question': 'How Many Planes returned from the circumnavigation in 1924?'}, '5a2d5455f28ef0001a52648d': {'truth': '', 'predicted': '1960s', 'question': 'In what year was the Douglas Company renamed as McDonnell Douglas?'}, '5a2d5455f28ef0001a52648e': {'truth': '', 'predicted': 'Donald Wills Douglas, Sr.', 'question': 'Who was one of the pilots that flew four Douglas-built planes attempting the first aerial circumnavigation of the world?'}, '56de43294396321400ee2738': {'truth': 'North Korea', 'predicted': '', 'question': 'Which East Asian dictatorship was excluded from the 2011 report?'}, '5ad0cfc9645df0001a2d0482': {'truth': '', 'predicted': 'unavailability of certain crucial data', 'question': 'What is not the main reason that countries were excluded from the 2011 report?'}, '5ad0cfc9645df0001a2d0483': {'truth': '', 'predicted': 'unavailability of certain crucial data', 'question': 'What is not the main reason that countries were excluded from the 2012 report?'}, '5731fe53e17f3d14004225bc': {'truth': 'President Franklin D. Roosevelt', 'predicted': 'Franklin D. Roosevelt', 'question': 'What American president was a member of the Pacific War Council?'}, '56e030597aa994140058e32b': {'truth': '1770', 'predicted': 'about 1770', 'question': 'When did the Island start to experience a period of prosperity?'}, '56e030597aa994140058e32c': {'truth': 'James Cook', 'predicted': 'Captain James Cook', 'question': 'What captain visited the island in 1775 on their trip around the world?'}, '570cf05db3d812140066d349': {'truth': 'in the mouth', 'predicted': 'the mouth', 'question': 'Where does the digestions of some fats start?'}, '570cf05db3d812140066d34b': {'truth': 'produces hormones that stimulate the release of pancreatic lipase from the pancreas and bile from the liver', 'predicted': 'hormones', 'question': 'What happens when there is a presence of fat in the small intestine?'}, '570cf05db3d812140066d34c': {'truth': 'helps in the emulsification of fats for absorption of fatty acids', 'predicted': 'emulsification of fats for absorption of fatty acids', 'question': 'What does bile from the liver help do?'}, '573236d2e17f3d1400422735': {'truth': 'Julian', 'predicted': '', 'question': 'Who rejected the Christian religion?'}, '573236d2e17f3d1400422739': {'truth': 'Christian control', 'predicted': 'Christian', 'question': \"After Julian's death, under to what type of religion did the empire return?\"}, '5ad4105d604f3c001a40019f': {'truth': '', 'predicted': 'Truman', 'question': ' Who was the first President to address the NICP?'}, '5ad4105d604f3c001a4001a0': {'truth': '', 'predicted': '10,000', 'question': ' How many people left the speech that Truman made?'}, '5ad4105d604f3c001a4001a1': {'truth': '', 'predicted': 'the Lincoln Memorial', 'question': \"Where did Truman's historic speech end?\"}, '5ad4105d604f3c001a4001a2': {'truth': '', 'predicted': 'equality of opportunity', 'question': \"During the speech, Truman made the statement that each man shouldn't be guaranteed what?\"}, '5a557754134fea001a0e1ab4': {'truth': '', 'predicted': 'Public Safety Canada', 'question': 'Canadian cyberspace was boosted by who? '}, '5a557754134fea001a0e1ab8': {'truth': '', 'predicted': 'Canadians to be secure online', 'question': 'Who is the security measure meant to help the most? '}, '5a5cfad65e8782001a9d5e37': {'truth': '', 'predicted': 'multiple', 'question': 'How many departments does the new strategy involve?'}, '570629ba52bb891400689917': {'truth': 'compression ratios', 'predicted': '', 'question': 'What can CD parameters be used as references for?'}, '5730b108069b531400832267': {'truth': '3000 years', 'predicted': '3000', 'question': 'How many years ago did migrations of people happen in the Pacific area?'}, '5730b108069b531400832268': {'truth': 'canoe', 'predicted': 'canoe voyaging', 'question': 'By what means did locale people travel between Pacific islands?'}, '5730b108069b53140083226b': {'truth': 'eight standing together', 'predicted': '\"eight standing together\"', 'question': 'What is the native language meaning of Tuvalu?'}, '56cddcae62d2951400fa6914': {'truth': 'fifteen hundred', 'predicted': 'over fifteen hundred extras were hired for the pre-title sequence set in Mexico, though they were duplicated in the film, giving the effect of around ten thousand', 'question': 'How many actual people were used for the opening sequence of Spectre?'}, '56cddcae62d2951400fa6917': {'truth': 'February 2015', 'predicted': '', 'question': 'When was the opening scene of Spectre filmed?'}, '56cf45bcaab44d1400b88ef4': {'truth': 'Austria.', 'predicted': 'Austria', 'question': 'In which country were the scenes with Detlef Bothe shot?'}, '5ad22be1d7d075001a428608': {'truth': '', 'predicted': 'Stephanie Sigman', 'question': 'Alessandro who was cast as Estrella?'}, '5ad22be1d7d075001a428609': {'truth': '', 'predicted': 'Detlef Bothe', 'question': 'Who was cast as a hero for scenes set in Austria?'}, '5ad22be1d7d075001a42860a': {'truth': '', 'predicted': 'pre-title sequence', 'question': 'Over one hundred extras were hired for what?'}, '56d9b546dc89441400fdb714': {'truth': 'respond with aggression', 'predicted': '52%) or respond with aggression', 'question': 'When these feral dogs are approached by a person, they tend to do this 11% of the time?'}, '56d9b546dc89441400fdb715': {'truth': 'pet dogs living in human homes.', 'predicted': 'pet dogs living in human homes', 'question': 'Dog cognition has been studied on what kind of dogs?'}, '5730e777aca1c71400fe5b45': {'truth': 'U.S. War Department', 'predicted': 'The U.S. War Department', 'question': 'Who created the first version of the US Air Force in 1907?'}, '5730e777aca1c71400fe5b46': {'truth': '40 years later', 'predicted': '40 years later. In World War II, almost 68,000 U.S airmen died helping to win the war; only the infantry suffered more enlisted casualties. In practice, the U.S. Army Air Forces (USAAF) was virtually independent of the Army during World War II', 'question': 'When did the US Air Force separate from the War Department? '}, '5730e777aca1c71400fe5b47': {'truth': '68,000', 'predicted': 'almost 68,000 U.S airmen died helping to win the war; only the infantry suffered more enlisted casualties', 'question': 'How many causalities did the US Air Force suffer during WWII? '}, '5727891b708984140094e031': {'truth': 'Kazakh students', 'predicted': '', 'question': 'Who was demonstrating?'}, '5727891b708984140094e033': {'truth': '30,000 to 40,000', 'predicted': '', 'question': 'What were the attendance estimates from non governmental groups?'}, '5726ec2bdd62a815002e955b': {'truth': 'called a conference in Shenyang', 'predicted': \"called a conference in Shenyang to discuss the PVA's logistical problems\", 'question': 'What did Zhou Enlai do as a result of the significant amount of Chinese casualties?'}, '5726ec2bdd62a815002e955d': {'truth': 'These commitments did little to directly address the problems', 'predicted': 'These commitments did little to directly address the problems confronting PVA troops.', 'question': 'Did the actions of the Chinese fix their problems?'}, '56e7860900c9c71400d77233': {'truth': '280,000 registered households', 'predicted': '280,000', 'question': 'During the time of the North–South division, how many households were in Nanjing?'}, '56e7860900c9c71400d77234': {'truth': 'more than 1.4 million residents', 'predicted': '1.4 million', 'question': 'What is the estimated population of Nanjing during that time?'}, '56fb7c108ddada1400cd6457': {'truth': 'Baltic', 'predicted': 'the Baltic', 'question': 'What sea were the Hanseatic cities located on?'}, '56fb7c108ddada1400cd6459': {'truth': 'Marco Polo', 'predicted': 'one of the traders, Marco Polo', 'question': 'Who wrote The Travels of Marco Polo?'}, '57313a17497a881900248c90': {'truth': 'Dzungars', 'predicted': 'the Dzungars', 'question': 'Who did Kangxi fight?'}, '57313a17497a881900248c93': {'truth': 'Russia', 'predicted': '', 'question': 'What European country did Kangxi fight?'}, '572a3b61af94a219006aa8e3': {'truth': 'In 1915', 'predicted': '1915', 'question': 'When did the Russian Caucasus Army advance into eastern Anatolia?'}, '572a3b61af94a219006aa8e4': {'truth': 'ethnic Armenian', 'predicted': 'Armenian', 'question': 'What ethnic group was deported by the Ottoman Government from eastern Anatolia?'}, '572a3b61af94a219006aa8e6': {'truth': 'the Syrian desert', 'predicted': 'Syrian desert', 'question': 'Armenian women and children were deported on death marches through what desert?'}, '572a8395111d821400f38b94': {'truth': 'free', 'predicted': '', 'question': 'How much does it cost to use Metromover?'}, '5ad3984c604f3c001a3fe7eb': {'truth': '', 'predicted': 'Metrorail', 'question': \"What is the name of Miami's light-rail system?\"}, '5ad3984c604f3c001a3fe7ec': {'truth': '', 'predicted': '24.4', 'question': 'How many yards long is Metrorail?'}, '573036ea947a6a140053d2bf': {'truth': 'high-occupancy-vehicle (HOV) \"managed lanes\"', 'predicted': 'high-occupancy-vehicle (HOV) \"managed lanes', 'question': 'What type of special lanes were added to Interstate 15?'}, '5ad4dbaa5b96ef001a10a450': {'truth': '', 'predicted': 'The Merge', 'question': 'What is the name given to the highly congested spot where Interstates 85 and 805 meet?'}, '56dfb0e97aa994140058dfe6': {'truth': 'bullet', 'predicted': '', 'question': 'What was in James Garfield that they wanted to get out?'}, '56dfb0e97aa994140058dfe8': {'truth': 'first', 'predicted': '', 'question': \"On which run did Bell's metal detector give a small indication?\"}, '56dfb0e97aa994140058dfe9': {'truth': 'in tests', 'predicted': 'tests', 'question': \"When did Bell's metal detector work well?\"}, '570a5bbf4103511400d5965a': {'truth': '£329.5 million', 'predicted': '', 'question': 'How much income was generated from research grants and contracts for 2013?'}, '570a5bbf4103511400d5965b': {'truth': 'Funding Council', 'predicted': '', 'question': 'Who granted Imperial close to 170 million pounds in grants?'}, '57313c1205b4da19006bcf05': {'truth': 'the 3rd century BC', 'predicted': '3rd century BC', 'question': 'When was the Greek figure style mostly formed?'}, '57313c1205b4da19006bcf06': {'truth': 'the 4th-century BC', 'predicted': '4th-century BC', 'question': 'When was the famous mosaic \"The Beauty of Durres\" created?'}, '56e141e2e3433e1400422d04': {'truth': 'People of Irish descent', 'predicted': 'Irish descent', 'question': 'What people form the largest ethnic group in the city?'}, '5727d11d3acd2414000ded1a': {'truth': '17 °F', 'predicted': '', 'question': \"What is Oklahoma's record low temperature for Nov 11?\"}, '5727d11d3acd2414000ded1b': {'truth': 'an Arctic cold front', 'predicted': '', 'question': \"What caused Oklahoma's temperature to plummet 66 degrees on Nov 11, 1911?\"}, '5727d11d3acd2414000ded1d': {'truth': 'a day', 'predicted': 'about one tornado per hour over the course of a day', 'question': 'How long did the 1912 tornado outbreak last?'}, '56e06d44231d4119001ac106': {'truth': 'slack or breathy', 'predicted': 'slack or breathy voice', 'question': 'What kind of voice are muddy consonants pronounced with?'}, '5acd2a8c07355d001abf378e': {'truth': '', 'predicted': 'Muddy consonants', 'question': 'What causes a syllable to be pronounced with high pitch or light tone?'}, '5acd2a8c07355d001abf3790': {'truth': '', 'predicted': 'Muddy consonants as initial', 'question': 'What causes a stop to be pronounced with low pitch or light tone?'}, '56ce771daab44d1400b887cf': {'truth': 'hydrogen production from protons', 'predicted': 'hydrogen production', 'question': 'What is a possible alternative to making carbon-based fuels from reduction of carbon dioxide?'}, '5709a2e5200fba1400368202': {'truth': 'hardware-based or assisted computer security', 'predicted': '', 'question': 'What offers an alternative to soft-ware only computer security?'}, '5709a2e5200fba1400368204': {'truth': 'physical access (or sophisticated backdoor access)', 'predicted': 'physical access', 'question': 'What is required in order for hardware to be compromised?'}, '5a55606b134fea001a0e1a66': {'truth': '', 'predicted': 'hardware-based or assisted', 'question': 'What step can increase computer security?'}, '5a55606b134fea001a0e1a67': {'truth': '', 'predicted': 'sophisticated backdoor access', 'question': 'What is used in physical access of a computer?'}, '5a5cd75a5e8782001a9d5dde': {'truth': '', 'predicted': 'hardware-based or assisted computer security', 'question': 'What is an example of software-only computer security?'}, '572713ce708984140094d965': {'truth': 'a positional decimal system on counting boards', 'predicted': 'positional decimal system', 'question': 'What method did early Chinese mathematicians use to calculate?'}, '572713ce708984140094d969': {'truth': 'Cubic equations', 'predicted': '', 'question': 'What advanced mathematical methods did the Tang dynasty have?'}, '56fa2008f34c681400b0bfc7': {'truth': 'diffuse-porous', 'predicted': 'diffuse-porous woods', 'question': 'What kind of woods have pores that are uniformly sized?'}, '5ad31659604f3c001a3fdb4e': {'truth': '', 'predicted': 'scarlet fascias', 'question': 'What do Western Catholic cardinals sometimes line?'}, '5ad31659604f3c001a3fdb4f': {'truth': '', 'predicted': 'scarlet', 'question': 'What color are Western style cassocks made entirely of?'}, '572fddea947a6a140053cd7e': {'truth': 'Ice Hockey Federation', 'predicted': 'International Ice Hockey Federation', 'question': 'What does IIHF stand for?'}, '570b3f0fec8fbc190045b912': {'truth': 'the Sulu Archipelago', 'predicted': 'Sulu Archipelago', 'question': 'Where did anti-terrorist fighting take place?'}, '5ad178b7645df0001a2d1d7b': {'truth': '', 'predicted': 'al-Qaida', 'question': 'Terrorists in the Koreas were linked to what larger terrorist organization?'}, '5ad178b7645df0001a2d1d7c': {'truth': '', 'predicted': 'Sulu Archipelago', 'question': ' Where did terrorist fighting take place?'}, '5ad178b7645df0001a2d1d7d': {'truth': '', 'predicted': '2,000', 'question': ' How many troops did the US initially send to the Koreas?'}, '5ad0185377cf76001a6869b1': {'truth': '', 'predicted': '1264', 'question': 'When did Prince Daniil rule?'}, '56f8c2519e9bad19000a0446': {'truth': 'Sunday', 'predicted': 'Sunday Leagues in the Southampton area are the City of Southampton Sunday Football League and the Southampton and District Sunday Football League', 'question': 'Local Southampton football leagues have what day of the week in their names?'}, '56f8c2519e9bad19000a0448': {'truth': 'City of Southampton Sunday Football League', 'predicted': 'City of Southampton Sunday Football League and the Southampton and District Sunday Football League', 'question': 'What\\'s the name of the local league with \"City\" in its name?'}, '56e136e7cd28a01900c676be': {'truth': 'the Back Bay', 'predicted': 'Back Bay', 'question': 'A train full of gravel came from Needham to fill what?'}, '56e136e7cd28a01900c676bf': {'truth': 'the Back Bay', 'predicted': 'Back Bay', 'question': 'The Boston Public Library is located in what part of Boston?'}, '56e78bb537bdd419002c4108': {'truth': 'admiral Zheng He', 'predicted': 'Zheng He', 'question': 'What Admiral called Nanjing his home?'}, '56e78bb537bdd419002c4109': {'truth': '(Boni 渤泥)', 'predicted': '(Boni 渤泥),', 'question': 'What visiting king died in China in 1408?'}, '56e78bb537bdd419002c410b': {'truth': 'a tortoise stele', 'predicted': 'tortoise', 'question': \"What stele is at Boni's tomb?\"}, '56df6e5156340a1900b29b2c': {'truth': 'Oklahoma Secondary School Activities Association', 'predicted': 'The Oklahoma Secondary School Activities Association', 'question': 'What organization organizes High School football?'}, '5a6b5ebba9e0c9001a4e9f4f': {'truth': '', 'predicted': 'overall outcomes', 'question': 'Supplemental oxygen does not improve what?'}, '5a6b5ebba9e0c9001a4e9f50': {'truth': '', 'predicted': 'thrombolysis', 'question': 'What medicine is used to remove blockages?'}, '5a6b5ebba9e0c9001a4e9f52': {'truth': '', 'predicted': 'heparin', 'question': 'What blood thinner is used in ST elevation treatments?'}, '56cfb1a2234ae51400d9be85': {'truth': 'half million', 'predicted': 'over half million', 'question': 'About how many students attend schools in the City University of New York system?'}, '56cfb1a2234ae51400d9be89': {'truth': '600,000', 'predicted': 'Over 600,000', 'question': 'How many students in New York partcipate in higher education?'}, '5731e07b0fdd8d15006c65e6': {'truth': 'are not recognised by the state', 'predicted': '', 'question': \"What recognition do Baha'i and Hmadi community get from Egyptian government?\"}, '5731e07b0fdd8d15006c65e7': {'truth': '2008', 'predicted': '', 'question': 'Until what year did some minorities need to lie about religion or not get mandatory state issued ID?'}, '570fe7425ab6b819003910b7': {'truth': 'Rhode Island', 'predicted': 'Wisconsin (the only state with only one execution), Rhode Island', 'question': \"What state notably abolished the death penalty and then reintroduced it, but didn't use it again?\"}, '5ad3f120604f3c001a3ff85e': {'truth': '', 'predicted': '1978', 'question': 'In what year did an Oregon referendum succeed in restoring the death penalty, only to be passed due to a court ruling?'}, '5ad3f120604f3c001a3ff85f': {'truth': '', 'predicted': 'Rhode Island', 'question': 'What state notably abolished the death penalty and then reintroduced it, and used it frequently?'}, '57267ab2dd62a815002e868b': {'truth': 'Ant-Man', 'predicted': '', 'question': 'What Marvel hero is related to a tiny insect?'}, '5a5fa27aeae51e001ab14b68': {'truth': '', 'predicted': 'Doctor Doom, Magneto, Galactus, Loki, the Green Goblin, and Doctor Octopus', 'question': \"Who was Daredevil's main antagonist?\"}, '5a5fa27aeae51e001ab14b6b': {'truth': '', 'predicted': 'Marvel Universe', 'question': 'What are the real-life cities New York, Los Angeles, and Chicago called?'}, '5726fd98708984140094d7d2': {'truth': \"Al-Shifa'\", 'predicted': \"Al-Shifa' (Sanatio\", 'question': \"What is the name of Avicenna's larger encyclopedic treatise? \"}, '5726fd98708984140094d7d3': {'truth': 'the Bodleian Library', 'predicted': 'Bodleian Library', 'question': \"Where is Avicenna's Al-Shifa manuscript located?\"}, '5726fd98708984140094d7d4': {'truth': 'the An-najat', 'predicted': 'An-najat', 'question': 'What is the shorter form of Al-Shifa called?'}, '5726fd98708984140094d7d5': {'truth': 'Logic and Metaphysics', 'predicted': '', 'question': \"What two subjects of Avicenna's have been reprinted extensively?\"}, '5aceaec232bba1001ae4b027': {'truth': '', 'predicted': 'Schmoelders', 'question': \"Who published Avicenna's song on logic?\"}, '5aceaec232bba1001ae4b02a': {'truth': '', 'predicted': 'An-najat', 'question': 'What is the longer form of Al-Shifa called?'}, '5726ee62dd62a815002e9583': {'truth': 'Thomas Kuhn', 'predicted': '', 'question': 'Who depicts the history of science in a wider matrix?'}, '5726a7b4dd62a815002e8c24': {'truth': 'The liberal-communitarian debate', 'predicted': 'liberal-communitarian debate', 'question': 'What is often considered valuable for generating a new set of philosophical problems? '}, '5726a7b4dd62a815002e8c25': {'truth': 'greater local control', 'predicted': 'greater local', 'question': 'What type of control do communitarians tend to support?'}, '5726a7b4dd62a815002e8c26': {'truth': 'policies which encourage the growth of social capital', 'predicted': 'greater local control as well as economic and social policies which encourage the growth of social capital', 'question': 'What type of economic and social policies do Communitarians tend to support?'}, '570fa4675ab6b81900390f5d': {'truth': 'it inappropriately measures heterosexuality and homosexuality', 'predicted': '', 'question': 'What was a concern of the Kinsey scale?'}, '570fa4675ab6b81900390f5e': {'truth': 'as independent concepts on a separate scale', 'predicted': 'independent concepts on a separate scale', 'question': 'How are masculinity and femininity more appropriately measured?'}, '570fa4675ab6b81900390f5f': {'truth': 'they act as tradeoffs such, whereby to be more feminine one had to be less masculine and vice versa', 'predicted': '', 'question': 'What happens if the concepts are measured on the same scale?'}, '570fa4675ab6b81900390f60': {'truth': 'the degree of heterosexual and homosexual can be independently determined, rather than the balance between heterosexual and homosexual', 'predicted': 'the degree of heterosexual and homosexual can be independently determined', 'question': 'What is the advantage of measuring these elements separately?'}, '570ffc01a58dae1900cd67ae': {'truth': 'inappropriately measures heterosexuality and homosexuality on the same scale', 'predicted': 'inappropriately measures heterosexuality and homosexuality', 'question': 'What is considered to be a problem with the Kinsey scale?'}, '570ffc01a58dae1900cd67b0': {'truth': 'would allow one to be both very heterosexual and very homosexual or not very much of either.', 'predicted': 'allow one to be both very heterosexual and very homosexual or not very much of either', 'question': 'What would be possible if homesexuality and heterosexuality where measured on different scales?'}, '5726a1d5708984140094cc64': {'truth': 'political', 'predicted': '', 'question': 'The Annual Register covered international events of what type?'}, '5726a1d5708984140094cc65': {'truth': '1766', 'predicted': '', 'question': 'Burke was the only known writer for the Register until what year?'}, '5ad0b591645df0001a2d0108': {'truth': '', 'predicted': '1789', 'question': 'When did Burke become the chief editor?'}, '56f8997b9b226e1400dd0c93': {'truth': 'Deoxyribonucleic acid (DNA)', 'predicted': 'Deoxyribonucleic acid', 'question': 'What was shown to be the molecular repository of genetic information by experiments in the 1940s to 1950s?'}, '57320ba7e99e3014001e647a': {'truth': 'Minerva', 'predicted': 'Jupiter, Juno and Minerva', 'question': 'What goddess became a part of the Capitoline triad?'}, '56f71740711bf01900a44933': {'truth': 'It is possible', 'predicted': 'It is possible, however, for a bilateral treaty to have more than two parties; consider for instance the bilateral treaties between Switzerland and the European Union (EU)', 'question': 'Is it possible for a bilateral treaty to have more than two parties?'}, '56f71740711bf01900a44934': {'truth': 'the European Economic Area agreement', 'predicted': 'European Economic Area agreement', 'question': 'The bilateral treaties between Switzerland and the European Union followed the Swiss rejection of what?'}, '56f71740711bf01900a44935': {'truth': 'it does not', 'predicted': 'it does not establish any rights and obligations', 'question': 'Does the bilateral treaty between Switzerland and the European Union establish rights or obligations amongst the EU and its member states?'}, '56f71740711bf01900a44936': {'truth': 'bilateral', 'predicted': 'bilateral treaties between Switzerland and the European Union (EU) following the Swiss rejection of the European Economic Area agreement', 'question': 'The treaty between Switzerland and the European Union is an example of what kind of treaty?'}, '57278fc7dd62a815002ea071': {'truth': 'familiarize privileged schoolboys with social conditions in deprived areas', 'predicted': 'to familiarize privileged schoolboys with social conditions in deprived areas', 'question': 'What was the purpose for creating an Eton Mission?'}, '57278fc7dd62a815002ea072': {'truth': 'it was decided that a more local project (at Dorney) would be more realistic', 'predicted': '', 'question': 'Why did construction of Eton Mission cease in 1971?'}, '57278fc7dd62a815002ea074': {'truth': 'district of Hackney Wick in east London', 'predicted': 'Hackney Wick', 'question': 'Where was the Eton Mission originally to be located?'}, '56e02a437aa994140058e2de': {'truth': 'as albums', 'predicted': 'albums', 'question': 'How were comics published when serialization became less common?'}, '5acf875d77cf76001a685104': {'truth': '', 'predicted': 'albums', 'question': 'How were comics published when serialization became more common?'}, '5acf875d77cf76001a685105': {'truth': '', 'predicted': \"L'Association\", 'question': 'Which major publisher published in formats that were not traditional?'}, '5acf875d77cf76001a685106': {'truth': '', 'predicted': 'print market', 'question': 'Comics continue to decline regardless of the decrease in what market?'}, '56fb879b8ddada1400cd64d2': {'truth': \"the Peasants' Revolt\", 'predicted': \"Peasants' Revolt\", 'question': 'What English popular revolt took place during this period?'}, '572e8578c246551400ce42ba': {'truth': 'The Dutch Republic, long-time British ally, kept its neutrality intact, fearing the odds against Britain and Prussia fighting the great powers of Europe', 'predicted': 'fearing the odds against Britain and Prussia fighting the great powers of Europe', 'question': 'Why did Denmark-Norway remain neutral rather than assisting its longtime ally, Britain?'}, '572e8578c246551400ce42bb': {'truth': 'the taxation of salt and alcohol begun by Empress Elizabeth in 1759 to complete her addition to the Winter Palace.', 'predicted': 'to complete her addition to the Winter Palace', 'question': 'What did Russian Empress Elizabeth use the proceeds of the tax on salt and alcohol for?'}, '572e8578c246551400ce42bc': {'truth': 'Naples, Sicily, and Savoy, although sided with Franco-Spanish party, declined to join the coalition under the fear of British power', 'predicted': '', 'question': 'Why did Naples remain neutral?'}, '572e8578c246551400ce42bd': {'truth': 'Sicily, and Savoy, although sided with Franco-Spanish party', 'predicted': 'Franco-Spanish party', 'question': 'Who would Sicily and Savoy normally align with?'}, '5ad155de645df0001a2d17e2': {'truth': '', 'predicted': 'Paramount', 'question': 'What company did B. Hal Wallis work at before Universal?'}, '5ad155de645df0001a2d17e3': {'truth': '', 'predicted': 'Universal', 'question': 'What company did B. Hal Wallis work at after Paramount?'}, '5ad155de645df0001a2d17e6': {'truth': '', 'predicted': 'Rooster Cogburn', 'question': 'What 1969 film was a sequel to True Grit?'}, '5726c709f1498d1400e8eb02': {'truth': 'In the 1990s', 'predicted': '1990s', 'question': 'When did Taiwanese Hokkien have a fast change in development?'}, '5a1e1aa53de3f40018b264cb': {'truth': '', 'predicted': 'universities', 'question': 'Who offers poetry or literature courses for training Hokkien-fluent talents?'}, '5a82122f31013a001a3351a5': {'truth': '', 'predicted': 'sounds', 'question': 'What are assigned to phonemes by different languages?'}, '5a82122f31013a001a3351a6': {'truth': '', 'predicted': 'brain', 'question': 'What part of a human does allophone processing?'}, '5a82122f31013a001a3351a7': {'truth': '', 'predicted': 'allophones', 'question': 'The phonetical similarity of what thing causes disagreements between phenomes?'}, '5a82122f31013a001a3351a8': {'truth': '', 'predicted': 'Different linguists', 'question': 'Who takes different approaches to the problem of assigning sounds to allophones?'}, '5a82122f31013a001a3351a9': {'truth': '', 'predicted': 'Different linguists', 'question': 'Who differs in the extent to which they requires phonemes to be phonetically similar?'}, '5acfafea77cf76001a685874': {'truth': '', 'predicted': 'Vladimir Putin', 'question': 'Who escalted some reforms from Yeltsin? '}, '57240e580a492a1900435608': {'truth': 'average of 2,500', 'predicted': '2,500', 'question': 'How many words a day did Victoria write?'}, '57240e580a492a190043560c': {'truth': 'transcribed and edited', 'predicted': '', 'question': 'What did Beatrice do to her mothers diaries after her death?'}, '5725677c69ff041400e58c6a': {'truth': '122', 'predicted': '', 'question': 'How many journals did Queen Victoria write in her lifetime?'}, '5725677c69ff041400e58c6c': {'truth': 'burned the originals', 'predicted': '', 'question': 'What did Beatrice do with the journals after she transcribed and edited them? '}, '5725677c69ff041400e58c6e': {'truth': 'Giles St Aubyn', 'predicted': '', 'question': 'What biographer said Queen Victoria wrote an average of 2500 words a day in her journals?'}, '57267d5c5951b619008f7483': {'truth': 'Victoria wrote an average of 2,500 words a day', 'predicted': 'Victoria wrote an average of 2,500 words a day during her adult life', 'question': 'How avid of a writer was the Queen?'}, '57267d5c5951b619008f7484': {'truth': '122 volumes', 'predicted': '122', 'question': 'How many volumes did her journal span?'}, '57267d5c5951b619008f7485': {'truth': 'her youngest daughter, Princess Beatrice', 'predicted': '', 'question': \"Who was Victoria's literary executer?\"}, '57267d5c5951b619008f7486': {'truth': 'transcribed and edited', 'predicted': '', 'question': \"What did Beatrice do with her mother's journals?\"}, '57267d5c5951b619008f7487': {'truth': 'burned', 'predicted': '', 'question': \"What did Beatrice do with the origional volumes of her mother's diaries?\"}, '5ad17f1f645df0001a2d1e4a': {'truth': '', 'predicted': '2,500', 'question': 'How many words a day did Victoria read?'}, '5ad17f1f645df0001a2d1e4d': {'truth': '', 'predicted': 'Princess Beatrice', 'question': 'Who was Victorias oldest daughter?'}, '5726d4705951b619008f7f56': {'truth': 'World Bank and the International Monetary Fund', 'predicted': 'the World Bank and the International Monetary Fund', 'question': 'Mali signed agreements with what to parties that began their economic changes?'}, '5726d4705951b619008f7f58': {'truth': 'sixteen enterprises', 'predicted': '', 'question': 'How many enterprises have been made completely private since the agreement?'}, '5728c7dd2ca10214002da7b0': {'truth': 'Third Plague Pandemic', 'predicted': 'The Third Plague Pandemic', 'question': 'What killed 10 million people in India?'}, '5730af4a069b53140083224b': {'truth': 'original sin only', 'predicted': '', 'question': 'What is the Immaculate Conception a representation of the avoidance of ?'}, '5730af4a069b53140083224c': {'truth': 'Mary was preserved from any stain (in Latin, macula or labes', 'predicted': '', 'question': 'What was Mary prevented from having to endure ?'}, '5730af4a069b53140083224d': {'truth': 'privilege granted by Almighty God, in view of the merits of Jesus Christ, the Saviour of the human race, was preserved free from all stain', 'predicted': '', 'question': 'Who was believed to have prevented this from occurring to Mary ?'}, '5730af4a069b53140083224e': {'truth': 'Jesus Christ, the Saviour of the human race, was preserved free from all stain of original sin.\"', 'predicted': '', 'question': 'What was the outcome of preventing Mary from having to endure such an injustice ?'}, '5730af4a069b53140083224f': {'truth': 'sanctifying grace that would normally come with baptism after birth.', 'predicted': '', 'question': 'What normally followed the delivery of a child by a woman in Mary time period ?'}, '5a2716bbc93d92001a4003cd': {'truth': '', 'predicted': 'sanctifying grace', 'question': 'What did Mary receive at baptism?'}, '572a4e22fed8de19000d5b89': {'truth': 'an increase in the active secretion, or there is an inhibition of absorption', 'predicted': 'increase in the active secretion', 'question': 'What is secretory diarrhea?'}, '572a4e22fed8de19000d5b8a': {'truth': 'cause of this type of diarrhea is a cholera toxin that stimulates the secretion of anions, especially chloride ions', 'predicted': 'cholera toxin', 'question': 'What are the causes of secretory diarrhea?'}, '572a4e22fed8de19000d5b8b': {'truth': '. There is little to no structural damage.', 'predicted': 'little to no', 'question': 'Is there any structural damage associated with secretory diarrhea?'}, '5a0dfd6ed7c85000188644b7': {'truth': '', 'predicted': 'little to no', 'question': 'Is there any structural damage associated with oral food intake?'}, '570b27b4ec8fbc190045b894': {'truth': 'Game Room', 'predicted': 'Game Room to Xbox Live', 'question': 'What is the name of the online virtual arcade that launched in 2010?'}, '57284af44b864d19001648d0': {'truth': 'Ilyas Kashmiri', 'predicted': '', 'question': 'Who did a US drone kill in Sep 2009?'}, '57284af44b864d19001648d2': {'truth': 'Kashmiri militants', 'predicted': '', 'question': 'After 2009, who began fighting in Waziristan?'}, '57284af44b864d19001648d3': {'truth': 'Hizbul Mujahideen', 'predicted': 'Hizbul', 'question': 'What group did Al-Badar Mujahideen break away from?'}, '5a85e99bb4e223001a8e72d9': {'truth': '', 'predicted': '8 July 2012', 'question': 'When did terror groups call to end the jihad in Kashmir?'}, '5728be814b864d1900164d3f': {'truth': 'The Baltic Council', 'predicted': '', 'question': 'What is the combined group of the interparliamentary Baltic Assembly and the intergovernmental Baltic Council of Ministers?'}, '572805304b864d1900164253': {'truth': 'Pope Pius IX', 'predicted': 'Pius IX', 'question': 'Who did John XXIII wish to see canonized?'}, '5a611100e9e1cc001a33ce89': {'truth': '', 'predicted': '1959', 'question': 'When was Pope Pius IX at a spiritual retreat?'}, '5a611100e9e1cc001a33ce8a': {'truth': '', 'predicted': 'holy and glorious memory', 'question': 'When thinking of John XXIII, what did Pope Pius IX want to be worthy of?'}, '5a611100e9e1cc001a33ce8b': {'truth': '', 'predicted': 'holy and glorious memory', 'question': 'How did Pope Pius IX remember John XXIII as being?'}, '56fdfbee19033b140034ce25': {'truth': 'more slowly,', 'predicted': 'more slowly', 'question': 'Multitasking would seemingly cause a computer to run in what fashion?'}, '56fdfbee19033b140034ce26': {'truth': 'input/output devices', 'predicted': 'slow input/output devices to complete their tasks', 'question': 'What do a lot of programs spend time waiting for?'}, '572fefcb947a6a140053ce2e': {'truth': '101 BC', 'predicted': '113–101 BC', 'question': 'When did the Cimbrian War end?'}, '572fefcb947a6a140053ce30': {'truth': 'The Cimbrian War', 'predicted': 'Cimbrian War', 'question': 'What war began in the year 113 BC?'}, '57288f642ca10214002da46f': {'truth': '45 percent of the student body at BYU has been missionaries for LDS Church', 'predicted': 'Over three quarters of the student body has some proficiency in a second language (numbering 107 languages in total). This is partially due to the fact that 45 percent of the student body at BYU has been missionaries for LDS Church', 'question': \"What can be attributed to BYU's high percentage of second language proficient students?\"}, '57288f642ca10214002da471': {'truth': 'largest of their kind in the nation', 'predicted': '', 'question': \"What designation does BYU's Russian language program hold?\"}, '5acea68932bba1001ae4aef0': {'truth': '', 'predicted': 'a second language', 'question': 'Over one-third of the student body has some proficiency in what?'}, '5acea68932bba1001ae4aef1': {'truth': '', 'predicted': 'foreign language classes', 'question': 'What are three quarters of students enrolled in during any given semester?'}, '5706b4af0eeca41400aa0d5c': {'truth': '\"No UFOs\"', 'predicted': 'Model 500 \"No UFOs\"', 'question': 'what hit single did atkins release in 1985?'}, '5706b4af0eeca41400aa0d5d': {'truth': '\"Strings of Life\"', 'predicted': '\"Strings of Life', 'question': 'what unusual single did derrick may release, featuring a darker strain of house?'}, '5706b4af0eeca41400aa0d5e': {'truth': 'Techno-Scratch', 'predicted': '', 'question': 'what did knights of the turntable release in 1984?'}, '5ad28d6fd7d075001a429a2a': {'truth': '', 'predicted': 'Cybotron', 'question': 'May was a former member of what music group?'}, '5ad28d6fd7d075001a429a2b': {'truth': '', 'predicted': 'Model 500 \"No UFOs\"', 'question': 'What hit single did May release in 1985?'}, '5ad28d6fd7d075001a429a2c': {'truth': '', 'predicted': '\"Strings of Life', 'question': 'What unusual single did Derrick Atkins release, featuring a darker strain of house?'}, '5ad28d6fd7d075001a429a2e': {'truth': '', 'predicted': 'Tony Wilson', 'question': 'Who was the manager of the factory nightclub and co-owner of The Hummingbird?'}, '57267786dd62a815002e85f4': {'truth': 'two days', 'predicted': 'two', 'question': 'How many days a week does the city encourage people to go without a car?'}, '57267786dd62a815002e85f6': {'truth': 'biannual', 'predicted': '', 'question': 'How often do citizens need to get their cars examined?'}, '572814c34b864d190016441e': {'truth': 'Ratchet & Clank Future: Tools of Destruction', 'predicted': '', 'question': 'Which Ratchet & Clank title debuted at E3 2007?'}, '572814c34b864d190016441f': {'truth': '2007', 'predicted': '', 'question': 'What year was Warhawk released for the PlayStation 3?'}, '572814c34b864d1900164421': {'truth': 'Gran Turismo 5 Prologue', 'predicted': '', 'question': 'What Gran Turismo game was shown in 2007 but not released until after 2007?'}, '5ad2a7a2d7d075001a429e12': {'truth': '', 'predicted': \"Drake's Fortune\", 'question': 'Which Uncharted title debuted at E3 2008?'}, '5ad33f45604f3c001a3fdbc5': {'truth': '', 'predicted': 'Metal Gear Solid 4: Guns of the Patriots', 'question': 'Which much anticipated third-party game with the name of a month of the year in it did Sony show at E4 2007?'}, '57303815947a6a140053d2c6': {'truth': '1960s', 'predicted': 'The 1960s', 'question': 'What decade was significant to Iranian film?'}, '57303815947a6a140053d2c8': {'truth': '25 commercial films', 'predicted': '25', 'question': 'How many commercial films were produced yearly on average in the early 1960s in Iran?'}, '56e0758e7aa994140058e503': {'truth': 'hydrogen chloride and hydrogen fluoride', 'predicted': 'hydrogen halides, hydrogen chloride and hydrogen fluoride', 'question': 'What are two other dangerous acids?'}, '56e0758e7aa994140058e505': {'truth': 'room temperature', 'predicted': '', 'question': 'What temperature does hydrogen react with these elements?'}, '56cf7f014df3c31400b0d85b': {'truth': 'Obama', 'predicted': 'President Obama', 'question': 'What president did Kanye comment on as having trouble pushing policies while in office?'}, '56cf7f014df3c31400b0d85c': {'truth': 'Jewish people', 'predicted': 'Jewish', 'question': 'What type of people did Kanye state had more power than Black people?'}, '56d126e117492d1400aaba96': {'truth': 'November 26, 2013', 'predicted': 'November 26', 'question': 'On what day did Kanye do an interview about President Obama pushing policies in Washington?'}, '570e08690dc6ce1900204d9a': {'truth': 'Antarctic krill', 'predicted': '', 'question': 'What is a very important species in the Southern Ocean?'}, '5725cc5938643c19005acd27': {'truth': 'the government of Montevideo', 'predicted': 'government of Montevideo', 'question': 'Who owns The Solis Theater?'}, '5731bb10b9d445190005e4d0': {'truth': 'fervently Catholic', 'predicted': 'Catholic', 'question': \"What was the Duke of York's relationship to his religion described as being?\"}, '5ad141b5645df0001a2d13fb': {'truth': '', 'predicted': 'no religious test', 'question': 'What did the Province of West Jersey specify there would be for those running for an office, in 1681?'}, '5ad141b5645df0001a2d13fc': {'truth': '', 'predicted': '1799', 'question': 'When was an oath requiring militia to abjure the pretensions of the pope not replaced?'}, '572749fdf1498d1400e8f5ab': {'truth': 'the cotton gin', 'predicted': 'cotton gin', 'question': 'What invention, for which he is primarily known, did Eli Whitney develop in New Haven?'}, '572749fdf1498d1400e8f5ad': {'truth': '\"The Arsenal of America\"', 'predicted': 'The Arsenal of America', 'question': 'What was the nickname given to Connecticut due to the large number of arms manufacturers that arose in the state? '}, '572948091d0469140077924b': {'truth': 'A. C. Gilbert', 'predicted': 'A. C. Gilbert Company', 'question': 'What was the company that was responsible Connecticut rise as a manufacturing economy? '}, '572a329aaf94a219006aa886': {'truth': 'hunter-gatherer communities', 'predicted': '', 'question': 'What type of societies were not affected by famine?'}, '5a7d388a70df9f001a875017': {'truth': '', 'predicted': 'drought or pests', 'question': 'What were the causes of cultivation in early farm towns?'}, '5a7d388a70df9f001a875019': {'truth': '', 'predicted': 'agrarian communities', 'question': 'What type of societies were usually still successful after dealing with cultivation?'}, '5a7d388a70df9f001a87501b': {'truth': '', 'predicted': 'particularly acute', 'question': 'What could sensitivity to cultivation be?'}, '57098140200fba14003680cc': {'truth': 'copper toxicity', 'predicted': 'acute copper toxicity', 'question': 'When ingested in large amounts what does copper salts produce in humans?'}, '57098140200fba14003680ce': {'truth': 'growth rates', 'predicted': 'feed conversion efficiency, growth rates, and carcass dressing percentages', 'question': 'What is a major benefit to rabbits having a higher concentration of copper in their diet?'}, '5a8371c6e60761001a2eb71c': {'truth': '', 'predicted': 'acute copper toxicity', 'question': 'When ingested in small amounts what does copper salts produce in humans?'}, '572771b85951b619008f8a07': {'truth': 'tetraploid cotton', 'predicted': '', 'question': 'What is the final sequencing goal of sequencing diploid cotton genomes first ?'}, '572771b85951b619008f8a0a': {'truth': 'diploid', 'predicted': '', 'question': 'What type of genome must be sequenced first to prevent confusion before the tetraploid form?'}, '5a81a8b031013a001a334d35': {'truth': '', 'predicted': 'retrotransposons', 'question': 'In order to understand the tetraploid forms, what must be used as a comparison in cotton GORGE?'}, '57260e5b271a42140099d406': {'truth': 'Sophists', 'predicted': 'The Sophists', 'question': 'Who declared the centrality of humanity and agnosticism?'}, '57260e5b271a42140099d408': {'truth': 'Epicurus', 'predicted': '', 'question': 'Who thought that gods were distant and uninterested?'}, '57260e5b271a42140099d409': {'truth': 'down to earth', 'predicted': 'earth', 'question': 'Rulers brought the concept of divinity to where?'}, '5727ec41ff5b5019007d988e': {'truth': 'there was no industry standard', 'predicted': '', 'question': 'What was the industry standard in Europe for record equalization?'}, '5727ec41ff5b5019007d988f': {'truth': 'US', 'predicted': '', 'question': 'In which country was the treble roll off greater?'}, '5727ec41ff5b5019007d9891': {'truth': 'there was no industry standard', 'predicted': '', 'question': 'What was the industry standard on equalization practices?'}, '573036c4947a6a140053d2b2': {'truth': 'traveling wave antennas', 'predicted': 'traveling wave', 'question': \"What antenna's are nonresonant?\"}, '573036c4947a6a140053d2b4': {'truth': 'helical antenna', 'predicted': '', 'question': 'What antenna does not have linear polarization?'}, '573036c4947a6a140053d2b5': {'truth': 'resistor', 'predicted': '', 'question': 'What are undirectional traveling wave directions terminated by?'}, '573036c4947a6a140053d2b6': {'truth': \"antenna's characteristic resistance\", 'predicted': '', 'question': 'What is the resistor equal to?'}, '57283ac5ff5b5019007d9f8e': {'truth': 'Democratic-Republicans', 'predicted': '', 'question': 'What was the federalist party of the United States opposed to?'}, '57283ac5ff5b5019007d9f8f': {'truth': 'the Legislature had too much power (mainly because of the Necessary and Proper Clause) and that they were unchecked', 'predicted': '', 'question': 'What did the democratic-republican party believe in?'}, '57283ac5ff5b5019007d9f90': {'truth': 'the judicial system of courts', 'predicted': 'judicial system of courts', 'question': 'Who decided the rights in specific cases?'}, '5acfb20177cf76001a685908': {'truth': '', 'predicted': 'it was impossible to list all the rights', 'question': 'What was the federalist party of the United States in support of?'}, '5acfb20177cf76001a68590c': {'truth': '', 'predicted': 'judicial system of courts', 'question': 'Who decided the rights in all cases?'}, '5725cedc38643c19005acd58': {'truth': 'revolutionary', 'predicted': 'revolutionary tactics and training', 'question': \"What was Chapman's style of tactics that provided the basis for the Arsenal club's success?\"}, '5725cedc38643c19005acd5a': {'truth': 'Chapman', 'predicted': '', 'question': 'What football club manager got an underground station renamed for Arsenal?'}, '5acd005507355d001abf3158': {'truth': '', 'predicted': 'Herbert Chapman', 'question': \"What was the name of one of Arsenal's coaches prior to 1925?\"}, '5acd005507355d001abf315b': {'truth': '', 'predicted': 'Herbert Chapman', 'question': 'Who named the team Arsenal?'}, '5728d7c4ff5b5019007da7f4': {'truth': 'about $3,600', 'predicted': '$3,600', 'question': 'How much was the average cost of hospital stays for asthma-related issues for children??'}, '5728d7c4ff5b5019007da7f5': {'truth': 'from $5,200 to $6,600', 'predicted': '$5,200 to $6,600', 'question': 'How much was the average cost of hospital stays for asthma-related issues for adults?'}, '56dcf8b79a695914005b94a6': {'truth': 'multi-party', 'predicted': 'multi-party political system', 'question': \"What kind of political system has existed in Congo-Brazzaville since the '90s?\"}, '5ad00faf77cf76001a686846': {'truth': '', 'predicted': 'the early 1990s', 'question': 'Since when has Congo-Brazzaville has a single party political system?'}, '5ad00faf77cf76001a686848': {'truth': '', 'predicted': 'Parti Congolais du Travail', 'question': 'What is the Congolese Labour Party called in German?'}, '57265bdaf1498d1400e8dd1c': {'truth': 'English descent and Americans of Scots-Irish descent began moving into northern Florida from the backwoods of Georgia and South Carolina', 'predicted': 'Georgia and South Carolina', 'question': 'Where did English and Scotch Irish descent move to Florida from '}, '57265bdaf1498d1400e8dd1d': {'truth': 'Florida Crackers', 'predicted': '', 'question': 'Backwoods settlers of Northern Florida are known as '}, '57265bdaf1498d1400e8dd1e': {'truth': 'Spanish were never able to effectively police the border region and the backwoods settlers', 'predicted': '', 'question': 'Were the Spanish able to police the backwoods settlements '}, '572a229a3f37b31900478721': {'truth': 'the plague and famine', 'predicted': '', 'question': \"What wiped out one third of East Prussia's population during the early 1700's?\"}, '572a229a3f37b31900478722': {'truth': 'speakers of Old Prussian', 'predicted': 'Old Prussian', 'question': \"What was lost in Prussia's history during the Plague?\"}, '572a229a3f37b31900478723': {'truth': 'Russian troops', 'predicted': 'Imperial Russian troops', 'question': 'What military overran much of East Prussia?'}, '5728283a2ca10214002d9f71': {'truth': 'ridges whose tips bear hooked chetae', 'predicted': '', 'question': 'What kind of parapodia do burrowing annelids often have?'}, '570cff2cb3d812140066d394': {'truth': 'until', 'predicted': 'until) denotes a state up to a point', 'question': 'What is the meaning of the Greek word \"heos?\"'}, '570cff2cb3d812140066d395': {'truth': '1:23', 'predicted': '', 'question': \"Which verse in Matthew is believed to refer to Isaiah's prohecy of the Virgin Mary?\"}, '570cff2cb3d812140066d397': {'truth': 'virgin', 'predicted': '', 'question': 'What is the English tranlation of the Greek word \"parthenos?\"'}, '5ad18f44645df0001a2d1f4e': {'truth': '', 'predicted': 'Joseph', 'question': 'Who knew Mary before she brought forth her first born son?'}, '5ad18f44645df0001a2d1f50': {'truth': '', 'predicted': 'Galilee', 'question': 'Where did Matthew and Luke gather with the disbelieving Jews?'}, '5ad18f44645df0001a2d1f52': {'truth': '', 'predicted': 'Hebrew', 'question': 'What language does Biblical scholar Bart Ehrmam primarily speak?'}, '5acd809c07355d001abf449d': {'truth': '', 'predicted': 'excess demand', 'question': 'What is the failure attributed to?'}, '57062c2552bb89140068992c': {'truth': '1000 MB', 'predicted': '', 'question': 'What was the average high end hard drive size?'}, '56e0a41f7aa994140058e68d': {'truth': 'the Russian SFSR', 'predicted': 'Russian SFSR', 'question': 'What administrative division did Kaliningrad Oblast become a part of?'}, '5ace063c32bba1001ae49996': {'truth': '', 'predicted': 'April 17, 1946', 'question': 'On what date was Kaliningrad Ozarks annexed?'}, '5ace063c32bba1001ae49997': {'truth': '', 'predicted': 'East Prussia', 'question': 'What state of Germany did Kaliningrad Oblast form a part of?'}, '5ace063c32bba1001ae49999': {'truth': '', 'predicted': 'East Prussia', 'question': 'What administrative division did Kaliningrad Oblast leave?'}, '5726cc10f1498d1400e8eb80': {'truth': 'precision and speed limitations', 'predicted': '', 'question': 'Why were stepper motors abandoned in computer drive designs?'}, '5726cc10f1498d1400e8eb81': {'truth': 'voice coil-based head actuator systems', 'predicted': '', 'question': 'What do newer computer drives use instead of stepper motors?'}, '5726cc10f1498d1400e8eb83': {'truth': 'the structure in a typical (cone type) loudspeaker', 'predicted': 'loudspeaker', 'question': 'To what device does the term, \"voice coil\" historically refer?'}, '56e8379037bdd419002c44aa': {'truth': 'the south', 'predicted': 'south', 'question': 'Hakka is a language from what geographic part of China?'}, '5ad27b63d7d075001a429652': {'truth': '', 'predicted': 'alphabets', 'question': 'What do Chinese characters use to give hints to pronunciation?'}, '5ad27b63d7d075001a429654': {'truth': '', 'predicted': 'Hong Kong', 'question': 'Where is Macau the most commonly used language?'}, '5ad27b63d7d075001a429655': {'truth': '', 'predicted': 'Xiang, Wu, Gan, Min, Yue', 'question': 'Which language is the most commonly used in Cantonese?'}, '5ad27b63d7d075001a429656': {'truth': '', 'predicted': 'alphabets', 'question': 'What do most languages use to indicate varieties?'}, '57264be9dd62a815002e80bc': {'truth': 'Commercial turkeys are usually reared indoors under controlled conditions', 'predicted': 'indoors under controlled conditions', 'question': 'What type of accomidations are domesticated turkey normally grown in?'}, '57264be9dd62a815002e80be': {'truth': 'Females achieve slaughter weight at about 15 weeks of age and males at about 19', 'predicted': '', 'question': 'At what age is the average turkey considered ready for the initial step of the commercial food process?'}, '57264be9dd62a815002e80bf': {'truth': 'Mature commercial birds may be twice as heavy as their wild counterparts', 'predicted': 'twice as heavy', 'question': 'How much more does a average commercial  turkey weigh in comparison to its wild turkey cousins ?'}, '57264be9dd62a815002e80c0': {'truth': '60 million birds in the United States', 'predicted': '60 million', 'question': 'What the average for the amount of  turkeys are consumed in the U.S on Thanksgiving Day?'}, '5a860699b4e223001a8e73c2': {'truth': '', 'predicted': '19', 'question': 'What age is the moose considered ready for the initial step of the commercial food process?'}, '5a860699b4e223001a8e73c3': {'truth': '', 'predicted': '60 million', 'question': 'What is the average for the amount of parrots that are consumed in the U.S on Thanksgiving Day?'}, '5a860699b4e223001a8e73c4': {'truth': '', 'predicted': 'controlled', 'question': 'What type of conditions are used to decrease the weight and profitability of commercial turkeys?'}, '5a860699b4e223001a8e73c5': {'truth': '', 'predicted': 'twice as heavy', 'question': 'How much more does a average commercial feather weigh in comparison to its wild turkey cousins ?'}, '57283e8bff5b5019007d9fe3': {'truth': 'Alamogordo Army Airfield,', 'predicted': 'Alamogordo Army Airfield', 'question': 'Where did the first atomic blast test take place?'}, '5726fad05951b619008f840a': {'truth': 'MTV', 'predicted': 'MTV and music videos', 'question': 'Madonna used which TV company to help with her career?'}, '572e7f8003f98919007566dc': {'truth': 'Republic of Venice', 'predicted': 'the Republic of Venice', 'question': 'Who assumed control of the island after the death of James II?'}, '57270821708984140094d8d2': {'truth': 'to promote nutrition literacy', 'predicted': 'nutrition literacy', 'question': 'What is the goal of Smart Bodies?'}, '570ff9cda58dae1900cd679d': {'truth': '6', 'predicted': '6 percent', 'question': 'About what percentage of capital convictions are overturned due to state collateral review?'}, '570ff9cda58dae1900cd679f': {'truth': 'ineffective assistance of counsel', 'predicted': '', 'question': 'What is an example of an issue that is raised in collateral review?'}, '56de0abc4396321400ee2563': {'truth': 'Islam', 'predicted': 'Islamic Conquest of Persia', 'question': 'The proliferation of which religion had a profound effect on the development of Iranian languages?'}, '56de0abc4396321400ee2565': {'truth': 'Saffarid', 'predicted': 'Saffarid dynasty', 'question': 'What was the first dynasty to use Dari?'}, '56de0abc4396321400ee2567': {'truth': 'Khorasan', 'predicted': 'the eastern province of Khorasan', 'question': 'What area was the name Dari connected to by medieval Iranian thinkers?'}, '5a18e9499aa02b0018605f27': {'truth': '', 'predicted': 'Dari', 'question': \"What language do middle Iranian displace is the court's official language?\"}, '5a18e9499aa02b0018605f2a': {'truth': '', 'predicted': 'Khuzi\"', 'question': 'What was the official dialect of the royalty itself?'}, '5726c7305951b619008f7dd5': {'truth': 'for a financial bailout from the federal government', 'predicted': '', 'question': \"What reason did David Buffett give for Norfolk Island surrendering its' tax-free status?\"}, '5726c7305951b619008f7dd7': {'truth': 'social', 'predicted': 'social services', 'question': 'What type of services were Norfolk Island inhabitants unable to receive prior to this announcement?'}, '5a81c94f31013a001a334eb7': {'truth': '', 'predicted': 'social services', 'question': 'What type of services were Norfolk Island inhabitants unable to receive after this announcement?'}, '5728f9a14b864d190016515c': {'truth': 'December 2014, Myanmar signed an agreement to set up its first stock exchange', 'predicted': 'its first stock exchange', 'question': 'What occurred in the winter of 2014 of significance for Myanmar ?'}, '5728f9a14b864d1900165160': {'truth': 'Yangon Stock Exchange (YSX) officially opened for business on Friday, March 25, 2016.', 'predicted': 'March 25, 2016', 'question': 'What day did the business that first rang a bell to begin in the winter of 2014 in Myanmar open its doors to customers?'}, '56cd796762d2951400fa65ed': {'truth': 'Peter Oppenheimer', 'predicted': '', 'question': 'Who was Chief Financial Officer of Apple in July of 2009?'}, '56dfb914231d4119001abd05': {'truth': 'lodging', 'predicted': 'lodging customers', 'question': 'What is the main service of an inn, now also attainable in motels, hotels and lodges?'}, '572904223f37b31900477f86': {'truth': 'Homo habilis', 'predicted': '', 'question': 'Members of what species populated parts of Africa in a relatively short time?'}, '56dddab666d3e219004dad2f': {'truth': 'the House of Burgundy and subsequently the House of Habsburg', 'predicted': 'House of Burgundy', 'question': 'The majority of the Low Countries were ruled by which houses?'}, '56dddab666d3e219004dad32': {'truth': \"high taxes, persecution of Protestants by the government, and Philip's efforts to modernize and centralize the devolved-medieval government structures of the provinces\", 'predicted': 'high taxes', 'question': 'Why did the people of the Netherlands rise up against Philip II?'}, '5a11c08c06e79900185c3547': {'truth': '', 'predicted': 'House of Burgundy', 'question': 'The low countries ruled most of what two houses?'}, '5a11c08c06e79900185c354a': {'truth': '', 'predicted': '1568', 'question': 'When did Philip II conquer the Netherlands?'}, '5a1c89fcb4fb5d001871468c': {'truth': '', 'predicted': '1549', 'question': 'When did the House of Burgundy issue the Pragmatic Sanction?'}, '5a1c89fcb4fb5d001871468d': {'truth': '', 'predicted': 'centralize the devolved-medieval government structures of the provinces', 'question': 'What did modernizing the government structures do to the Low Countries status?'}, '5a1c89fcb4fb5d001871468e': {'truth': '', 'predicted': '1568', 'question': 'When did the House of Burgundy revolt against Philip II?'}, '5a1c89fcb4fb5d001871468f': {'truth': '', 'predicted': 'high taxes', 'question': 'Name two reasons why the Low Countries turned against Philip II?'}, '5a1c89fcb4fb5d0018714690': {'truth': '', 'predicted': \"the Eighty Years' War\", 'question': 'What event started after the revolt let by King Philip II of Spain?'}, '5726dcbf708984140094d3fb': {'truth': 'Goodluck Jonathan', 'predicted': 'Jonathan', 'question': 'Who won the 2011 election?'}, '5ace989e32bba1001ae4abe3': {'truth': '', 'predicted': 'Judaism', 'question': 'What religion was it legal to convert to in the Roman world from the mid-2nd century?'}, '5ace989e32bba1001ae4abe4': {'truth': '', 'predicted': 'the Roman world', 'question': 'Where was it legal to convert to Judaism?'}, '5ace989e32bba1001ae4abe5': {'truth': '', 'predicted': 'halakhic requirement of circumcision', 'question': 'What made conversion easy in the Roman world?'}, '5ace989e32bba1001ae4abe6': {'truth': '', 'predicted': '96 CE', 'question': 'When was the Fiscus Judaicus expanded to also include Christians?'}, '5acd416807355d001abf3aa2': {'truth': '', 'predicted': 'Earthquakes', 'question': 'What releases stored elastic kinetic energy in rocks?'}, '5acd416807355d001abf3aa3': {'truth': '', 'predicted': 'heat', 'question': 'What does radioactive decay of atoms in the core of Mars release?'}, '572a4f507a1753140016ae93': {'truth': 'eliminating all remnants of German history', 'predicted': '', 'question': 'In the Soviet section to the north, what did they want to expel from their land?'}, '572a4f507a1753140016ae94': {'truth': 'names were replaced by new Russian names', 'predicted': '', 'question': 'What else happened in the northern part of East Prussia in the now Russian area?'}, '5a3c02eacc5d22001a521d25': {'truth': '', 'predicted': '1967', 'question': 'In what year was the new \"House of the Soviets\" completed?'}, '57342937d058e614000b6a66': {'truth': 'a potato-based stew that can be made from several types of fish', 'predicted': 'a potato-based stew', 'question': 'What is caldeirada?'}, '57342937d058e614000b6a68': {'truth': 'arroz de sarrabulho (rice stewed in pigs blood) or the arroz de cabidela (rice and chickens meat stewed in chickens blood)', 'predicted': 'arroz de sarrabulho', 'question': 'What are two popular Northern Portugal dishes?'}, '57301519b2c2fd1400568829': {'truth': 'current', 'predicted': 'current-carrying capacity as well as resistance to thermal strains', 'question': 'What will a circuit board with heavy copper carry very well?'}, '5ace96bf32bba1001ae4ab4f': {'truth': '', 'predicted': 'IPC 2152', 'question': 'What is the standard for determining current-carrying capacity of printed circuit board trees?'}, '5ace96bf32bba1001ae4ab50': {'truth': '', 'predicted': 'current-carrying capacity of printed circuit board traces', 'question': 'IAC 2152 is the standard for what?'}, '57338497d058e614000b5c4c': {'truth': 'a series of regulatory proposals', 'predicted': '', 'question': 'What was introduced by President Barack Obama in June 2009?'}, '57338497d058e614000b5c4d': {'truth': 'consumer protection', 'predicted': 'consumer protection, executive pay, bank financial cushions or capital requirements', 'question': 'What was one of the items important to consumers that was addressed by the new regulatory proposals introduced in June 2009?'}, '57338497d058e614000b5c4e': {'truth': 'proprietary', 'predicted': 'proprietary trading', 'question': 'Regulations were proposed by Obama in January 2010 to limit the ability of banks to engage in which type trading?'}, '57277a72708984140094deb1': {'truth': 'nationalist movements and ethnic disputes', 'predicted': 'fostered nationalist movements and ethnic disputes within the Soviet Union. It also led indirectly to the revolutions of 1989, in which Soviet-imposed communist regimes of the Warsaw Pact were peacefully toppled (Romania excepted), which in turn increased pressure on Gorbachev to introduce greater democracy and autonomy', 'question': 'What were some of the downsides of the more liberal Soviet Union?'}, '57277a72708984140094deb3': {'truth': 'Communist Party', 'predicted': 'Communist Party of the Soviet Union', 'question': 'What party did Gorbachev belong to?'}, '57277a72708984140094deb5': {'truth': '1990', 'predicted': '', 'question': 'When were opposition parties first allowed in the Soviet Union?'}, '57255c8a69ff041400e58c38': {'truth': 'boosted after the establishment of the Third French Republic', 'predicted': 'after the establishment of the Third French Republic', 'question': 'How was the republican sentiment in Britain changed?'}, '57255c8a69ff041400e58c3a': {'truth': 'new antiseptic carbolic acid spray', 'predicted': 'antiseptic carbolic acid spray', 'question': \"What cutting edge treatment did Joseph Lister use to treat Queen Victoria's illness?\"}, '57267189708984140094c640': {'truth': 'establishment of the Third French Republic', 'predicted': 'the Third French Republic', 'question': 'What helped to boost the rebublicians in 1870?'}, '57267f57f1498d1400e8e1ce': {'truth': \"the tenth anniversary of her husband's death\", 'predicted': '', 'question': \"After what event did the Prince of Wales' health begin to improve?\"}, '5ad17959645df0001a2d1db5': {'truth': '', 'predicted': 'Trafalgar Square', 'question': 'Where was the republica rally held that called for Vuictorias promotion?'}, '5ad17959645df0001a2d1db6': {'truth': '', 'predicted': 'Radical MPs', 'question': 'Who spoke in support of Victoria at the rally in Trafalgar Square?'}, '5725cfda38643c19005acd71': {'truth': 'the Bill & Melinda Gates Foundation Trust, which manages the endowment assets and the Bill & Melinda Gates Foundation', 'predicted': '', 'question': 'What two entities was the foundation split into in october 2016'}, '5725cfda38643c19005acd72': {'truth': \"spend all of [the Trust's] resources within 20 years after Bill's and Melinda's deaths\", 'predicted': 'within 20 years', 'question': 'When must the trust resources be spent'}, '5725cfda38643c19005acd73': {'truth': 'the proceeds from the Berkshire Hathaway shares he still owns at death are to be used for philanthropic purposes within 10 years', 'predicted': 'philanthropic purposes', 'question': 'What does warren Buffet stipulate his berkshire hathaway  shares be used for in the 10 year period after his death '}, '5a0cfad6f5590b0018dab6d1': {'truth': '', 'predicted': '10 years', 'question': 'How long after the estate has been settled Will the Bill and Melinda gates foundation shares be used for philanthropy?'}, '56e83bdf37bdd419002c44ba': {'truth': 'avere', 'predicted': 'avere\" and \"essere', 'question': 'What Italian word is similar to the French word \"avoir\"?'}, '56e83bdf37bdd419002c44be': {'truth': 'sere', 'predicted': '', 'question': 'When forming compound tenses in Spanish, what auxiliary is no longer used?'}, '5ad27222d7d075001a429486': {'truth': '', 'predicted': 'various foods, some family relationships, and body parts', 'question': 'In Italian and French which words are very different while the same words are similar in Spanish?'}, '5ad27222d7d075001a429487': {'truth': '', 'predicted': 'Italian and Spanish', 'question': 'Which two languages have undergone more change than French regarding phonological structures?'}, '5ad27222d7d075001a429488': {'truth': '', 'predicted': 'essere', 'question': 'What Spanish word is similar to the Italian word avere?'}, '5ad27222d7d075001a429489': {'truth': '', 'predicted': 'avere', 'question': 'Which Spanish word is similar to the Italian word essere?'}, '5ad27222d7d075001a42948a': {'truth': '', 'predicted': 'avere\" and \"essere', 'question': 'Which Italian term is similar to avoir?'}, '57295b726aef051400154d56': {'truth': 'sailing, athletics, swimming, diving, triathlon and equestrian events', 'predicted': 'sailing, athletics, swimming, diving, triathlon and equestrian', 'question': 'What events did Bermuda compete in at the 2004 Summer Olympics?'}, '57295b726aef051400154d57': {'truth': 'made history by becoming the first black female diver to compete in the Olympic Games.', 'predicted': 'becoming the first black female diver to compete in the Olympic Games', 'question': 'What did Katura Horton-Perinchief do?'}, '57295b726aef051400154d58': {'truth': 'bronze medal in boxing.', 'predicted': 'bronze medal in boxing', 'question': 'What is the only medal Bermuda has ever won?'}, '57295b726aef051400154d59': {'truth': 'march in the Opening Ceremony in Bermuda shorts', 'predicted': 'march in the Opening Ceremony', 'question': 'What is the Olympic tradition for Bermuda, regardless of season?'}, '5ad42d05604f3c001a40094b': {'truth': '', 'predicted': 'sailing, athletics, swimming, diving, triathlon and equestrian events', 'question': 'What events did Bermuda participate in during the 2004 winter olympics?'}, '5ad42d05604f3c001a40094c': {'truth': '', 'predicted': 'becoming the first black female diver to compete in the Olympic Games', 'question': 'What did Katura Perinchief-Horton do?'}, '5ad42d05604f3c001a40094d': {'truth': '', 'predicted': \"Men's Skeleton\", 'question': 'What event did Bermuda participate in during the 2006 summer olympics?'}, '5ad42d05604f3c001a40094f': {'truth': '', 'predicted': 'Beijing Olympics', 'question': 'What games did Bermuda host in 2008?'}}, 'incorrect_text': {'5731e624e17f3d140042251b': {'truth': '4000 films', 'predicted': 'more than 4000', 'question': 'In more that 100 years how many films have been produced in Egypt?'}, '57326c5ae99e3014001e6794': {'truth': 'the South Bronx', 'predicted': 'Yankee Stadium', 'question': 'Where was arson a big problem in the Bronx?'}, '57326c5ae99e3014001e6795': {'truth': '1974', 'predicted': '1970s', 'question': 'When was the phrase \"The Bronx is burning\" first widespread?'}, '57326c5ae99e3014001e6796': {'truth': 'BBC', 'predicted': 'Edwin Pagan', 'question': 'Who made a documentary called \"The Bronx is burning\"?'}, '5709b308ed30961900e84430': {'truth': '1777', 'predicted': '1862', 'question': 'Which year was it when paper money was first issued without the backing of precious metals?'}, '5709b308ed30961900e84434': {'truth': 'article 1', 'predicted': 'section 10', 'question': 'The loss in value resulted in a clause being written in which article in the US Constitution?'}, '56be932e3aeaaa14008c90fb': {'truth': 'five', 'predicted': 'Australia, Hungary, Ireland, New Zealand and the United States', 'question': 'How many countries did her song \"Irreplaceable\" get number one status in?'}, '56be932e3aeaaa14008c90fc': {'truth': 'five', 'predicted': 'three', 'question': 'How many singles did her second album produce?'}, '57281dc22ca10214002d9e2a': {'truth': 'one thousand civilians', 'predicted': 'about one thousand', 'question': 'How many people died on the first German raid on London?'}, '5726ceaa5951b619008f7e93': {'truth': 'Russia', 'predicted': 'the Tsar', 'question': 'Who was given the special role of guardian over the Orthodox Christians in Moldavia and Wallachia?'}, '5726ceaa5951b619008f7e95': {'truth': 'Nicholas', 'predicted': 'Austria', 'question': 'Who felt Europe would not object to the joining of neighboring Ottoman provinces?'}, '5733e6a54776f41900661475': {'truth': 'Sheffield United', 'predicted': 'Manchester United', 'question': 'For which team was the first goal scored?'}, '5725f36689a1e219009ac0f2': {'truth': 'white sleeves', 'predicted': 'brighter pillar box red', 'question': 'What distinctive change did Chapman make to the Arsenal shirts?'}, '5725f36689a1e219009ac0f3': {'truth': 'red and white', 'predicted': 'all-red shirts', 'question': 'For what style of shirts Arsenal known ?'}, '570e6d560dc6ce1900205050': {'truth': 'below the level required for the George Medal', 'predicted': 'gallantry', 'question': 'Of what acts did the Members of the Order of the British Empire appoint?'}, '5730c36ab7151e1900c01530': {'truth': \"purification was eventually associated with the feast of Mary's very conception\", 'predicted': '\"Conception of St. Ann', 'question': \"What became associated with the celebration of Mary's inception in the womb ? \"}, '56cf306baab44d1400b88dea': {'truth': 'gay rights movement', 'predicted': 'modern gay rights', 'question': 'What movement is the Stonewall Inn most famously associated with?'}, '570af6876b8089140040f648': {'truth': 'hand-held mobile devices', 'predicted': 'workers on an off-shore oil rig', 'question': 'What is an example of a place that videoconferencing can be used today?'}, '572fcc11947a6a140053ccd2': {'truth': 'circular chromosome', 'predicted': 'single circular', 'question': 'What shape is chromosome of bacteria?'}, '5726c029f1498d1400e8ea33': {'truth': 'Pavarotti', 'predicted': 'Roger Taylor', 'question': 'Who performed with Brian May in 1998 at a benefit concert?'}, '5727dad64b864d1900163e9c': {'truth': 'loose tolerances', 'predicted': 'relatively loose', 'question': 'What type of tolerances does the USB standard specify for compliant USB connectors?'}, '572f9a2ba23a5019007fc7ca': {'truth': 'oxygen', 'predicted': 'pig iron', 'question': 'What element was used in the production of wrought iron?'}, '5731c7ade17f3d14004223d8': {'truth': 'indulgences', 'predicted': 'the Church and the papacy', 'question': 'What did the theses argue against selling?'}, '570a661f6d058f1900182e0d': {'truth': 'Oxford', 'predicted': 'Robert C. Solomon', 'question': 'Who published What Is An Emotion?: Classic and Contemporary Readings?'}, '572b8f5d111d821400f38f12': {'truth': 'foreign language most often used', 'predicted': '24.86 percent', 'question': 'What distinction does Czech have in Slovakia?'}, '5725ca1589a1e219009abeaf': {'truth': 'this foundation helps move public libraries into the digital age.', 'predicted': 'installed computers and software', 'question': 'What has the grant enabled '}, '57264dedf1498d1400e8db8c': {'truth': 'the North German Confederation', 'predicted': 'Schutz des geistigen Eigentums', 'question': 'Which constitution gave legislative power to protect intellectual property?'}, '57273aa2dd62a815002e99c5': {'truth': 'chivalry', 'predicted': 'chivalric', 'question': 'What was the code of conduct of the military orders called?'}, '56def0acc65bf219000b3e3d': {'truth': 'the cultural aspects of Christianity', 'predicted': 'cultural aspects of Christianity, irrespective of personal religious', 'question': 'Whether one partakes in practices or beliefs, the label Christian is sometimes attached because they associate with what?'}, '57301bfca23a5019007fcd83': {'truth': 'penicillin, vancomycin, penicillin and vancomycin, or chlortetracycline', 'predicted': 'subtherapeutic antibiotic treatment', 'question': 'What are some antibiotics can be used for STAT?'}, '57328cf2b3a91d1900202e35': {'truth': 'unclear', 'predicted': 'increased body mass', 'question': 'Do antibiotics increase the chance of getting fat for humans?'}, '56f8ee329e9bad19000a0719': {'truth': 'boys', 'predicted': 'males', 'question': 'What gender has a higher enrollment?'}, '57318f33a5e9cc1400cdc081': {'truth': 'geometric', 'predicted': 'two-tone stone mosaic paving', 'question': 'What is the most common pattern for Portuguese pavement?'}, '56d3847159d6e414001465ff': {'truth': 'church', 'predicted': 'Bible Belt', 'question': 'Where do people in the Southern United States often begin singing? '}, '56db3ebee7c41114004b4f97': {'truth': 'church', 'predicted': 'the Bible Belt', 'question': 'Where do a lot of people get their start in singing in the south?'}, '56f8c9d99b226e1400dd1004': {'truth': 'forebrain area', 'predicted': 'geometry', 'question': 'Which part of the brain has led to many distortions among different species?'}, '572ac154be1ee31400cb8215': {'truth': '\"... you end up getting us stuck in a war in Iraq. Just ask President Bush.\"', 'predicted': 'a jab at President Bush', 'question': \"What was Kerry supposed to say when he 'botched a joke'?\"}, '572ac154be1ee31400cb8217': {'truth': 'inadvertently left out the key word \"us\"', 'predicted': 'botched', 'question': 'What mistake did Kerry make in the joke?'}, '5726069c38643c19005acf61': {'truth': 'Greek', 'predicted': 'Hellenic overlords', 'question': 'Native populations in the Hellenistic world were discriminated by what peoples?'}, '5726069c38643c19005acf63': {'truth': 'highly localized', 'predicted': 'Gymnasiums and their Greek education, for example, were for Greeks only. Greek cities and colonies may have exported Greek art and architecture as far as the Indus, but these were mostly enclaves', 'question': 'What are the areas of concentration from where Greek culture eminates?'}, '56f8b9549e9bad19000a03b7': {'truth': 'their specific DNA loci', 'predicted': 'their functional products (proteins or RNA)', 'question': 'What does the typical definition of a gene categorize genes by?'}, '5727cf924b864d1900163db0': {'truth': \"Huxley had emphasised anatomical similarities between apes and humans, contesting Owen's view that humans were a separate sub-class\", 'predicted': 'Oxford evolution', 'question': 'What was the debate between Huxley and Owen concerning humans and apes?'}, '56e087957aa994140058e5c1': {'truth': '2', 'predicted': 'two', 'question': 'How many different spin isomers exist?'}, '5727a6233acd2414000de8d5': {'truth': 'Yale University', 'predicted': 'Albertus Magnus College', 'question': 'What private university is located in downtown New Haven?'}, '572ea0bedfa6aa1500f8d21f': {'truth': 'university entrance examinations', 'predicted': 'scores', 'question': 'What plays a large factor in determining admission at a university?'}, '5727ff933acd2414000df1be': {'truth': 'professional archivists', 'predicted': 'analogue-to-digital converters', 'question': 'What would offer the highest quality transfers of historic interest?'}, '5727ff933acd2414000df1bf': {'truth': 'standard record player with a suitable pickup, a phono-preamp (pre-amplifier) and a typical personal computer', 'predicted': 'to remove analog flaws without any further damage to the source recording', 'question': 'What would a hobbiest need to transfer historic recordings to digital formats?'}, '5727ff933acd2414000df1c0': {'truth': 'without any further damage to the source recording', 'predicted': 'analog flaws', 'question': 'Is an original destroyed when transferred to digital format?'}, '570ddc210dc6ce1900204cd0': {'truth': 'early maturing', 'predicted': 'late', 'question': 'Do early or late maturing girls have more unwanted pregnancies?'}, '56f8d8959e9bad19000a05e2': {'truth': 'PRS (Party for Social Renewal)', 'predicted': 'African Party for the Independence of Guinea and Cape Verde', 'question': 'Besides the PAIGC, what is the other major political party?'}, '56f8d8959e9bad19000a05e4': {'truth': 'President', 'predicted': 'PAIGC', 'question': 'What office did Sanha hold in 2012?'}, '572ee3a8c246551400ce477f': {'truth': 'General Zhang Huan', 'predicted': 'Chen Fan', 'question': 'Who made accusations of treason against Dou Wu?'}, '5727903ddd62a815002ea085': {'truth': 'US$95 to US$300', 'predicted': 'high cost', 'question': 'What was the initial cost range of early recording devices?'}, '56ceb9c4aab44d1400b8892c': {'truth': 'in Wenchuan', 'predicted': 'Wenchuan County', 'question': 'Where is Yingxiu located?'}, '572f820a04bcaa1900d76a37': {'truth': 'French reinforcements were blocked by British naval victory in the Battle of Cartagena', 'predicted': 'a wave of major operations against Île Saint-Jean (present-day Prince Edward Island), the St. John River valley, and the Petitcodiac River valley. The celebration of these successes was dampened by their embarrassing defeat in the Battle of Carillon (Ticonderoga), in which 4,000 French troops repulsed 16,000', 'question': 'How did the British assure numerical superiority in taking Louisbourg?'}, '5728f64eaf94a219006a9e7e': {'truth': 'Zen monks', 'predicted': 'Yoshimasa', 'question': 'Who brought Chinese arts to Japan?'}, '57261b6dec44d21400f3d8f3': {'truth': 'long work hours', 'predicted': 'worker representation', 'question': 'What is a feature of sweatshops beyond lack of benefits and representation?'}, '572629d9ec44d21400f3db29': {'truth': \"The company's mainstay business\", 'predicted': 'mainstay businesses', 'question': 'How important were cotton, silk, indigo dye,saltpetre and tea to the company?'}, '572629d9ec44d21400f3db2b': {'truth': 'ousting the Portuguese in 1640–41', 'predicted': 'aggressive competitors', 'question': 'what caused the Dutch to expand thier spice trade in the malaccan straits?'}, '57271234f1498d1400e8f322': {'truth': 'surgical procedures', 'predicted': 'describing procedures on various forms of surgery', 'question': 'What information is in the Susrutasamhita of Susruta?'}, '572ff0a2947a6a140053ce37': {'truth': 'natural gas', 'predicted': 'fossil fuels', 'question': 'What resource does Iran have the largest supply of in the world?'}, '572690d45951b619008f76ce': {'truth': 'the order of the Emperor Maximilian', 'predicted': 'Emperor Maximilian of Mexico', 'question': 'Who made the Angel of Independence?'}, '5728b2813acd2414000dfcf9': {'truth': 'traditional and folk culture', 'predicted': 'Madeira Island', 'question': 'Where can you see mandolins a part of in Portgal? '}, '572ec004c246551400ce45f6': {'truth': 'five days of artillery bombardment', 'predicted': 'Battle of Gross-Jägersdorf', 'question': 'What led to the defeat of the Prussians at Memel?'}, '572ec004c246551400ce45f7': {'truth': 'used Memel as a base to invade East Prussia', 'predicted': 'artillery bombardment', 'question': 'How did the Russians used the captured Memel?'}, '572ec004c246551400ce45f8': {'truth': 'defeated a smaller Prussian force', 'predicted': 'artillery bombardment', 'question': 'What caused the Russians based in Memel to be successful deeper into Prussia?'}, '57313b16e6313a140071cd53': {'truth': 'collection of the land tax', 'predicted': 'unorthodox sects', 'question': 'What did Yongzheng crack down on?'}, '57327ed206a3a419008aca8d': {'truth': 'Greek manuscripts', 'predicted': 'fall of the Byzantine Empire to the Turks', 'question': 'What caused a large migration of Greek refuges in the 1450s?'}, '5726d3475951b619008f7f27': {'truth': 'little risk of Chinese intervention in Korea', 'predicted': \"because of the General's discourteous refusal to meet the President on the continental United States.\", 'question': 'What was President Truman told at this meeting?'}, '57318a1305b4da19006bd25d': {'truth': 'Roman', 'predicted': 'the Arabs', 'question': 'In Syria and Egypt, other than early Christians, who influenced their mosaic work?'}, '56d64a821c85041400947075': {'truth': 'inadequately engineered', 'predicted': 'Due to the one-child policy', 'question': 'Why did so many schools collapse during the earthquake?'}, '56e0f68f7aa994140058e836': {'truth': 'June 16, 1963', 'predicted': 'Vostok 6', 'question': 'The first woman to launch into space was on what date?'}, '56e0f68f7aa994140058e837': {'truth': 'The USSR', 'predicted': 'Soviet', 'question': 'The first woman to go into space was from which country?'}, '57269b6b708984140094cb77': {'truth': 'two or more elements', 'predicted': 'pure or fairly pure chemical elements', 'question': 'What is an alloy composed of?'}, '57105362b654c5140001f8ce': {'truth': 'varied', 'predicted': 'widely', 'question': 'Was government response to the Enlightenment uniformly positive or widely varied?'}, '57096228ed30961900e84042': {'truth': 'within and outside the country', 'predicted': 'increased within and outside', 'question': 'Has the demand increased inside or outside the country?'}, '572675a3dd62a815002e85b5': {'truth': \"Britain's dominant position in world trade\", 'predicted': 'imperial century', 'question': \"What was the period of Britain acting as the world's police called?\"}, '5726dc97708984140094d3f5': {'truth': 'a submarine base', 'predicted': 'Basis Nord', 'question': 'What did the Germans use to avoid British blockades?'}, '5726dc97708984140094d3f7': {'truth': 'both the Atlantic and the Pacific', 'predicted': 'Northern Sea Route', 'question': 'Which oceans did the sub base provide access to?'}, '5728a75e4b864d1900164b93': {'truth': 'Anweisung die Mandoline von selbst zu erlernen nebst einigen Uebungsstucken von Bortolazzi', 'predicted': 'Cremonese mandolin', 'question': \"What was Bartolomeo Bortolazzi's popular mandolin method?\"}, '5726fec6dd62a815002e973d': {'truth': 'The area in which a glacier forms', 'predicted': 'corrie or cwm', 'question': 'What is a cirque?'}, '56e763f737bdd419002c3f2d': {'truth': 'tissue paper', 'predicted': '250 kg/m3', 'question': 'What is the lightest density of paper produced?'}, '56e763f737bdd419002c3f2e': {'truth': '800 kg/m3', 'predicted': '250 kg/m3', 'question': 'What is the common density of printing paper?'}, '572a54e07a1753140016aeb2': {'truth': 'On December 16, 2010', 'predicted': 'August 31, 2011', 'question': 'On which day did CBC release an updated announcement stating they were striving to update all 27 transmitters?'}, '57265269708984140094c254': {'truth': 'it is found in bushy places, in rough grassland, among agricultural crops, and in other places with dense cover.', 'predicted': 'bushy places, in rough grassland, among agricultural crops, and in other places with dense cover. It feeds on seeds, insects, and other small invertebrates. Being a largely ground-dwelling, gregarious bird, domestication of the quail was not difficult, although many of its wild instincts are retained in captivity', 'question': 'Where can quails typically be found in the wild?'}, '5730501a396df91900096054': {'truth': 'larger target, compared to attacking tail-on, as well as a better chance of not being seen by the bomber', 'predicted': 'a larger target', 'question': 'What benefits did attacking from below offer?'}, '570f40f65ab6b81900390eb4': {'truth': '9 p.m', 'predicted': '21:00', 'question': 'When is the melatonin onset?'}, '570f40f65ab6b81900390eb7': {'truth': 'melatonin phase', 'predicted': 'melatonin offset', 'question': 'What are the more reliable markers in determining sleep timing?'}, '572a630e7a1753140016aefd': {'truth': 'northern outskirts', 'predicted': 'Neustift am Walde', 'question': 'Where was Hayek buried in relation to his home town of Vienna?'}, '572fa91e04bcaa1900d76b67': {'truth': 'concentration gradients across membranes', 'predicted': 'energy generation', 'question': 'What is crucial for biochemical reactions?'}, '572fa91e04bcaa1900d76b6a': {'truth': 'light-gathering complexes may even form lipid-enclosed structures', 'predicted': 'chlorosomes', 'question': 'Can membrane of bacteria create lipid structure?'}, '5733f0774776f4190066156e': {'truth': 'define the broad outline of its policies in a programme, and present it to the Assembly for a mandatory period of debate', 'predicted': 'to define the broad outline of its policies in a programme', 'question': \"What process is required of each government's policies?\"}, '5727e243ff5b5019007d977d': {'truth': 'Seattle SuperSonics', 'predicted': 'Oklahoma City Thunder', 'question': \"What was the Thunder's previous name?\"}, '5726994b708984140094cb4d': {'truth': 'although it was a major first step towards Cubism it is not yet Cubist.', 'predicted': 'exaggeration', 'question': 'Is it true that the first Cubist picture is The Demoiselles? '}, '56f78981aef2371900625bab': {'truth': 'the South Pacific Mandate', 'predicted': 'the Empire of Japan', 'question': 'What group was the Marshall Islands a part of following World War I?'}, '56f78981aef2371900625bac': {'truth': 'the United States', 'predicted': 'the Empire of Japan', 'question': 'Who took over the Marshall Islands in the second world war?'}, '56ce5f72aab44d1400b8870f': {'truth': '50', 'predicted': '30%', 'question': 'What percentage of energy in commercial buildings comes from HVAC systems?'}, '56cfebbd234ae51400d9c0c8': {'truth': '50% (10.1 EJ/yr)', 'predicted': 'nearly 50%', 'question': 'How much energy does an HVAC system use in residential locations?'}, '572734eb5951b619008f86b5': {'truth': 'competition was suspended due to the First World War', 'predicted': 'Following the 1914–15', 'question': 'Was competition suspended due to the first world war? '}, '5735c0d8e853931400426b4c': {'truth': 'Birendra', 'predicted': 'Mahendra', 'question': 'Who was the penultimate king of Nepal?'}, '56f8d4209b226e1400dd10af': {'truth': 'reasons of sustainability', 'predicted': 'sustainability of the fragile Alpine terrain', 'question': 'Why are villages considering becoming car free zones?'}, '56cf39c4aab44d1400b88eba': {'truth': 'MGM', 'predicted': 'Danjaq, LLC', 'question': 'Which film studio won the full copyright film rights to Spectre?'}, '57267c12dd62a815002e86c3': {'truth': 'Canada', 'predicted': 'the UK', 'question': 'The first Home Rule bill would have given Ireland less self-control than what other territory?'}, '57267c12dd62a815002e86c5': {'truth': '1914', 'predicted': '1886', 'question': 'When was a Home Rule bill passed?'}, '572714d2708984140094d977': {'truth': 'consumption of carcinogenic preserved foods', 'predicted': \"affluence or a 'Western lifestyle\", 'question': 'What were cancers such as liver cancer or stomach cancer found to have a link to?'}, '56df86855ca0a614008f9c1d': {'truth': '3', 'predicted': 'three', 'question': 'How many major highways cross through Oklahoma City?'}, '5725c64b271a42140099d18f': {'truth': 'universities in the U.S., Europe, India, China and South Africa, have received grants to develop innovative on-site and off-site waste treatment solutions', 'predicted': 'more than a dozen research teams, mainly at universities in the U.S., Europe, India, China and South Africa', 'question': 'What countries have received grants '}, '56e6f6e0de9d371400068100': {'truth': 'Contemporary Christian music', 'predicted': 'Christian AC', 'question': 'What is CCM an acronym of?'}, '5706aab252bb891400689b3c': {'truth': 'differentiate the clubs and DJs', 'predicted': 'to maintain such exclusives', 'question': 'Why were DJs inspired to create their own house records?'}, '5729a3d56aef051400155078': {'truth': 'first-world countries', 'predicted': 'cultures that have other protein sources such as fish or livestock', 'question': 'Where is eating insects considered taboo?'}, '570e437d0dc6ce1900204eee': {'truth': 'sodium diuranate', 'predicted': 'yellow compound', 'question': 'What did Klaproth probably create when he dissolved pitchblende in nitric acid?'}, '56e6d988de9d371400068086': {'truth': 'hot AC', 'predicted': 'CHR', 'question': 'What AC format is still viable?'}, '5729667c3f37b3190047833c': {'truth': 'Florio, the Ducrot, the Rutelli, the Sandron, the Whitaker, the Utveggio', 'predicted': 'several families', 'question': 'Which families help to start cultural, industrial, and economic growth in Palermo?'}, '56df4fb48bc80c19004e4a61': {'truth': 'North Oklahoma City', 'predicted': 'north side', 'question': 'Which side is more urban and fashionable?'}, '56ce7376aab44d1400b887a7': {'truth': 'Roman times', 'predicted': '16th century', 'question': 'When were the first greenhouses used?'}, '56ce7376aab44d1400b887a8': {'truth': 'the 16th', 'predicted': '16th century', 'question': 'In what century were the first modern greenhouses constructed?'}, '56d0875b234ae51400d9c349': {'truth': 'enabling year-round production and the growth (in enclosed environments) of specialty crops', 'predicted': 'keep exotic plants brought back from explorations abroad', 'question': 'What is one purpose of a greenhouse?'}, '5727c25f2ca10214002d9593': {'truth': 'substantive', 'predicted': 'formalist', 'question': 'What is another word for a thick definition?'}, '5729778f6aef051400154f5b': {'truth': 'entire Northeast was affected', 'predicted': 'postwar period', 'question': 'Was the city the only one that suffer a decline within the manufacturing sector?'}, '573040dd947a6a140053d33a': {'truth': 'support the German submarine force', 'predicted': 'attack British port facilities', 'question': 'What did Erich Raeder believe the Luftwaffe needed to do?'}, '573040dd947a6a140053d33c': {'truth': 'the high success rates of the U-Boat force', 'predicted': 'the need to attack British port facilities', 'question': 'What ultimately convinced Hitler that Raeder was right?'}, '5732696fe17f3d1400422961': {'truth': 'Eisenhower', 'predicted': 'the Russians', 'question': 'Who refused to permit nuclear weapons inspections in the wake of the 1955 talks?'}, '57268054dd62a815002e8764': {'truth': 'pesticides are also used for non-agricultural purposes', 'predicted': 'protect plants from damaging influences such as weeds, fungi, or insects. This use of pesticides is so common that the term pesticide is often treated as synonymous with plant protection product, although it is in fact a broader term', 'question': 'What is the difference between a pesticide and a plant protection product?'}, '57268054dd62a815002e8766': {'truth': 'The most common use of pesticides is as plant protection products', 'predicted': 'attracting, seducing, and then destroying any pest', 'question': 'What are pesticides most commonly used for?'}, '57303157b2c2fd1400568a39': {'truth': '\"tweak\" the match', 'predicted': 'cancel a certain amount of reactance', 'question': 'Why would a transmitter have additional adjustments?'}, '5732836406a3a419008acaaf': {'truth': 'Renaissance', 'predicted': 'nineteenth century', 'question': 'During what time period did secular have a more neutral connotation?'}, '5732836406a3a419008acab0': {'truth': 'Gherardo', 'predicted': 'winning earthly glory and praising virtue', 'question': 'Petrarch felt that although he tried to do his own form of good whose life may have more meaning?'}, '5731f24bb9d445190005e6d5': {'truth': 'defiance of the omen', 'predicted': '\"though the sacred chickens would not eat', 'question': \"What was Publius's critical mistake in his sea campaign?\"}, '5731f24bb9d445190005e6d7': {'truth': 'impiety', 'predicted': 'divine will', 'question': \"What was the cause of Publius's failures according to Roman feeling?\"}, '56d4647c2ccc5a1400d83133': {'truth': '\"Ed in \\'08\"', 'predicted': 'Strong American Schools', 'question': 'What campaign did the Kanye West Foundation partner with in 2007?'}, '57268e59708984140094c9f8': {'truth': 'neolithic age domestication of plants and animals and the use of polished stone tools dating to sometime between 10,000 and 6,000 BC has been discovered', 'predicted': 'Archaeological evidence shows that Homo erectus lived in the region now known as Myanmar as early as 400,000 years ago', 'question': 'Did any other ancient cultures also leave behind evidence of existence in Myanmar?'}, '57262d20271a42140099d704': {'truth': 'as they got hotter, their electrical resistance decreased', 'predicted': 'negative temperature coefficient of resistance', 'question': 'What was the primary problem with early carbon filaments?'}, '57262d20271a42140099d705': {'truth': 'improved the uniformity and strength of filaments as well as their efficiency', 'predicted': \"stabilize the lamp's power consumption, temperature and light output against minor variations in supply voltage\", 'question': 'What were the positive effects of the flashing process?'}, '57262d20271a42140099d707': {'truth': \"helped stabilize the lamp's power consumption, temperature and light output against minor variations in supply voltage\", 'predicted': 'metallic conductor', 'question': 'What are the effects of giving the filament a positive temperature coefficient?'}, '56dedc703277331400b4d776': {'truth': 'Hummers', 'predicted': 'street-legal, civilian', 'question': 'In addition to the Humvee, what other vehicle manufactured by AM General was Schwarzenegger first to own?'}, '5706be1e0eeca41400aa0de8': {'truth': '$31,997', 'predicted': '$26,969', 'question': 'What was the median income for a family in the city?'}, '56d1091117492d1400aab7bf': {'truth': 'Daft Punk', 'predicted': '50 Cent', 'question': \"What music group was in Kanye's first release off of Graduation?\"}, '5726245b89a1e219009ac2f0': {'truth': 'ram it', 'predicted': 'bailed out', 'question': 'What did Holmes do to stop the German Plane?'}, '57305ed58ab72b1400f9c4ab': {'truth': 'an altar', 'predicted': 'Private and personal worship', 'question': 'What religious element could be found in all Roman households?'}, '5730b55f396df919000962cd': {'truth': \"Sony's PlayStation Portable (PSP), the Nintendo DS and Game Boy Advance, the Gizmondo, the Dingoo and the GP2X by GamePark Holdings\", 'predicted': 'handheld', 'question': 'What portable game systems have SNES emulators?'}, '56f9ebe18f12f3190062fffb': {'truth': 'an alliance of broadcasters, consumer electronics manufacturers and regulatory bodies', 'predicted': 'Digital Video Broadcasting', 'question': 'What is the DVB?'}, '5729058baf94a219006a9f6b': {'truth': 'Christian and Muslim populations do, however, face religious persecution', 'predicted': 'Many religions', 'question': 'Are all welcomed to practice faith openly in Burma '}, '56e7a21637bdd419002c42a0': {'truth': '25th', 'predicted': 'silver anniversary', 'question': 'What anniversary did the Arena Football League celebrate in 2012?'}, '572fe399a23a5019007fcae1': {'truth': 'antistatic bags', 'predicted': 'static sensitive', 'question': 'What special protection are most PCBs shipped in?'}, '56de471ccffd8e1900b4b770': {'truth': 'Grover Cleveland', 'predicted': 'Johnson Administration', 'question': 'Under which President was the Tenure of Office Act repealed? '}, '56dfbe777aa994140058e0e2': {'truth': 'ANSI and ASHRAE', 'predicted': 'The Illuminating Engineering Society of North America', 'question': 'Who else publishes along with IESNA? '}, '5722d357f6b826140030fc67': {'truth': 'fiftieth anniversary', 'predicted': '1887', 'question': 'What year anniversary does the Golden Jubilee celebrate?'}, '5723e1e80dadf01500fa1f6e': {'truth': 'Abdul Karim', 'predicted': 'teaching her Hindustani, and acting as a clerk', 'question': 'What was the name of the waited that was promoted to Munshi?'}, '57277778708984140094de55': {'truth': 'the texts, as transmitted, contain a considerable amount of variation,', 'predicted': 'because the texts, as transmitted, contain a considerable amount of variation', 'question': 'What is one reasons Shakespeare is a good place to focus on textual criticism?'}, '572802332ca10214002d9b53': {'truth': 'any electronic dance music and hip hop releases today are still preferred on vinyl', 'predicted': 'digital copies are still widely available. This is because for disc jockeys (\"DJs\"), vinyl has an advantage over the CD: direct manipulation of the medium', 'question': 'What is commonly preferred by DJs vinyl or CD?'}, '572f7b31b2c2fd140056817d': {'truth': 'social and societal upheaval', 'predicted': 'The Jazz Age', 'question': 'What is the \"Roaring Twenties\" ?'}, '56f9608b9b226e1400dd13de': {'truth': 'parliamentary-presidential system', 'predicted': 'mixed parliamentary-presidential', 'question': 'What is the governing system of the Marshall Islands?'}, '570bd2ec6b8089140040fa69': {'truth': 'Control-Q', 'predicted': 'Control-S', 'question': 'What caused the automatic paper tape reader to start again?'}, '573039c004bcaa1900d773c5': {'truth': '480 infantrymen', 'predicted': 'six centuries', 'question': 'Around how many units could be expected to be contained within a cohort?'}, '572785c8dd62a815002e9f6e': {'truth': 'physical and psychological implications', 'predicted': 'working with drug dealers', 'question': 'What are the risks of child labour in drug cartels?'}, '572842a0ff5b5019007da032': {'truth': 'gradually changed', 'predicted': 'He ceased consulting his colleagues and made more and more of the decisions himself', 'question': 'What did Nasser do over the years of his rule?'}, '5735e8736c16ec1900b9288a': {'truth': 'Hadza of Tanzania', 'predicted': 'the Hadza', 'question': 'Who are the only remaining full-time hunter-gatherers in Africa?'}, '56fc975cb53dbe1900755134': {'truth': 'feeding', 'predicted': 'phonological alternation', 'question': 'Aside from bleeding what is an order of rules that define how pronunciation of a sound changes?'}, '56fc975cb53dbe1900755135': {'truth': 'Phonology', 'predicted': 'phonotactics', 'question': 'Phonotactics, phonological alternation and prosody are topics contained in what discipline?'}, '56fc975cb53dbe1900755136': {'truth': 'prosody', 'predicted': 'stress', 'question': 'Stress and intonation are studied under what topic?'}, '572eb63603f989190075697e': {'truth': 'one was granted, one was partially granted, one was denied and three were withdrawn', 'predicted': 'identification of reasonable and prudent alternatives to avoid jeopardy', 'question': 'What were the results of those exemption petitions?'}, '5728cc17ff5b5019007da6e3': {'truth': 'three big provinces', 'predicted': 'Maharashtra, Bengal and Punjab', 'question': 'What parts of India shaped the demands of the people for nationalism?'}, '572f7d6f04bcaa1900d76a19': {'truth': '26.6 °C', 'predicted': '21–33 °C (70–91 °F)', 'question': 'What is the mean yearly temperature in Hyderabad in Celsius?'}, '5726e8eb708984140094d58b': {'truth': 'Upon being harmed (e.g., stung) by their prey, the appearance in such an organism will be remembered as something to avoid', 'predicted': 'While that particular prey organism may be killed, the coloring benefits the prey species as a whole', 'question': 'How does aposematism help a species population?'}, '5726e8eb708984140094d58c': {'truth': 'bright, easily recognizable and unique colors and patterns', 'predicted': 'brightly colored', 'question': 'What visual cues are characteristic of aposematism?'}, '572f5d5eb2c2fd1400568085': {'truth': 'prescribed targets are not hit', 'predicted': 'inability to damage industries sufficiently', 'question': 'Why did Hitler feel the Luftwaffe was unsuccessful in bombing raids?'}, '570e25b30dc6ce1900204dfb': {'truth': 'its colonial history', 'predicted': 'Ethiopian cooking', 'question': 'Where does the Italian influences on Eritrean cuisine come from?'}, '5727ff0c2ca10214002d9ae7': {'truth': 'Mughal Empire', 'predicted': 'Mughals', 'question': 'What empire covered most of India in the 16th century?'}, '572820842ca10214002d9e7e': {'truth': 'no ex-ante time or size limit', 'predicted': 'temporary', 'question': 'What is the duration of the Outright Monetary Transactions program?'}, '5731225ca5e9cc1400cdbc75': {'truth': 'Asia', 'predicted': 'Beringia', 'question': 'Where do most theories today attribute the settlement of the Americas as originating from?'}, '5726909f5951b619008f76c3': {'truth': '$173 billion', 'predicted': '€130 billion', 'question': 'in 2012, what was the amount of the bailout?'}, '5726909f5951b619008f76c6': {'truth': '€3 billion', 'predicted': '4.95%', 'question': 'How much did Greece make from a bond sale in 2014?'}, '570fc65b80d9841400ab366d': {'truth': 'going up', 'predicted': 'going down', 'question': \"What was happening to Dell's average sale to individuals?\"}, '572792f3f1498d1400e8fc8d': {'truth': 'end-of-year compilation', 'predicted': 'two', 'question': 'Along with individual tributes, what form did TCM Remembers occur in?'}, '57268ac8dd62a815002e88da': {'truth': 'eighth', 'predicted': '30th', 'question': 'What ranking in terms of GDP is Mexico City globally?'}, '5727d9b5ff5b5019007d96d6': {'truth': 'a mystical theologian', 'predicted': 'Areopagite', 'question': 'Who was Dionysus?'}, '572fa925947a6a140053cb1c': {'truth': 'theatre and drama', 'predicted': 'music', 'question': 'What type of art might one encounter at Lalithakala Thoranam?'}, '5706a52352bb891400689b13': {'truth': 'National Gallery of Modern Art', 'predicted': 'Indira Gandhi Memorial Museum', 'question': 'What is the name of the modern art museum located in new Delhi?'}, '57329efbcc179a14009dab80': {'truth': '19', 'predicted': 'almost 100 times', 'question': 'For how many months was Australia attacked from the air by Japan?'}, '570acf964103511400d59a25': {'truth': 'expansion of seaways', 'predicted': 'the decline and extinction of sauropods', 'question': 'What event of the Early Cretaceous caused the extinction of several species?'}, '56e788ab37bdd419002c40d5': {'truth': 'no lines', 'predicted': 'walk from document to document as they wish.', 'question': 'What is an aspect of a visit to the Rotunda for the Charters of Freedom?'}, '57293b691d0469140077918e': {'truth': 'dramatically lower solar prices', 'predicted': 'lower solar prices and weakened US and EU markets', 'question': 'Why did the total investment in renewable energy go down in 2012?'}, '56de24b24396321400ee25fd': {'truth': 'John Locke', 'predicted': 'Thomas Hobbes', 'question': 'Which Enlightenment thinker supported the idea of separation of powers?'}, '56de33fc4396321400ee2694': {'truth': 'John Locke', 'predicted': 'Thomas Hobbes, strongly opposed it. Montesquieu', 'question': 'Who was an advocate of separation of powers?'}, '572a7a17be1ee31400cb8029': {'truth': 'Miami International', 'predicted': 'MIA', 'question': \"What is Florida's busiest airport?\"}, '572ec21cdfa6aa1500f8d350': {'truth': '\"People\\'s Courts\" were founded to try various monarchist politicians and journalists, and though many were imprisoned, none were executed.', 'predicted': 'purged monarchists and members of Idris\\' Senussi clan from Libya\\'s political world and armed forces; Gaddafi believed this elite were opposed to the will of the Libyan people and had to be expunged. \"People\\'s Courts\" were founded to try various monarchist politicians and journalists, and though many were imprisoned', 'question': 'What happened to the monarchists and and journalists?'}, '57317177a5e9cc1400cdbf71': {'truth': 'monarchists', 'predicted': 'Idris', 'question': 'Along with the Senussi, who was purged from the military?'}, '56dd37fe66d3e219004dac78': {'truth': 'Right Honourable', 'predicted': 'president', 'question': 'What honorific title can be given to prime ministers in commonwealth nations?'}, '56fdee67761e401900d28c57': {'truth': 'basic instruction can be given a short name that is indicative of its function', 'predicted': 'ADD, SUB, MULT or JUMP', 'question': \"A computer's assembly language is known as what?\"}, '5727fb4eff5b5019007d99e5': {'truth': 'two', 'predicted': 'four', 'question': \"How many posts did the Muslim Brotherhood get in Naguib's cabinet?\"}, '5727fb4eff5b5019007d99e6': {'truth': 'opposed', 'predicted': \"most of the RCC insisted on executing the riot's two ringleaders\", 'question': \"What was Nasser's position on executing the rioter's leaders?\"}, '56e4cfd839bdeb14003479e0': {'truth': 'smart growth, architectural tradition and classical design', 'predicted': 'sustainable approach towards construction', 'question': 'What are three things the new movements try to achieve?'}, '573428b44776f419006619ca': {'truth': 'Marana', 'predicted': 'the Northwest', 'question': 'Where is the Continental Ranch planned community?'}, '573428b44776f419006619cb': {'truth': 'Marana', 'predicted': 'the Northwest', 'question': 'Where is the Dove Mountain planned community?'}, '573428b44776f419006619cc': {'truth': 'Oro Valley', 'predicted': 'the Northwest', 'question': 'Where is the Rancho Vistoso planned community?'}, '572b6f49111d821400f38ea0': {'truth': 'Berkeley', 'predicted': 'John Foster', 'question': 'Whose work is Sense without Matter regarded as updating?'}, '56dfc0ae231d4119001abd99': {'truth': 'upstream ISPs', 'predicted': 'contracting ISP', 'question': 'what usually has a larger network, the ISP of the customer or the upstream ISP?'}, '572ed3f503f9891900756a67': {'truth': 'Moses', 'predicted': 'Jesus', 'question': 'Which Biblical character is the most often mentioned person in the Quran?'}, '56d632371c85041400946fe4': {'truth': 'infections.', 'predicted': 'serious infections', 'question': 'A dog scratch can lead to what medical condition?'}, '56d9e7e4dc89441400fdb905': {'truth': 'infections.', 'predicted': 'serious infections', 'question': 'According to the text, dog scratches can cause what?'}, '57303bb004bcaa1900d773f4': {'truth': 'Government of Ireland', 'predicted': 'the North/South Ministerial Council', 'question': 'The Northern Ireland Executive meets with what other government to develop policies for the island of Ireland?'}, '57278133dd62a815002e9ee9': {'truth': 'the brain of one of the lower animals', 'predicted': 'the tip of the radicle', 'question': 'To what did Darwin compare the top of the plant radical?'}, '57278133dd62a815002e9eea': {'truth': 'movements of plant shoots', 'predicted': 'the tip of the radicle . . acts like the brain of one of the lower animals . . directing the several movements', 'question': 'Why did Darwin feel plants had something comparable to a brain?'}, '57278133dd62a815002e9eeb': {'truth': 'promotes cell growth', 'predicted': 'control of plant growth', 'question': 'What do auxins do?'}, '572a7b02111d821400f38b52': {'truth': 'Detroit', 'predicted': 'Miami', 'question': 'As of 2004, what city was the poorest in the United States?'}, '572f8622947a6a140053ca19': {'truth': 'nationalist demagogues', 'predicted': 'Adolf Hitler', 'question': 'In some world states who did the people turn to? '}, '57096fa9ed30961900e84122': {'truth': 'anthroposemiotics', 'predicted': 'zoo semiotics', 'question': 'What is the study of human communication called?'}, '56f986ed9b226e1400dd1512': {'truth': 'the hypothalamus,', 'predicted': 'The hypothalamus', 'question': 'A collection of small nuclei at the base of the forebrain is called what?'}, '572a23643f37b31900478729': {'truth': 'Archaic Era', 'predicted': 'Formative stage', 'question': 'What term is used to describe the Early Neolithic era in American education?'}, '56fa5493f34c681400b0c085': {'truth': 'furniture', 'predicted': 'beds', 'question': 'What category of products usually made from wood includes chairs?'}, '56fa5493f34c681400b0c087': {'truth': 'handles', 'predicted': 'wooden spoon', 'question': 'Which parts of tools are sometimes made out of wood?'}, '56fa5493f34c681400b0c089': {'truth': 'chopsticks', 'predicted': 'spoon', 'question': 'What special wooden utensils do many people use to eat Chinese takeout?'}, '572efd66cb0c0d14000f16cc': {'truth': 'social protocols', 'predicted': 'personal hygiene', 'question': 'The invention of elevators brought with it questions of social etiquette and formalities, generally referred to as what?'}, '5730c888aca1c71400fe5ab9': {'truth': 'calculators', 'predicted': 'incandescent and neon indicator lamps', 'question': 'What was one use of early LED light in products?'}, '5730c888aca1c71400fe5abb': {'truth': '1970s', 'predicted': '1968', 'question': 'In what decade were production costs greatly reduced for LEDs to enable successful commercial uses?'}, '57282ec23acd2414000df67b': {'truth': '300', 'predicted': '480', 'question': 'How many miles long was the human chain?'}, '572848e94b864d19001648c4': {'truth': 'LaserDisc', 'predicted': 'DVD still looks slightly more artificial', 'question': 'Which format is considered to look most realistic, LaserDisc or DVD?'}, '572a3b486aef0514001553b6': {'truth': \"His father's\", 'predicted': 'university professor', 'question': \"Who's occupation inspired Hayek when he was older?\"}, '572a3b486aef0514001553b7': {'truth': 'Franz von Juraschek', 'predicted': 'Gustav Edler von Hayek', 'question': \"Eugen Bohm was friends with which of Hayek's grandfathers?\"}, '572a3b486aef0514001553b8': {'truth': 'scholars', 'predicted': \"statistician and was later employed by the Austrian government. Friedrich's paternal grandfather, Gustav Edler von Hayek, taught natural sciences\", 'question': \"What occupation did Hayek's grandfather's have?\"}, '5725be0738643c19005acc41': {'truth': 'Vlaams', 'predicted': 'Flemish', 'question': 'What would someone in Belgium call the variation of Dutch spoken in Flanders?'}, '5725be0738643c19005acc42': {'truth': 'Hollands', 'predicted': 'West-Vlaams \"Western Flemish', 'question': 'What is the Dutch name for the \"Hollandish\" dialect of the language?'}, '5730878f2461fd1900a9ce92': {'truth': 'Poliane', 'predicted': 'Finnic Chud tribe', 'question': 'Which tribe resided in the south?'}, '572759cb5951b619008f888d': {'truth': 'rayon', 'predicted': 'Nylon', 'question': 'What was the first manufactured fiber?'}, '5732a3dfcc179a14009dabc3': {'truth': 'Cretaceous–Paleogene extinction event', 'predicted': 'the Cretaceous–Paleogene', 'question': 'Which extinction marked the beginning of the Cenozoic Era?'}, '56d8dc9cdc89441400fdb351': {'truth': 'Taksim Square', 'predicted': 'Sultanahmet Square', 'question': 'Where did the torch end up in Istanbul?'}, '571aeaf69499d21900609ba9': {'truth': 'different theological views', 'predicted': 'Scholars now believe that the Arian Party was not monolithic', 'question': 'Did all Arians believe the same things?'}, '571aeaf69499d21900609bad': {'truth': 'not monolithic', 'predicted': 'They supported the tenets of Origenist thought and theology', 'question': 'Did Arians have one set of beliefs?'}, '5726da10dd62a815002e929c': {'truth': 'Polish nobles, teachers, social workers, priests, judges and political activists', 'predicted': 'Polish elites', 'question': 'Who were the “intelligentia?” '}, '5727a8874b864d19001639bc': {'truth': \"Switzerland's isolation from the rest of Europe\", 'predicted': 'isolation from the rest of Europe, Bern and Brussels signed seven bilateral agreements to further liberalise trade ties', 'question': 'What were the original bilateral agreements meant to minimize the negative consequences of?'}, '570a5e0e6d058f1900182dbb': {'truth': '2003', 'predicted': '2007', 'question': 'In which year were these allegations raised?'}, '572e7f0adfa6aa1500f8d04b': {'truth': 'King Cinyras, Teucer and Pygmalion', 'predicted': 'Aphrodite and Adonis', 'question': 'Cyprus is home to which Greek mythological figures?'}, '570b2117ec8fbc190045b844': {'truth': 'Germany', 'predicted': 'the United States', 'question': 'What European country is a leader in providing VRS services to its citizens?'}, '5705fcd775f01819005e783c': {'truth': 'Thomson', 'predicted': 'Rupert Murdoch', 'question': 'Who did a media magnate in the 1980s buy The Times from?'}, '56e0cfce7aa994140058e735': {'truth': 'Google', 'predicted': 'Mozilla', 'question': 'Which company pays Firefox to make their search engine the default on their browser?'}, '5706969952bb891400689ab5': {'truth': 'Indian ragas performed in a disco style', 'predicted': \"electronic instrumentation and minimal arrangement of Charanjit Singh's Synthesizing: Ten Ragas to a Disco Beat (1982), an album of Indian ragas\", 'question': \"What did Singh's album contain?\"}, '5706969952bb891400689ab7': {'truth': 'minimal arrangement', 'predicted': 'electronic instrumentation and minimal', 'question': 'What sort of arrangement did Charanjit Singh use on his 1982 album?'}, '572649865951b619008f6f20': {'truth': '1 January 1981', 'predicted': '1980', 'question': 'Greece joined what later became the European Union when?'}, '5728b8862ca10214002da65b': {'truth': 'feudal', 'predicted': 'Permanent Settlement', 'question': 'What type of land taxation system did the East India Company instigate in Bengal?'}, '570b09a96b8089140040f700': {'truth': 'STOBAR', 'predicted': '55,000 tonne', 'question': 'What type of carrier is Admiral Flota Sovetskovo Soyuza Kuznetsov?'}, '56e163afe3433e1400422e62': {'truth': 'the National Football League', 'predicted': 'Major League Soccer', 'question': 'What league do the new England patriots belong to?'}, '5726adf0dd62a815002e8cc5': {'truth': 'the landmark building', 'predicted': 'A Gothic cathedral or abbey', 'question': 'Prior to the 20th century, a Gothic cathedral was considered to be what type of building in the town in which it was constructed?'}, '56dfb89e7aa994140058e073': {'truth': 'horses', 'predicted': \"traveller's horse\", 'question': \"Aside from human beings, what creature's needs were traditionally seen to at inns?\"}, '5731eaa7e17f3d1400422548': {'truth': 'evolution of the Greek economy', 'predicted': 'the gradual development of industry and further development of shipping in a predominantly agricultural economy', 'question': 'What does recent research from 2006 examine?'}, '572786b5dd62a815002e9f89': {'truth': 'Darwin always finished one book before starting another', 'predicted': 'writing his \"big book\" would take five years', 'question': \"What was Darwin's process on writing his books?\"}, '57266970f1498d1400e8dedf': {'truth': 'heel', 'predicted': 'a top face', 'question': 'What did Hulk Hogan become? '}, '5726e48b708984140094d500': {'truth': 'as the dielectric', 'predicted': 'insulators', 'question': 'For what use were non conductive materials used in the first capacitors?'}, '5726e48b708984140094d503': {'truth': 'as decoupling capacitors', 'predicted': 'telephony', 'question': 'What other use did paper capacitors serve in the telecommunications industry?'}, '57317c50a5e9cc1400cdbfbf': {'truth': 'by a mosaic replica of this last painting', 'predicted': 'a mosaic replica of this last painting, the Transfiguration', 'question': \"How is Raphael portrayed in St. Peter's?\"}, '572a3a0b6aef0514001553a6': {'truth': 'The B-series', 'predicted': 'A-series', 'question': \"What is McTaggart's second series called?\"}, '5730ef4205b4da19006bcc62': {'truth': 'How can there be absence of sin where there is concupiscence (libido)?', 'predicted': 'sinless', 'question': 'What did the query starter believe to be the ultimate difficulty in accepting  the a virgin conception of Mary ?'}, '5726c3b1f1498d1400e8ea97': {'truth': 'Secretariat for non-Christians', 'predicted': 'Pontifical Council for Interreligious Dialogue', 'question': 'What organization did Paul VI create to address non believers by the church?'}, '571a10584faf5e1900b8a880': {'truth': 'around 100', 'predicted': '28', 'question': 'How many theater companies does Seattle have in residence?'}, '570c6506fed7b91900d45985': {'truth': \"'More than a club'\", 'predicted': 'freedom', 'question': 'What motto of the team Barcelona appealed to the Catalans?'}, '572844c5ff5b5019007da069': {'truth': 'which granted home rule to Ireland as a constituent country of the former United Kingdom of Great Britain and Ireland', 'predicted': 'Government of Ireland Act 1914 which granted home rule', 'question': 'What is Ireland Act 1914?'}, '56fdc60e19033b140034cd68': {'truth': '(clay spheres, cones, etc.)', 'predicted': 'clay spheres, cones, etc.) which represented counts of items', 'question': 'Calculi during the Fertile Crescent refers to what?'}, '56d3913859d6e41400146793': {'truth': 'one', 'predicted': 'Funeral March', 'question': 'How many instrumental works did Chopin give a descriptive name to?'}, '57291a3e1d04691400779037': {'truth': 'neither a Western German state nor part of one', 'predicted': 'Western Allies', 'question': 'Which state was West Berlin apart of?'}, '57291a3e1d04691400779039': {'truth': '1952', 'predicted': 'nine', 'question': 'How many states was Germany reduced to in 1952'}, '5730ed3ea5e9cc1400cdbaf1': {'truth': '10 years', 'predicted': 'Required attendance at school', 'question': 'What is the required education for males on Tuvalu?'}, '5726e07a5951b619008f8107': {'truth': 'the Fourth Phase Offensive', 'predicted': 'Gettysburg of the Korean War.', 'question': \"What is considered to be the the Korean War's equivalent to Gettysburg?\"}, '5726e07a5951b619008f8109': {'truth': 'more than 25,000', 'predicted': '15 to 1', 'question': 'How many PVA soldiers fought in this battle and lost?'}, '57266c865951b619008f7252': {'truth': 'Out of the 22.5%, the largest groups were 6.5% (1,213,438) Cuban', 'predicted': 'Latino', 'question': 'What Origin makes up most of the Hispanics in Florida '}, '57266c865951b619008f7254': {'truth': 'the second largest Puerto Rican population after New York, as well as the fastest-growing in the nation', 'predicted': '4.5%', 'question': 'What is percentage of Puerto Ricans in Florida '}, '57278cb9f1498d1400e8fbb4': {'truth': 'cases involving members of congress, senators', 'predicted': 'President and Vice-President of the Republic', 'question': 'Which legislative bodies does this court sit in cases over?'}, '5725d375ec44d21400f3d646': {'truth': '1839', 'predicted': '1809', 'question': 'When did construction of the fortress finish?'}, '5731a21de17f3d1400422298': {'truth': 'private benefactors', 'predicted': 'local governments', 'question': 'What type of entity created competition with government created universities?'}, '5709b165ed30961900e8442a': {'truth': 'Dollar', 'predicted': 'the dime', 'question': 'What was the last coin to be converted to the modern day style of having historic Americans on the face?'}, '572827843acd2414000df5b0': {'truth': 'bristles', 'predicted': 'hairs', 'question': 'What are setae?'}, '57325124e17f3d140042285c': {'truth': 'tolls were resented', 'predicted': 'led to the abandonment of tolls', 'question': 'Why did farmers build a bridge over the Harlem River?'}, '57325124e17f3d140042285d': {'truth': 'between New England and New York', 'predicted': 'New England and New York (Manhattan)', 'question': \"What strategic advantage did the Bronx's location have?\"}, '57276683dd62a815002e9c35': {'truth': 'they are exposed to this heat', 'predicted': 'When the boys are at work', 'question': 'Under glass making conditions were the children exposed to any heating elements?'}, '57276683dd62a815002e9c37': {'truth': 'factory owners preferred boys under 16 years of age', 'predicted': 'Many factory owners', 'question': 'Did the glass industry have a preference for older working boys?'}, '56de7b394396321400ee2959': {'truth': 'guano', 'predicted': 'maize', 'question': 'What animal byproduct was imported to Plymouth in the 19th century?'}, '572bc28a111d821400f38f78': {'truth': 'requires knowledgeable managers and engineers', 'predicted': 'knowledgeable managers and engineers who are able to operate new machines or production practices borrowed from the leader in order to close the gap through imitation', 'question': 'What is greatly needed with technology transfer when it relates to education?'}, '5723fc250dadf01500fa1fe4': {'truth': '17', 'predicted': '14', 'question': 'How old was Princess Victoria when she was married?'}, '57266b70708984140094c568': {'truth': '17', 'predicted': '14', 'question': \"How old was Victoria's oldest daughter when she was amrried?\"}, '57266b70708984140094c569': {'truth': 'liberalising influence in the enlarging Prussian state', 'predicted': 'would be a liberalising influence', 'question': 'What did Queen Victoria hope for the marriage between her daughter and Prince Frederick William?'}, '571a2b2410f8ca1400304f2d': {'truth': 'Like most', 'predicted': 'most parts of the United States, government and laws are also run by a series of ballot initiatives (allowing citizens to pass or reject laws', 'question': \"To which states' election laws are Seattle's law and ballots similar?\"}, '56e7b1d437bdd419002c437a': {'truth': 'Friday or Saturday', 'predicted': 'Sunday', 'question': 'What days were AFL games traditionally played on before the TV deal?'}, '573271ece17f3d1400422981': {'truth': 'National Security Council', 'predicted': 'Strategic Air Command', 'question': 'Along with the Joint Chiefs and SAC, what body was involved with formulating plans for nuclear war with China?'}, '5706b5fa0eeca41400aa0d72': {'truth': 'Haunted House', 'predicted': 'Deep Love', 'question': 'one voice records released a remix of what dada nada song in 1989?'}, '57294fa6af94a219006aa285': {'truth': 'United States Guantánamo Bay detention camp', 'predicted': 'Bermuda', 'question': 'Where were the Uyghurs transferred from?'}, '5726b8addd62a815002e8e26': {'truth': 'a dialogue', 'predicted': 'contact with all people', 'question': 'What did Paul VI want to keep open with the modern world and people from all walks of life?'}, '56f8d9db9b226e1400dd10e1': {'truth': 'Guinea', 'predicted': 'Senegal', 'question': 'What country is on the south border of Guinea-Bissau?'}, '56f8d9db9b226e1400dd10e4': {'truth': '13° and 17°W', 'predicted': '11° and 13°N', 'question': 'What longitudes does Guinea-Bissau mostly lie between?'}, '56e6df336fe0821900b8ec11': {'truth': 'boybands', 'predicted': 'power pops', 'question': 'What type of band are Backstreet Boys and Westlife?'}, '57301c4fb2c2fd1400568892': {'truth': \"the People's Redemption Council\", 'predicted': \"People's Redemption Council (PRC)\", 'question': 'The coup leaders later became known as?'}, '56bfb8dca10cfb140055127b': {'truth': 'man-tending anthems', 'predicted': 'female-empowerment', 'question': 'With Jay Z what were her new themes?'}, '56bfb8dca10cfb140055127c': {'truth': 'co-producing credits', 'predicted': 'co-writing', 'question': 'What does she get credits for in her music?'}, '56d4dd502ccc5a1400d832ae': {'truth': 'Women', 'predicted': 'Jay Z', 'question': \"Beyoncé's early recordings empowered who?\"}, '572a982b34ae481900deaba5': {'truth': 'Naval Reserve', 'predicted': 'Vietnam Veterans Against the War', 'question': 'What branch of the military did Kerry join?'}, '56ce5d70aab44d1400b886f7': {'truth': 'Active', 'predicted': 'demand side', 'question': 'Are supply side solar technologies generally active or passive?'}, '56ce5d70aab44d1400b886f8': {'truth': 'Passive', 'predicted': 'supply side', 'question': 'Are demand side solar technologies generally active or passive?'}, '56cfdf65234ae51400d9bfce': {'truth': 'designing spaces that naturally circulate air', 'predicted': 'photovoltaics, concentrated solar power, solar thermal collectors, pumps, and fans', 'question': 'What is an active solar technique used to generate energy?'}, '56cfdf65234ae51400d9bfcf': {'truth': 'increase the supply of energy', 'predicted': 'convert sunlight into useful outputs', 'question': 'What does an active solar technique do?'}, '56cfdf65234ae51400d9bfd0': {'truth': 'reduce the need for alternate resources', 'predicted': 'convert sunlight into useful outputs', 'question': 'What does a passive solar technique do?'}, '572956496aef051400154d15': {'truth': 'Cenotaph in front of the Cabinet Building', 'predicted': 'The Cenotaph', 'question': \"What is the site for Bermuda's Remembrance Day?\"}, '56cd59a162d2951400fa652a': {'truth': 'iTunes', 'predicted': 'software', 'question': 'What kind program is commonly used to move files between iTunes and an iPod?'}, '56cfc4d6234ae51400d9bf4b': {'truth': 'iTunes', 'predicted': 'software that has been specifically designed to transfer media files to iPods', 'question': 'Rather than copying media files directly to it, what software must be used for this purpose so that they are accessible?'}, '570b26c7ec8fbc190045b88a': {'truth': 'Blades', 'predicted': 'Xbox 360 Dashboard', 'question': 'The tabs on the user interface were called what?'}, '56dfa2414a1a83140091ebe2': {'truth': '17%', 'predicted': '31%', 'question': 'What is the success rate for male Aeta hunters?'}, '57264dcd708984140094c1da': {'truth': 'between Crete and Turkey', 'predicted': 'southeast', 'question': 'The Dodecanese islands are located where?'}, '571ae53e9499d21900609b99': {'truth': 'after the death of Emperor Constantius', 'predicted': '361', 'question': 'When did Athanasius return to his position as Patriarch?'}, '572fffb8a23a5019007fcc2c': {'truth': 'Octavian', 'predicted': 'Imperator Caesar', 'question': 'Who has been designated as the first Emperor of Rome?'}, '56e75e1200c9c71400d77018': {'truth': 'Laidlines', 'predicted': 'laidlines', 'question': 'What type of lines does wove paper not exhibit?'}, '5725f1dc271a42140099d357': {'truth': 'horses', 'predicted': 'royal carriages', 'question': 'The Royal Mews houses which type of animal?'}, '572e6bacc246551400ce422c': {'truth': 'punk rock', 'predicted': 'new wave', 'question': 'What previous movement is post-punk often identified as coming after?'}, '57271739f1498d1400e8f386': {'truth': 'obesity', 'predicted': 'type 2 diabetes', 'question': 'Insulin resistance has been strongly linked to which health issue?'}, '56cd81df62d2951400fa6666': {'truth': 'Verité', 'predicted': 'Electronic Industry Code of Conduct Implementation Group', 'question': 'Who did Apple partner with to monitor its labor policies?'}, '5727b808ff5b5019007d935c': {'truth': 'law', 'predicted': 'arbitrary decisions of individual government officials', 'question': 'According to the rule of law, what should hold the determination for rules in a land? '}, '57293faa6aef051400154be6': {'truth': 'The first blacks to arrive in Bermuda in any numbers were free blacks from Spanish-speaking areas of the West Indies', 'predicted': 'distinctly different', 'question': 'Why is the black population in Bermuda different from that in the British West Indies and the United States?'}, '57293faa6aef051400154be7': {'truth': 'South-West Africa', 'predicted': 'the West Indies', 'question': 'Where did the Spanish and Portugese enslave most of their black people from?'}, '572f9c99a23a5019007fc7d6': {'truth': 'a newer variant', 'predicted': 'a three-terminal device', 'question': 'What does a letter at the end of a device number mean?'}, '5732321ce17f3d1400422719': {'truth': 'traditional religions', 'predicted': 'Hellenic', 'question': 'Besides the acceptance of Christianity, what other religious cults were tolerated?'}, '5732321ce17f3d140042271b': {'truth': 'Christian, Imperial, and \"divus\"', 'predicted': 'a Christian, Imperial, and \"divus', 'question': 'As what was Constantine honored when he died?'}, '571aeca132177014007e9fef': {'truth': 'Zyprexa, and the other involved Bextra', 'predicted': \"Eli Lilly's antipsychotic Zyprexa\", 'question': 'What drugs were involved in cases of the largest criminal fines?'}, '57313831497a881900248c72': {'truth': 'sea level rise', 'predicted': 'tropical cyclones', 'question': \"To what climate change condition does Tuvalu's low elevation make it susceptible?\"}, '572a470efed8de19000d5b65': {'truth': 'The Beinecke Rare Book and Manuscript', 'predicted': 'Beinecke Rare Book and Manuscript Library', 'question': 'Which museum feature the original copy of the Gutenberg Bible?'}, '56d0772c234ae51400d9c2f8': {'truth': 'Nidānakathā of the Jataka tales of the Theravada', 'predicted': 'Buddhaghoṣa', 'question': 'The Nidānakathā of the Jataka tales of the Theravada is attributed to who?'}, '56d0772c234ae51400d9c2fa': {'truth': 'Most accept that he lived, taught and founded a monastic order', 'predicted': \"historical facts of the Buddha's life\", 'question': 'What do scholars recognize about the life of the Buddha?'}, '572804792ca10214002d9b9f': {'truth': 'both are elements of a systematic mental framework', 'predicted': 'an a priori intuition that allows us (together with the other a priori intuition, space) to comprehend sense experience. With Kant, neither space nor time are conceived as substances', 'question': 'What did Kant portray space and time to be?'}, '570d5aabfed7b91900d45f07': {'truth': 'North Africa', 'predicted': 'Kingdom of Aragon', 'question': 'Where did the Moriscos go when they were forced out of Spain?'}, '56f8a3aa9b226e1400dd0d25': {'truth': 'unknown', 'predicted': 'the decrease of glaciated areas combined with a succession of winters with lower-than-expected precipitation may have a future impact on the rivers in the Alps as well as an effect on the water availability to the lowlands', 'question': 'What are the effects of diverting the water from rivers?'}, '56e7909700c9c71400d772e4': {'truth': 'Yuan Shikai', 'predicted': 'Sun Yat-sen', 'question': 'Who moved the capital from Nanjing to Beijing?'}, '56dd1e8366d3e219004dabdd': {'truth': 'foreign workers', 'predicted': 'Catholics', 'question': 'Who form the majority of Islamic residents of the Congo?'}, '572fc623947a6a140053cc97': {'truth': 'Colonel General Yuri Khatchaturov', 'predicted': 'the President of Armenia, Serzh Sargsyan', 'question': 'Who is in charge of the the Armenian military?'}, '572ebac003f98919007569af': {'truth': 'The Rocky Mountain', 'predicted': 'Rocky Mountain region', 'question': 'In the US, which region is the highest by elevation?'}, '56fb2e3bf34c681400b0c1f9': {'truth': 'heresy', 'predicted': 'interstate conflict, civil strife, and peasant revolts', 'question': 'Along with controversy and schism, what upset the peace of the Church during the Late Middle Ages?'}, '5733f165d058e614000b663b': {'truth': 'former colonies and territories', 'predicted': 'those countries', 'question': 'Portuguese law continues to be a major influence for what?'}, '5733f165d058e614000b663c': {'truth': 'a civilian police force who work in urban areas', 'predicted': 'the Polícia de Segurança Pública', 'question': 'What is the Policia de Seguranca Publica - PSP (Public Security Police)?'}, '56f7c779aef2371900625c0b': {'truth': 'możny', 'predicted': 'magnates', 'question': 'What is another name referring polish nobles?'}, '5723d010f6b826140030fc8d': {'truth': 'chronic stomach trouble', 'predicted': 'stomach trouble. In August, Victoria and Albert visited their son, the Prince of Wales, who was attending army manoeuvres near Dublin, and spent a few days holidaying in Killarney. In November, Albert was made aware of gossip that his son had slept with an actress in Ireland. Appalled, Albert travelled to Cambridge, where his son was studying, to confront him. By the beginning of December, Albert was very unwell. He was diagnosed with typhoid fever', 'question': 'What illness was Albert suffering from while he helped Victoria through her grief?'}, '57257e8fcc50291900b28537': {'truth': 'typhoid fever', 'predicted': \"worry over the Prince of Wales's philandering\", 'question': \"What caused Prince Albert's death?\"}, '57257e8fcc50291900b28538': {'truth': 'their son, the Prince of Wales', 'predicted': 'Conroy and Lehzen', 'question': \"Who did Victoria blame for Prince Albert's death?\"}, '5726bd86f1498d1400e8e9b2': {'truth': 'semantic indicator', 'predicted': 'one of a limited set of characters', 'question': 'What suggests the general meaning of a compound character? '}, '5726593a5951b619008f7037': {'truth': 'universal compulsory', 'predicted': 'compulsory military service', 'question': 'What type of military service does Greece require?'}, '57280c3f3acd2414000df310': {'truth': 'The London Fire Brigade', 'predicted': 'London Fire and Emergency Planning Authority', 'question': 'What agency provides fire fighting and rescue service in London?'}, '572ac792111d821400f38d5f': {'truth': 'turn over every single bit of his chemical weapons to the international community in the next week', 'predicted': \"Turn it over, all of it, without delay, and allow a full and total accounting for that. But he isn't about to do it\", 'question': 'What did Kerry say Syria could do to avoid a military strike?'}, '57307352069b5314008320ed': {'truth': 'World Council of Hellenes Abroad put the figure at around 7 million worldwide', 'predicted': '3 million', 'question': 'How many Greeks do they believe would be an accurate number for census numbers ?'}, '572cb837750c471900ed4cf4': {'truth': '103.5 million', 'predicted': '272,795', 'question': 'How many new cases were filed in 2010?'}, '56dc544814d3a41400c267c1': {'truth': 'RNA', 'predicted': 'DNA', 'question': 'What constitutes the viral genome?'}, '57283f892ca10214002da184': {'truth': '$5.00 per two-sided disc', 'predicted': '$1.00', 'question': 'How much did LaserDiscs cost to produce by the end of the 1980s?'}, '5729355b1d0469140077916e': {'truth': 'regional ancestry', 'predicted': 'Race\" is still sometimes used within forensic anthropology (when analyzing skeletal remains), biomedical research, and race-based medicine. Brace has criticized this, the practice of forensic anthropologists for using the controversial concept \"race', 'question': 'What term would Brace prefer forensic anthropologists use?'}, '5726f52bdd62a815002e9646': {'truth': 'bone marrow transplant', 'predicted': 'successfully carried out this surgery', 'question': 'Nigeria was the second African country to perform which medical procedure?'}, '57095defed30961900e84005': {'truth': 'ships', 'predicted': 'aquaculture', 'question': \"Because of copper's biostatic properties where is a common use for copper?\"}, '57279c33708984140094e238': {'truth': 'his judgment', 'predicted': 'a similar problem', 'question': 'What was one of the criticisms Bowers faced after editing Maggie?'}, '5728cbea3acd2414000dfeac': {'truth': 'Detroit techno', 'predicted': 'science fiction', 'question': 'What genre of music featured robotic themes?'}, '5728cbea3acd2414000dfeae': {'truth': 'Memorial Day Weekend', 'predicted': 'late May', 'question': 'When does \"Movement\" occur?'}, '572a8990f75d5e190021fb56': {'truth': 'Deobandis', 'predicted': 'the Barelvis', 'question': 'What group makes up a larger percentage of people in India?'}, '572a8990f75d5e190021fb58': {'truth': 'Mathematics, Computers and science', 'predicted': 'Quranic education', 'question': 'What disciplines does India want to introduce to madaris?'}, '57277e4edd62a815002e9eb4': {'truth': 'southern', 'predicted': 'North Brabant and Limburg', 'question': 'In what provinces is the Carnival mainly celebrated in the Netherlands?'}, '570e432f0dc6ce1900204ee5': {'truth': 'Gulf War Syndrome', 'predicted': 'politically and environmentally contentious', 'question': 'What illness is possibly tied to the use of depleted uranium munitions?'}, '5732549b0fdd8d15006c69c8': {'truth': 'biographer', 'predicted': 'political education', 'question': 'What was Blanche Wiesen Cook in relation to Eisenhower?'}, '5727972a708984140094e1a7': {'truth': 'International Program on the Elimination of Child Labour (IPEC)', 'predicted': 'setting the international law', 'question': 'What did the United Nations take charge of with regards to child labour?'}, '5727972a708984140094e1a8': {'truth': 'progressively eliminate child labour', 'predicted': 'strengthening national capacities', 'question': 'What is the aim of this?'}, '56dcfc7b66d3e219004dab7c': {'truth': 'pets', 'predicted': 'property', 'question': 'The treatment of Pygmies has been compared to the treatment of what?'}, '5726872c5951b619008f75cd': {'truth': \"Aung San Suu Kyi's party won a majority in both houses, ending military rule.\", 'predicted': \"improved the country's human rights record and foreign relations, and has led to the easing of trade and other economic sanctions\", 'question': 'Has the country been able to overcome the problems of government with the previous regime?'}, '57279c1aff5b5019007d90eb': {'truth': 'ecclesiastical', 'predicted': 'monastic', 'question': 'What kind of settlement was Cork?'}, '5723df4df6b826140030fcd0': {'truth': 'Masonic communication', 'predicted': 'inter-visitation', 'question': 'What, besides Recognition, must happen between two Grand Lodges in order for them to be considered in amity? '}, '56de5ba04396321400ee284f': {'truth': '1960s', 'predicted': '1956', 'question': 'In what decade did colleges of technology gain the University designation?'}, '570a6f996d058f1900182e5c': {'truth': 'stoic', 'predicted': 'Aristotle', 'question': 'What school of thought saw emotion as an impediment to virtue?'}, '572f6c2cb2c2fd14005680f8': {'truth': 'Khilji dynasty', 'predicted': 'Malik Kafur', 'question': 'Which entity subsumed the Kakatiya dynasty?'}, '571ae71a32177014007e9fc6': {'truth': 'those based on re-formulation of an existing active ingredient', 'predicted': 'approved drugs', 'question': 'What drugs are the least expensive to develop?'}, '5726fc5d5951b619008f8412': {'truth': '32', 'predicted': '26', 'question': 'How old was Joséphine de Beauharnais when she was married to Napoleon?'}, '572f6eacb2c2fd1400568109': {'truth': 'on a swing arm', 'predicted': 'the bottom of the polycarbonate layer', 'question': 'Where is the semiconductor laser found in a CD player?'}, '572f6eacb2c2fd140056810a': {'truth': '780 nm', 'predicted': 'photodiode', 'question': 'What wavelenght is used to pull data from a CD?'}, '5735b062dc94161900571f26': {'truth': 'five-star', 'predicted': 'Hyatt Regency', 'question': \"De L'Annapurna is an example of what sort of hotel?\"}, '573236d2e17f3d1400422737': {'truth': 'non-Christian practices', 'predicted': 'an Augustan form of principate', 'question': 'What did Julian try to restore to the empire?'}, '57272cf65951b619008f8691': {'truth': 'federal', 'predicted': 'black citizens', 'question': \"Who's authority did Truman want to increase throughout the states?\"}, '572fb2fb947a6a140053cbad': {'truth': 'Yerevan', 'predicted': 'Tsitsernakaberd hill', 'question': 'Where is the memorial for the Armenian Genocide?'}, '570d7be1b3d812140066d9df': {'truth': '500 people', 'predicted': 'c.\\u2009500', 'question': 'How many were killed by the Communards?'}, '570629ba52bb891400689918': {'truth': 'Digital Audio Tape (DAT) SP', 'predicted': 'Compact Disc', 'question': 'Other than CD parameters, what else can be used as parameter references?'}, '5730b108069b531400832269': {'truth': 'nine islands', 'predicted': 'Eight', 'question': 'How many islands are in the Tuvalu group?'}, '56d9b546dc89441400fdb713': {'truth': 'run away', 'predicted': '52%', 'question': 'When these feral dogs are approached by a person, they tend to do this 52% of the time?'}, '572805f5ff5b5019007d9b1c': {'truth': 'the large restriction on possible patterns', 'predicted': 'without relying on the BOM', 'question': 'Why is it possible to distinguish UTF-8 from other protocols?'}, '5726ec2bdd62a815002e955a': {'truth': 'Chinese troops', 'predicted': 'far greater', 'question': 'Did the UN troops or Chinese troops experience more war casualties?'}, '5726ec2bdd62a815002e955c': {'truth': \"discuss the PVA's logistical problems\", 'predicted': 'accelerate the construction of railways and airfields in the area', 'question': 'What was the purpose of the Shengyang meeting?'}, '56e7860900c9c71400d77235': {'truth': 'historical documents', 'predicted': 'Nanjing was the international hub of East Asia', 'question': 'Where did the information on registered households during that period originate?'}, '56fb7c108ddada1400cd645b': {'truth': 'Italy', 'predicted': 'Europe', 'question': 'In what region was gold coinage first reintroduced?'}, '57313a17497a881900248c8f': {'truth': 'Outer Mongolia', 'predicted': 'Lhasa', 'question': 'Where did Kangxi lead an army?'}, '56dfb0e97aa994140058dfe7': {'truth': 'metal bed frame', 'predicted': 'disturbed the instrument', 'question': 'What did Bell think was wrong with the bed, which prevented his machine from finding the bullet?'}, '56e141e2e3433e1400422d06': {'truth': 'Italians', 'predicted': 'Irish descent', 'question': 'What is the second largest ethnic group in the city?'}, '56e141e2e3433e1400422d07': {'truth': '8.3%', 'predicted': '15.8%', 'question': 'What percentage of the citys population is italian?'}, '56e06d44231d4119001ac105': {'truth': 'Muddy consonants', 'predicted': 'muddy consonants', 'question': 'What is /b/ representative of, in addition to aspirated and unaspirated consonants?'}, '570b3f0fec8fbc190045b913': {'truth': '1,200', 'predicted': '2,000', 'question': 'How many troops did the US initially send to the Philippines?'}, '56fb2c94f34c681400b0c1ec': {'truth': 'the Medieval Warm Period', 'predicted': 'Medieval Warm Period climate change', 'question': 'What event led to larger crop yields in the High Middle Ages?'}, '5726ee62dd62a815002e9584': {'truth': 'the 18th century', 'predicted': '18th century through late 20th century', 'question': 'What time period did the history of science begin to take a progressive narrative?'}, '570ffc01a58dae1900cd67af': {'truth': 'more appropriately measured as independent concepts on a separate scale rather than as a single continuum', 'predicted': 'concepts of masculinity and femininity are more appropriately measured as independent concepts on a separate scale', 'question': 'What did the research performed in the 1970s show about masculinity and feminity?'}, '570ffc01a58dae1900cd67b1': {'truth': 'the degree of heterosexual and homosexual can be independently determined,', 'predicted': 'inappropriately measures heterosexuality and homosexuality on the same scale', 'question': 'What is another benefit of measuring sexuality on two scaless verses just the Kinsey scale?'}, '5728c1414b864d1900164d5b': {'truth': 'Iatromantis', 'predicted': 'Manticus', 'question': 'Which epithet did Apollo have as god of healing and of prophecy?'}, '5726a1d5708984140094cc67': {'truth': 'Philip Magnus', 'predicted': 'Robert Murray', 'question': 'Who wrote a biography of Burke?'}, '57320ba7e99e3014001e6478': {'truth': 'plebeian', 'predicted': 'artisan', 'question': 'At the end of the regal period, what class was kept out of the state political and priesthood arenas?'}, '57320ba7e99e3014001e6479': {'truth': 'neighbours', 'predicted': 'Etruscan', 'question': 'With whom did Rome have alliances at the end of the regal period?'}, '57326cefe99e3014001e67a7': {'truth': 'comedy', 'predicted': 'Italian-American', 'question': \"What genre was 'True Love'?\"}, '57326cefe99e3014001e67a8': {'truth': 'Annabella Sciorra and Ron Eldard', 'predicted': 'Ted Danson', 'question': \"Who starred in 'True Love'?\"}, '572805304b864d1900164251': {'truth': '1963 from 12 May to 29 June', 'predicted': '12 November 1962', 'question': 'When was the next session scheduled?'}, '572fefcb947a6a140053ce31': {'truth': 'northern Europe', 'predicted': 'France', 'question': 'Where did the tribes that were almost annihilated in the Battle of Vercellae hail from?'}, '56d11b8d17492d1400aab997': {'truth': 'Her father', 'predicted': 'lawyer', 'question': 'Lee modeled the character Atticus after what laywer?'}, '572814c34b864d1900164422': {'truth': 'Devil May Cry 4', 'predicted': 'Metal Gear Solid 4: Guns of the Patriots', 'question': 'Which much anticipated third-party game with the name of a month of the year in it did Sony show at E3 2007?'}, '57303815947a6a140053d2c7': {'truth': 'the Golrizan Festival', 'predicted': 'Tehran World Festival', 'question': 'What Iranian film festival in 1954 was the progenitor of future film festivals in 1969 and 1973?'}, '57303815947a6a140053d2c9': {'truth': '65', 'predicted': '25', 'question': 'How many commercial films were produced yearly on average by the end of the 1960s in Iran?'}, '570e08690dc6ce1900204d9b': {'truth': 'important food', 'predicted': 'keystone species of the ecosystem of the Southern Ocean', 'question': 'Why is the krill so important to the Antarctic area?'}, '572771b85951b619008f8a08': {'truth': 'Tetraploid', 'predicted': 'cultivated cotton', 'question': 'What type of cotton has two separate genomes within its nucleus? '}, '572771b85951b619008f8a0b': {'truth': 'diploid counterparts', 'predicted': 'A and D genomes', 'question': 'In order to understand the tetraploid forms, what must be used as a comparison in cotton gene sequencing?'}, '573036c4947a6a140053d2b3': {'truth': 'one direction', 'predicted': 'multiple wavelengths long', 'question': \"How long are the wire antenna's that the voltage and current waves travel in the same direction?\"}, '5728d7c4ff5b5019007da7f7': {'truth': 'the lowest income communities', 'predicted': 'children and adults', 'question': 'Who was more likely to seek hospital help in the US for asthma reasons?'}, '5728283a2ca10214002d9f73': {'truth': 'unjointed paired extensions of the body wall', 'predicted': 'limbs', 'question': 'What are parapodia?'}, '56f71a5e3d8e2e1400e3735d': {'truth': 'Tito', 'predicted': 'Milan Gorkić', 'question': 'Who became Secretary-General of the CPY after the prior one was murdered?'}, '570cff2cb3d812140066d396': {'truth': 'Luke', 'predicted': 'John', 'question': \"Which Gospel writer provided a version of the virgin birth that was different than Matthew's?\"}, '57264be9dd62a815002e80bd': {'truth': 'The lights can be switched on for 24-hrs/day, or a range of step-wise light regimens to encourage the birds to feed often and therefore grow rapidly', 'predicted': 'controlled', 'question': 'What type of conditions are used to increase the weight and profitability of commercial turkeys?'}, '57270821708984140094d8d5': {'truth': 'childhood obesity', 'predicted': 'lifelong healthful eating patterns and physically active lifestyles for children and their families', 'question': 'What does the initiative specifically target?'}, '570ff9cda58dae1900cd679e': {'truth': '53', 'predicted': '6 percent', 'question': 'In 2010, how many death sentences were overturned due to reversals from courts or appeals?'}, '5728f9a14b864d190016515d': {'truth': 'Yangon Stock Exchange Joint Venture Co. Ltd', 'predicted': 'Yangon Stock Exchange (YSX)', 'question': 'What is the name of the business that first rang a bell to begin in the winter of 2014 in Myanmar ?'}, '5728f9a14b864d190016515e': {'truth': 'Myanma Economic Bank sharing 51 percent', 'predicted': 'First Myanmar Investment Co., Ltd', 'question': 'W is set to to be the major stock holder of the business that first rang a bell to begin in the winter of 2014 in Myanmar ?'}, '5728f9a14b864d190016515f': {'truth': \"Japan's Daiwa Institute of Research Ltd 30.25 percent and Japan Exchange Group 18.75 percent.\", 'predicted': 'Myanmar signed an agreement to set up its first stock exchange', 'question': 'Did other countries actively participate in business that first rang a bell to begin in the winter of 2014 in Myanmar ?'}, '56dfb914231d4119001abd06': {'truth': 'alcohol', 'predicted': 'meals', 'question': 'What is the main provision that pubs offer?'}, '5726dcbf708984140094d3fc': {'truth': '22,495,187', 'predicted': '12,214,853', 'question': 'How many votes did Goodluck get in 2011?'}, '56e83bdf37bdd419002c44bb': {'truth': 'être', 'predicted': 'avere', 'question': 'What French word is similar to the Italian word \"essere\"?'}, '56f7165e3d8e2e1400e37339': {'truth': 'Slovene', 'predicted': 'Croat', 'question': \"What ethnicity was Broz's mother?\"}}}\n"
          ]
        }
      ],
      "source": [
        "import sklearn\n",
        "result, text= model.eval_model(randomtest, acc=sklearn.metrics.accuracy_score, verbose= True)\n",
        "print(result)#contains evaluation result\n",
        "print(text)#a dict containing the the correct_text, similar_text and incorrect text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "526aWiKBEcbG"
      },
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "IS3WVykoJ1sv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "6f90928ef69b40a5a209edecba3aa3cf",
            "eec117dbdbe14429968905749d842926",
            "6c4c8a7645a1469cbe7dc9018716be00",
            "f8872b4a2bea422e8cc04a4b344fd9de",
            "8ea0a2688687437a9591b297effb2281",
            "e4de373b692d4cf4aa9c34e933462124",
            "6072f906d90a436e8beeac084b5c3c9a",
            "68e69d988f3f4f44a389ef9c3d480d28",
            "c150cfe30c4f4c488fee984fd20b8f2b",
            "a4540c3fd664488a8027e5860db77427",
            "f9530f2ed6c4436f915060f31426d469"
          ]
        },
        "outputId": "becbec47-d0e4-47b4-d427-ed5607ba9f95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "convert squad examples to features: 100%|██████████| 11873/11873 [01:24<00:00, 140.24it/s]\n",
            "add example index and unique id: 100%|██████████| 11873/11873 [00:00<00:00, 694575.39it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running Prediction:   0%|          | 0/1540 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f90928ef69b40a5a209edecba3aa3cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '56ddde6b9a695914005b9628', 'answer': ['France']}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "import sklearn\n",
        "dev = [item for topic in dev['data'] for item in topic['paragraphs'] ]\n",
        "predtn1, raw_outputs = model.predict(dev, n_best_size=2)#return a list of dict+ containg each question mapped to its answer and a list of dicts of question id mapped to probability score of the answer\n",
        "\n",
        "predtn1[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "BAVk2q0qd0tJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cccd235-d258-4722-86d3-d80e66240c16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '56ddde6b9a695914005b9628', 'probability': [0.9999919865006011]}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "raw_outputs[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_text = \"Linear regression is used for predicting quantitative values, such as an individual’s salary. In order to predict qualitative values, such as whether a patient survives or dies, or whether the stock market increases or decreases, Fisher proposed linear discriminant analysis in 1936.\"\n",
        "predtn2, raw_outputs = model.predict(\n",
        "    [\n",
        "        {\n",
        "            \"context\": context_text,\n",
        "            \"qas\": [\n",
        "                {\n",
        "                    \"question\": \"Who proposed linear discriminant analysis?\",\n",
        "                    \"id\": \"0\",\n",
        "                }\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "n_best_size=2)"
      ],
      "metadata": {
        "id": "eR4RFLAMeajY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "a09cc3c521964d2aa3dd97d65a1614d6",
            "8e83b175a1ba4cf6b2bb19c93b88a599",
            "50af2b78f2de4438a2ce4fa5c1940b9f",
            "4895f07c097744ce8df0dea6dbf7186d",
            "b54bfdc879f54c1c91918e17db35a769",
            "18319a74c98b43999f1924d97764b363",
            "76eacced6229481bba7296347b4ae869",
            "b11fe922afdf40fbb4cd4984799fb7ad",
            "21b1923aad424794bb7033f8930d81c1",
            "9204f250440c4091ac001dcaee7bfbdd",
            "52a3e89247844acdbe02cdd660a38b09"
          ]
        },
        "outputId": "112442fc-c1e7-4e7b-ac68-9070407fea6a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 67.94it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 9822.73it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a09cc3c521964d2aa3dd97d65a1614d6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "umrczR-m9s0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db16caab-a831-4b70-cc8d-7badad271f7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': '0', 'answer': ['Fisher']}]\n",
            "[{'id': '0', 'probability': [0.6566582705381819]}]\n"
          ]
        }
      ],
      "source": [
        "print(predtn2)#return a list of dict containg each question mapped to its answer\n",
        "print(raw_outputs)#a list of dicts of question id mapped to probability score of the answer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_text = \"Thomas Alva Edison was an American inventor and businessman who has been described as America's greatest inventor.One of his inventions, is the phonograph\"\n",
        "predtn3, raw_outputs = model.predict(\n",
        "    [\n",
        "        {\n",
        "            \"context\": context_text,\n",
        "            \"qas\": [\n",
        "                {\n",
        "                    \"question\": \"Who invented phonograph?\",\n",
        "                    \"id\": \"0\",\n",
        "                }\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "n_best_size=2)"
      ],
      "metadata": {
        "id": "cm4szUPpedFu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "98cf748a7b0444bf9e9c76e0ecbbdde3",
            "e20fdc46b0b74ddb9302ef77c078e18d",
            "cf881d5e54ab4d6ab65f42baa27ebecd",
            "82e3c92a97be4ed8a5631ab9a907a52e",
            "a9ff7f042eeb4e84a13de20f31ecea47",
            "2830ea1c89e04a0f8d4ab013bb8143c4",
            "6ed157457cd34a2fb76e66d6044d3481",
            "ba7f0fa5f3f8493597a8450152800834",
            "f7b2cf11fad843d3857a45e86b1e9db1",
            "99bf3079226c4ce2a65e2f1addd4c210",
            "caa8de8f14784a49ae66a743f28e4bc7"
          ]
        },
        "outputId": "2588f2db-0e8f-454b-8c49-46fff9c9633a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 148.61it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 10407.70it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98cf748a7b0444bf9e9c76e0ecbbdde3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "dGl8u9Sq0Osh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "746b1bba-67eb-4077-9944-f7f088f4df4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': '0', 'answer': ['Thomas Alva Edison']}]\n",
            "[{'id': '0', 'probability': [0.9997089142587192]}]\n"
          ]
        }
      ],
      "source": [
        "print(predtn3)#return a list of dict containg each question mapped to its answer\n",
        "print(raw_outputs)#a list of dicts of question id mapped to probability score of the answer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_text = \"Mary drove for 3 hours to work in the morning and 5 hours in the evening to her house\"\n",
        "predtn4, raw_outputs = model.predict(\n",
        "    [\n",
        "        {\n",
        "            \"context\": context_text,\n",
        "            \"qas\": [\n",
        "                {\n",
        "                    \"question\": \"How many hours did Mary drive in the morning?\",\n",
        "                    \"id\": \"0\",\n",
        "                }\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "n_best_size=2)\n",
        "print(predtn4)\n",
        "print(raw_outputs)"
      ],
      "metadata": {
        "id": "lemYcTEyegWh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "e20d25db28a345a4b315c67d254650bc",
            "cd350df69b854725b9c6bce503209bb5",
            "48518da9fe464db29b7d091fbfd05df6",
            "27a0309b731c40c6a09e425a79bec030",
            "274ecc178f454276bdd9a0a7acbd1f81",
            "2710f27b21974ac99f6364bfce66bde7",
            "4420ec4db41447f2ac478c5c00605302",
            "33b05b0a5a7a4a089e7d10c941e4e841",
            "fdca0330f11944d3be3dd33d6bd8b9f2",
            "e96f163c9ad84fb0a9751e088894e6a6",
            "849d452ef33448bda33ec02eeeb90f2e"
          ]
        },
        "outputId": "37f57533-7b4d-481c-9b9b-15c35d95c8fe"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 1908.24it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 8577.31it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e20d25db28a345a4b315c67d254650bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': '0', 'answer': ['3', '3 hours']}]\n",
            "[{'id': '0', 'probability': [0.5547302605573938, 0.4440005177735457]}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_text=\"I like to eat apples\"\n",
        "predtn4, raw_outputs = model.predict(\n",
        "    [\n",
        "        {\n",
        "            \"context\": context_text,\n",
        "            \"qas\": [\n",
        "                {\n",
        "                    \"question\": \"what do i like to eat?\",\n",
        "                    \"id\": \"0\",\n",
        "                }\n",
        "            ],\n",
        "        }\n",
        "    ], n_best_size=2\n",
        ")"
      ],
      "metadata": {
        "id": "T2LvlQmpo_EU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "2f69ff569ff2487f9578e9eee96b47db",
            "77809fffc6a54aa8b75463fe1521e659",
            "e2bc63cbec0c49eeaafc89eaa03e2237",
            "db768356ea954ccf832d18cc9b01b15f",
            "20c71a0c22104b5e877c8fb9ae405c27",
            "a1fa40c95e604589a8fac61d4c4d8a4f",
            "d7b74ed3f770448191bba4d3ff824253",
            "73c05b52847a4c77869210b76ee09c1e",
            "37c6387f55b64b9a997c669b7a870aae",
            "6593f2ff20684b2b955f05908990338d",
            "393c07c8729644649ce0ea4b242bf65b"
          ]
        },
        "outputId": "c20a4383-9414-4ae3-9723-9437519aa222"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 68.08it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 10699.76it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f69ff569ff2487f9578e9eee96b47db"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "jca_0BLt0c7D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f907ccae-c5c6-4c05-c86c-1d5819b890b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': '0', 'answer': ['apples']}]\n",
            "[{'id': '0', 'probability': [0.9996132796245574]}]\n"
          ]
        }
      ],
      "source": [
        "print(predtn4)#return a list of dict containg each question mapped to its answer\n",
        "print(raw_outputs)#a list of dicts of question id mapped to probability score of the answer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "efecfbd613514644bea92e13cd8dc0db",
            "089c2f8653dc45189e4ba8cd31924dfd",
            "6ec38247b8224a6e923b6c206fc4e7d3",
            "bef5d34a129c4261b3a8381281b4b4ec",
            "1eb7a89fe25e44898492a56b6a18764c",
            "5d616323fcb24a4fbcc3547c31f7fa52",
            "fbdcb801ec9442de83d8d2707fd13c8f",
            "f8556bd1578248ad86a9cbb99c595a57",
            "eb6917b62bdc41d397ff6cbb0d25a1e9",
            "a35c1a71a39948909eeb1ccebfcd7596",
            "ba7ffc4efd664920aa7b7f822521780d"
          ]
        },
        "id": "L7Mw15XjIn4g",
        "outputId": "5d943d83-4057-4fa5-8b87-d283e78cd0ad"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 20.92it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 9177.91it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efecfbd613514644bea92e13cd8dc0db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': '0', 'answer': ['purple', 'purple color']}]\n",
            "[{'id': '0', 'probability': [0.7864244169599528, 0.2133233559024132]}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "7FDAWS_i4G3x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "070c2771cb664c8fb839bdbd0899c820",
            "bc7f564d79b342a99caf778a22b1daae",
            "d1840ee4b0ad47259cd36ae27827900e",
            "7776613a85f348b2b060fcb82f73a3ca",
            "8b49aade44034f019d196baf626af470",
            "8c3d41a0d2da4e7b91798c96b35353fd",
            "02da86c48af049baac5b3aad0e119ed9",
            "659a265b18434d02876baf86852b5ab4",
            "e46d09b026d643f9a1e6bf73f06b3cb4",
            "22d5bac3f75f4d9bbb60026998e880f4",
            "67c3e45f310a4fb89a7b828432cb14aa"
          ]
        },
        "outputId": "6d397afe-6882-4d0a-c2d1-1631ccbc5444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.37.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.10/dist-packages (2.6.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardx) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardx) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardx) (3.20.3)\n",
            "Requirement already satisfied: simpletransformers in /usr/local/lib/python3.10/dist-packages (0.64.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.66.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2023.6.3)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.35.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.2.2)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.2.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.14.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.5.3)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.15.0)\n",
            "Requirement already satisfied: wandb>=0.10.32 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.16.0)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.28.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.1.99)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (0.19.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (6.0.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (0.4.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (3.1.40)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (1.37.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (3.20.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2023.7.22)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (3.8.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (3.2.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit->simpletransformers) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (5.3.2)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (6.8.0)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (9.4.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (13.7.0)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.5.0)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (5.2)\n",
            "Requirement already satisfied: validators<1,>=0.2 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (0.22.0)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (0.8.1b0)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (6.3.2)\n",
            "Requirement already satisfied: watchdog>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (3.0.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.59.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (3.5.1)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (3.0.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.12.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.11)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit->simpletransformers) (3.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.31.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.13.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->simpletransformers) (3.2.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 7810.62it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 7913.78it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "070c2771cb664c8fb839bdbd0899c820"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': '0', 'answer': ['Assam and Meghalaya', 'Assam']}]\n",
            "[{'id': '0', 'probability': [0.9330708030500986, 0.06680395834719555]}]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "!pip install wandb  #monitor training in realtime\n",
        "!pip install transformers  #make computer understand how humans understand language\n",
        "!pip install seqeval  #used for sequence labelling evaluations,, evaluates NLP\n",
        "!pip install tensorboardx #used for visualizing matrix through histograms or plots\n",
        "!pip install simpletransformers #used to quickly evaluate transformer model, uses SIMPLE one line code to train model\n",
        "\n",
        "from simpletransformers.question_answering import QuestionAnsweringModel\n",
        "\n",
        "\n",
        "paragraph = \"The year 1991 saw a devastating coastal flood incident that affected the Indian states of Assam and Meghalaya. The region had experienced heavy monsoon rains in the months leading up to the disaster, which caused the Brahmaputra River and its tributaries to swell, leading to massive flooding in the region. The flooding was further exacerbated by a tidal surge caused by a cyclonic storm that hit the Bay of Bengal. The combination of these factors resulted in a catastrophic disaster that had a significant impact on the lives of the people in the affected regions.\" #@param {type:\"string\"}\n",
        "question = \"Which states of India were affected by coastal flood\" #@param {type:\"string\"}\n",
        "\n",
        "algorithm = 'BERT' #@param{type:\"string\"}\n",
        "\n",
        "model = pickle.load(open('/content/drive/MyDrive/NLP/model.pkl', 'rb'))\n",
        "\n",
        "predtn5, raw_outputs = model.predict(\n",
        "    [\n",
        "        {\n",
        "            \"context\": paragraph,\n",
        "            \"qas\": [\n",
        "                {\n",
        "                    \"question\": question,\n",
        "                    \"id\": \"0\",\n",
        "                }\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "n_best_size=2)\n",
        "\n",
        "print(predtn5)\n",
        "print(raw_outputs)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "668037cc109149c2aa4abe052f9d4294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_483f14d05409436e82a42cb8de3d3170",
              "IPY_MODEL_d06a6fd3d18b452e9697458889090cd5",
              "IPY_MODEL_c2a6234013fd4d72b03b27d848be3899"
            ],
            "layout": "IPY_MODEL_8974f24207ec493684c1d72a1462d968"
          }
        },
        "483f14d05409436e82a42cb8de3d3170": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9212d4d5ed9840988f71290a32d77944",
            "placeholder": "​",
            "style": "IPY_MODEL_a3f0bb117c5c48c7ae7ccf2bd016d925",
            "value": "config.json: 100%"
          }
        },
        "d06a6fd3d18b452e9697458889090cd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d73b080d1aee47afa75e0eb414c6f7c7",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3bced5575fb40d1a26bd2f11120d389",
            "value": 570
          }
        },
        "c2a6234013fd4d72b03b27d848be3899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b67aabd0ac324737bf1dba4ddd5652e0",
            "placeholder": "​",
            "style": "IPY_MODEL_add678f153a14148935615c2d39591ce",
            "value": " 570/570 [00:00&lt;00:00, 23.6kB/s]"
          }
        },
        "8974f24207ec493684c1d72a1462d968": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9212d4d5ed9840988f71290a32d77944": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3f0bb117c5c48c7ae7ccf2bd016d925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d73b080d1aee47afa75e0eb414c6f7c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3bced5575fb40d1a26bd2f11120d389": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b67aabd0ac324737bf1dba4ddd5652e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "add678f153a14148935615c2d39591ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3760f99ad8c4e10bff230a64a472634": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c5ab5cb5f4d4477cb580b91b2188fd6e",
              "IPY_MODEL_191641813c504439b9cc610f7bd9fcf7",
              "IPY_MODEL_ec2c8848d8814c989e4714a031755bd9"
            ],
            "layout": "IPY_MODEL_c4cd9af8f68b49a385ccc9b0db8a4329"
          }
        },
        "c5ab5cb5f4d4477cb580b91b2188fd6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c825dee2f13a435aad2573a6476684f2",
            "placeholder": "​",
            "style": "IPY_MODEL_fbde67809a2542febf325435feae88f2",
            "value": "model.safetensors: 100%"
          }
        },
        "191641813c504439b9cc610f7bd9fcf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fd7df06314449f995dd4217849a38a7",
            "max": 435755784,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7ed31eaaa1d466297e0b8f397a12594",
            "value": 435755784
          }
        },
        "ec2c8848d8814c989e4714a031755bd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23547b7f5e1243db88f809ec71d3a97c",
            "placeholder": "​",
            "style": "IPY_MODEL_74e9308bc8d54019be58e4949cf6269f",
            "value": " 436M/436M [00:02&lt;00:00, 236MB/s]"
          }
        },
        "c4cd9af8f68b49a385ccc9b0db8a4329": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c825dee2f13a435aad2573a6476684f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbde67809a2542febf325435feae88f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fd7df06314449f995dd4217849a38a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7ed31eaaa1d466297e0b8f397a12594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23547b7f5e1243db88f809ec71d3a97c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74e9308bc8d54019be58e4949cf6269f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "760e0af64a8b4b26acb429f9a1858e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4694afb2f6648e6a15cb25a3deabdfd",
              "IPY_MODEL_b42a8fc4ec3a44a7bd3285962118fc08",
              "IPY_MODEL_e93538224f124fef98c04d1393ebd8e3"
            ],
            "layout": "IPY_MODEL_c388568e8ace4b058b0aecae41347ea9"
          }
        },
        "b4694afb2f6648e6a15cb25a3deabdfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7258d917853945248afed358fc1052fe",
            "placeholder": "​",
            "style": "IPY_MODEL_95e86e23e01246dbbf8a807c789123e4",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "b42a8fc4ec3a44a7bd3285962118fc08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a048f1f216774d489df08d3eed6cf0ac",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53a5a395f9c94150b860ffb97d6c151c",
            "value": 29
          }
        },
        "e93538224f124fef98c04d1393ebd8e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1153ed326a3452882f83a9156525b56",
            "placeholder": "​",
            "style": "IPY_MODEL_2ec4af78b15940c7acdfcbe3238a1b7f",
            "value": " 29.0/29.0 [00:00&lt;00:00, 1.68kB/s]"
          }
        },
        "c388568e8ace4b058b0aecae41347ea9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7258d917853945248afed358fc1052fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95e86e23e01246dbbf8a807c789123e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a048f1f216774d489df08d3eed6cf0ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53a5a395f9c94150b860ffb97d6c151c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e1153ed326a3452882f83a9156525b56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ec4af78b15940c7acdfcbe3238a1b7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fc576a5911e4b3eabcb427a20fddcca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6dc80151e474432a4fe9319bd6225e5",
              "IPY_MODEL_ab13a15b1c08443d8151361d79cce0f1",
              "IPY_MODEL_ad1d398e14e74d4d9b9705e1eaeca4c0"
            ],
            "layout": "IPY_MODEL_1fea7acc27374523a9e46e2ef3718cc7"
          }
        },
        "b6dc80151e474432a4fe9319bd6225e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33ae6fc55bfd405aade725dda00ae365",
            "placeholder": "​",
            "style": "IPY_MODEL_06d6d8769e1f4e56bd329935f92431b3",
            "value": "vocab.txt: 100%"
          }
        },
        "ab13a15b1c08443d8151361d79cce0f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_563cc5116f624a4bacd450331b01735f",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_416e7eeef4e64452959c95810a0ffbd5",
            "value": 213450
          }
        },
        "ad1d398e14e74d4d9b9705e1eaeca4c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f91ba27385144a692b9af1ade12ad0e",
            "placeholder": "​",
            "style": "IPY_MODEL_1eddd377325143918b9ecf58d2b11f09",
            "value": " 213k/213k [00:00&lt;00:00, 2.84MB/s]"
          }
        },
        "1fea7acc27374523a9e46e2ef3718cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33ae6fc55bfd405aade725dda00ae365": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06d6d8769e1f4e56bd329935f92431b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "563cc5116f624a4bacd450331b01735f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "416e7eeef4e64452959c95810a0ffbd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f91ba27385144a692b9af1ade12ad0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1eddd377325143918b9ecf58d2b11f09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1092d94ab1da47129972661bca79f678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73f997a414094693bf653d4d07342235",
              "IPY_MODEL_c1d2848a539147658eac99a6ffb93776",
              "IPY_MODEL_5c203093397747b9be13fcca0bdb056e"
            ],
            "layout": "IPY_MODEL_85a3d034a610470ba1563650ccc5b95e"
          }
        },
        "73f997a414094693bf653d4d07342235": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c97a79cfdd6487c89bda1270f530f1b",
            "placeholder": "​",
            "style": "IPY_MODEL_3683fd8c3e844b92b3de4cf6496a0b3b",
            "value": "tokenizer.json: 100%"
          }
        },
        "c1d2848a539147658eac99a6ffb93776": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_166da64887b747518e8945a89e76cac9",
            "max": 435797,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c2c2f46ff404083a9b58aa9fc85718d",
            "value": 435797
          }
        },
        "5c203093397747b9be13fcca0bdb056e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9af0d0f971b34749972d9b8f021d11c8",
            "placeholder": "​",
            "style": "IPY_MODEL_be9db368c28d4b11be78a309177074b5",
            "value": " 436k/436k [00:00&lt;00:00, 3.61MB/s]"
          }
        },
        "85a3d034a610470ba1563650ccc5b95e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c97a79cfdd6487c89bda1270f530f1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3683fd8c3e844b92b3de4cf6496a0b3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "166da64887b747518e8945a89e76cac9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c2c2f46ff404083a9b58aa9fc85718d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9af0d0f971b34749972d9b8f021d11c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be9db368c28d4b11be78a309177074b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71240df29c124fac8439e45da3cec1cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8aa8f03329c4550be002e265902fbb6",
              "IPY_MODEL_0cb021f9fb9d42eeb7a9865aa5669592",
              "IPY_MODEL_b559c02168f049c097d31b21c6c17f9b"
            ],
            "layout": "IPY_MODEL_1ace84ff3dcb4ba9b30718646254f05d"
          }
        },
        "f8aa8f03329c4550be002e265902fbb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf690c41b74542aca2abfe43bc502a1a",
            "placeholder": "​",
            "style": "IPY_MODEL_483843f70d83498ebb394212951173ba",
            "value": "Epoch 2 of 2: 100%"
          }
        },
        "0cb021f9fb9d42eeb7a9865aa5669592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbd08f353f524ebaa3b3db54c1676575",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ceea9e55b81c437dba528722f36121c1",
            "value": 2
          }
        },
        "b559c02168f049c097d31b21c6c17f9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_604e38262fd14fa4aae7b68dc0ea3a13",
            "placeholder": "​",
            "style": "IPY_MODEL_ae3951fd83c94f4ab70908a4a08ea86b",
            "value": " 2/2 [42:26&lt;00:00, 1272.86s/it]"
          }
        },
        "1ace84ff3dcb4ba9b30718646254f05d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf690c41b74542aca2abfe43bc502a1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "483843f70d83498ebb394212951173ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cbd08f353f524ebaa3b3db54c1676575": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceea9e55b81c437dba528722f36121c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "604e38262fd14fa4aae7b68dc0ea3a13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae3951fd83c94f4ab70908a4a08ea86b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e00ddc2dcd5e425e9a608157da4bcfc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_747ac935bc7d43ac915672f825acb1aa",
              "IPY_MODEL_b6c90e447d5e49f69446227751833901",
              "IPY_MODEL_e0360b3768944a909baaad7e7971a25a"
            ],
            "layout": "IPY_MODEL_d8394eb1ee7d4f4f974ad0209b4519a7"
          }
        },
        "747ac935bc7d43ac915672f825acb1aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9c2870d4c2746199a28a642fd50a967",
            "placeholder": "​",
            "style": "IPY_MODEL_0758692742c8478ab83528972a33a543",
            "value": "Epochs 0/2. Running Loss:    0.9922: 100%"
          }
        },
        "b6c90e447d5e49f69446227751833901": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6eb9446075742cc932234abbde338be",
            "max": 17470,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bfca722076ba40449aeedfee080aaab5",
            "value": 17470
          }
        },
        "e0360b3768944a909baaad7e7971a25a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_375bd00f440847278f0758e8fe2cd8ab",
            "placeholder": "​",
            "style": "IPY_MODEL_0f0579e9abfc487895755650020e55d7",
            "value": " 17470/17470 [21:08&lt;00:00, 13.65it/s]"
          }
        },
        "d8394eb1ee7d4f4f974ad0209b4519a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9c2870d4c2746199a28a642fd50a967": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0758692742c8478ab83528972a33a543": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6eb9446075742cc932234abbde338be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfca722076ba40449aeedfee080aaab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "375bd00f440847278f0758e8fe2cd8ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f0579e9abfc487895755650020e55d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26a3fe5cec4b43b5abec4630f0072bfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e44185a78766441ca633cfdff6db8d9b",
              "IPY_MODEL_fc80aa2859a24d9ba7a90978c7c700ad",
              "IPY_MODEL_4ea2d94d9edd4b61bca0cf497e42a758"
            ],
            "layout": "IPY_MODEL_1021f0c5a24641cc93c57bfbbc2b7694"
          }
        },
        "e44185a78766441ca633cfdff6db8d9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f275f411d0946ab80d81c18b7c1c6df",
            "placeholder": "​",
            "style": "IPY_MODEL_a9d8bac6278e4f749955488164851fc3",
            "value": "Epochs 1/2. Running Loss:    0.1408: 100%"
          }
        },
        "fc80aa2859a24d9ba7a90978c7c700ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33c391544b3242078847cd646250b1ca",
            "max": 17470,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17e4a7a5b61b43f1bac36ea7d1ca9ec3",
            "value": 17470
          }
        },
        "4ea2d94d9edd4b61bca0cf497e42a758": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7b78e995a4740089913a9f29ca0c619",
            "placeholder": "​",
            "style": "IPY_MODEL_6cb62d37b0284413baedbf0378e600eb",
            "value": " 17470/17470 [21:06&lt;00:00, 15.90it/s]"
          }
        },
        "1021f0c5a24641cc93c57bfbbc2b7694": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f275f411d0946ab80d81c18b7c1c6df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9d8bac6278e4f749955488164851fc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33c391544b3242078847cd646250b1ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17e4a7a5b61b43f1bac36ea7d1ca9ec3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7b78e995a4740089913a9f29ca0c619": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cb62d37b0284413baedbf0378e600eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73d38ffbbd7948349e3bea09bfca5577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3a57b9b1d6a409aae22d6dbde3212fc",
              "IPY_MODEL_077132bb0066475c855cedb3471b64a5",
              "IPY_MODEL_7d221bbfc8c5489aa53d5e929c8208aa"
            ],
            "layout": "IPY_MODEL_e326c8cb76f04d3ca798987812b33655"
          }
        },
        "c3a57b9b1d6a409aae22d6dbde3212fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76746b6894b14f4999b3fe10f5141c3a",
            "placeholder": "​",
            "style": "IPY_MODEL_26c3f922361c462b8ff3a1a661b71724",
            "value": "Running Evaluation: 100%"
          }
        },
        "077132bb0066475c855cedb3471b64a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94aae4ebc8ba4f92be77bb79b9cf2f2e",
            "max": 864,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7105c68d674847c2b9a9a51702602014",
            "value": 864
          }
        },
        "7d221bbfc8c5489aa53d5e929c8208aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5733371449f044a7a28b225c21e20ecf",
            "placeholder": "​",
            "style": "IPY_MODEL_233126b957cf42cebadcb1fd7705fddc",
            "value": " 864/864 [01:00&lt;00:00, 14.70it/s]"
          }
        },
        "e326c8cb76f04d3ca798987812b33655": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76746b6894b14f4999b3fe10f5141c3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26c3f922361c462b8ff3a1a661b71724": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94aae4ebc8ba4f92be77bb79b9cf2f2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7105c68d674847c2b9a9a51702602014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5733371449f044a7a28b225c21e20ecf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "233126b957cf42cebadcb1fd7705fddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f90928ef69b40a5a209edecba3aa3cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eec117dbdbe14429968905749d842926",
              "IPY_MODEL_6c4c8a7645a1469cbe7dc9018716be00",
              "IPY_MODEL_f8872b4a2bea422e8cc04a4b344fd9de"
            ],
            "layout": "IPY_MODEL_8ea0a2688687437a9591b297effb2281"
          }
        },
        "eec117dbdbe14429968905749d842926": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4de373b692d4cf4aa9c34e933462124",
            "placeholder": "​",
            "style": "IPY_MODEL_6072f906d90a436e8beeac084b5c3c9a",
            "value": "Running Prediction: 100%"
          }
        },
        "6c4c8a7645a1469cbe7dc9018716be00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68e69d988f3f4f44a389ef9c3d480d28",
            "max": 1540,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c150cfe30c4f4c488fee984fd20b8f2b",
            "value": 1540
          }
        },
        "f8872b4a2bea422e8cc04a4b344fd9de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4540c3fd664488a8027e5860db77427",
            "placeholder": "​",
            "style": "IPY_MODEL_f9530f2ed6c4436f915060f31426d469",
            "value": " 1540/1540 [01:48&lt;00:00, 14.32it/s]"
          }
        },
        "8ea0a2688687437a9591b297effb2281": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4de373b692d4cf4aa9c34e933462124": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6072f906d90a436e8beeac084b5c3c9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68e69d988f3f4f44a389ef9c3d480d28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c150cfe30c4f4c488fee984fd20b8f2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a4540c3fd664488a8027e5860db77427": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9530f2ed6c4436f915060f31426d469": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a09cc3c521964d2aa3dd97d65a1614d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e83b175a1ba4cf6b2bb19c93b88a599",
              "IPY_MODEL_50af2b78f2de4438a2ce4fa5c1940b9f",
              "IPY_MODEL_4895f07c097744ce8df0dea6dbf7186d"
            ],
            "layout": "IPY_MODEL_b54bfdc879f54c1c91918e17db35a769"
          }
        },
        "8e83b175a1ba4cf6b2bb19c93b88a599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18319a74c98b43999f1924d97764b363",
            "placeholder": "​",
            "style": "IPY_MODEL_76eacced6229481bba7296347b4ae869",
            "value": "Running Prediction: 100%"
          }
        },
        "50af2b78f2de4438a2ce4fa5c1940b9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b11fe922afdf40fbb4cd4984799fb7ad",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21b1923aad424794bb7033f8930d81c1",
            "value": 1
          }
        },
        "4895f07c097744ce8df0dea6dbf7186d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9204f250440c4091ac001dcaee7bfbdd",
            "placeholder": "​",
            "style": "IPY_MODEL_52a3e89247844acdbe02cdd660a38b09",
            "value": " 1/1 [00:00&lt;00:00, 13.73it/s]"
          }
        },
        "b54bfdc879f54c1c91918e17db35a769": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18319a74c98b43999f1924d97764b363": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76eacced6229481bba7296347b4ae869": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b11fe922afdf40fbb4cd4984799fb7ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21b1923aad424794bb7033f8930d81c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9204f250440c4091ac001dcaee7bfbdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52a3e89247844acdbe02cdd660a38b09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98cf748a7b0444bf9e9c76e0ecbbdde3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e20fdc46b0b74ddb9302ef77c078e18d",
              "IPY_MODEL_cf881d5e54ab4d6ab65f42baa27ebecd",
              "IPY_MODEL_82e3c92a97be4ed8a5631ab9a907a52e"
            ],
            "layout": "IPY_MODEL_a9ff7f042eeb4e84a13de20f31ecea47"
          }
        },
        "e20fdc46b0b74ddb9302ef77c078e18d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2830ea1c89e04a0f8d4ab013bb8143c4",
            "placeholder": "​",
            "style": "IPY_MODEL_6ed157457cd34a2fb76e66d6044d3481",
            "value": "Running Prediction: 100%"
          }
        },
        "cf881d5e54ab4d6ab65f42baa27ebecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba7f0fa5f3f8493597a8450152800834",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7b2cf11fad843d3857a45e86b1e9db1",
            "value": 1
          }
        },
        "82e3c92a97be4ed8a5631ab9a907a52e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99bf3079226c4ce2a65e2f1addd4c210",
            "placeholder": "​",
            "style": "IPY_MODEL_caa8de8f14784a49ae66a743f28e4bc7",
            "value": " 1/1 [00:00&lt;00:00, 13.67it/s]"
          }
        },
        "a9ff7f042eeb4e84a13de20f31ecea47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2830ea1c89e04a0f8d4ab013bb8143c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ed157457cd34a2fb76e66d6044d3481": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba7f0fa5f3f8493597a8450152800834": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7b2cf11fad843d3857a45e86b1e9db1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99bf3079226c4ce2a65e2f1addd4c210": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "caa8de8f14784a49ae66a743f28e4bc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e20d25db28a345a4b315c67d254650bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd350df69b854725b9c6bce503209bb5",
              "IPY_MODEL_48518da9fe464db29b7d091fbfd05df6",
              "IPY_MODEL_27a0309b731c40c6a09e425a79bec030"
            ],
            "layout": "IPY_MODEL_274ecc178f454276bdd9a0a7acbd1f81"
          }
        },
        "cd350df69b854725b9c6bce503209bb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2710f27b21974ac99f6364bfce66bde7",
            "placeholder": "​",
            "style": "IPY_MODEL_4420ec4db41447f2ac478c5c00605302",
            "value": "Running Prediction: 100%"
          }
        },
        "48518da9fe464db29b7d091fbfd05df6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33b05b0a5a7a4a089e7d10c941e4e841",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fdca0330f11944d3be3dd33d6bd8b9f2",
            "value": 1
          }
        },
        "27a0309b731c40c6a09e425a79bec030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e96f163c9ad84fb0a9751e088894e6a6",
            "placeholder": "​",
            "style": "IPY_MODEL_849d452ef33448bda33ec02eeeb90f2e",
            "value": " 1/1 [00:00&lt;00:00, 13.27it/s]"
          }
        },
        "274ecc178f454276bdd9a0a7acbd1f81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2710f27b21974ac99f6364bfce66bde7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4420ec4db41447f2ac478c5c00605302": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33b05b0a5a7a4a089e7d10c941e4e841": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdca0330f11944d3be3dd33d6bd8b9f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e96f163c9ad84fb0a9751e088894e6a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "849d452ef33448bda33ec02eeeb90f2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f69ff569ff2487f9578e9eee96b47db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77809fffc6a54aa8b75463fe1521e659",
              "IPY_MODEL_e2bc63cbec0c49eeaafc89eaa03e2237",
              "IPY_MODEL_db768356ea954ccf832d18cc9b01b15f"
            ],
            "layout": "IPY_MODEL_20c71a0c22104b5e877c8fb9ae405c27"
          }
        },
        "77809fffc6a54aa8b75463fe1521e659": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1fa40c95e604589a8fac61d4c4d8a4f",
            "placeholder": "​",
            "style": "IPY_MODEL_d7b74ed3f770448191bba4d3ff824253",
            "value": "Running Prediction: 100%"
          }
        },
        "e2bc63cbec0c49eeaafc89eaa03e2237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73c05b52847a4c77869210b76ee09c1e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_37c6387f55b64b9a997c669b7a870aae",
            "value": 1
          }
        },
        "db768356ea954ccf832d18cc9b01b15f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6593f2ff20684b2b955f05908990338d",
            "placeholder": "​",
            "style": "IPY_MODEL_393c07c8729644649ce0ea4b242bf65b",
            "value": " 1/1 [00:00&lt;00:00,  9.07it/s]"
          }
        },
        "20c71a0c22104b5e877c8fb9ae405c27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1fa40c95e604589a8fac61d4c4d8a4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7b74ed3f770448191bba4d3ff824253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73c05b52847a4c77869210b76ee09c1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37c6387f55b64b9a997c669b7a870aae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6593f2ff20684b2b955f05908990338d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "393c07c8729644649ce0ea4b242bf65b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efecfbd613514644bea92e13cd8dc0db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_089c2f8653dc45189e4ba8cd31924dfd",
              "IPY_MODEL_6ec38247b8224a6e923b6c206fc4e7d3",
              "IPY_MODEL_bef5d34a129c4261b3a8381281b4b4ec"
            ],
            "layout": "IPY_MODEL_1eb7a89fe25e44898492a56b6a18764c"
          }
        },
        "089c2f8653dc45189e4ba8cd31924dfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d616323fcb24a4fbcc3547c31f7fa52",
            "placeholder": "​",
            "style": "IPY_MODEL_fbdcb801ec9442de83d8d2707fd13c8f",
            "value": "Running Prediction: 100%"
          }
        },
        "6ec38247b8224a6e923b6c206fc4e7d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8556bd1578248ad86a9cbb99c595a57",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb6917b62bdc41d397ff6cbb0d25a1e9",
            "value": 1
          }
        },
        "bef5d34a129c4261b3a8381281b4b4ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a35c1a71a39948909eeb1ccebfcd7596",
            "placeholder": "​",
            "style": "IPY_MODEL_ba7ffc4efd664920aa7b7f822521780d",
            "value": " 1/1 [00:00&lt;00:00, 10.79it/s]"
          }
        },
        "1eb7a89fe25e44898492a56b6a18764c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d616323fcb24a4fbcc3547c31f7fa52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbdcb801ec9442de83d8d2707fd13c8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8556bd1578248ad86a9cbb99c595a57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb6917b62bdc41d397ff6cbb0d25a1e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a35c1a71a39948909eeb1ccebfcd7596": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba7ffc4efd664920aa7b7f822521780d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "070c2771cb664c8fb839bdbd0899c820": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc7f564d79b342a99caf778a22b1daae",
              "IPY_MODEL_d1840ee4b0ad47259cd36ae27827900e",
              "IPY_MODEL_7776613a85f348b2b060fcb82f73a3ca"
            ],
            "layout": "IPY_MODEL_8b49aade44034f019d196baf626af470"
          }
        },
        "bc7f564d79b342a99caf778a22b1daae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c3d41a0d2da4e7b91798c96b35353fd",
            "placeholder": "​",
            "style": "IPY_MODEL_02da86c48af049baac5b3aad0e119ed9",
            "value": "Running Prediction: 100%"
          }
        },
        "d1840ee4b0ad47259cd36ae27827900e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_659a265b18434d02876baf86852b5ab4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e46d09b026d643f9a1e6bf73f06b3cb4",
            "value": 1
          }
        },
        "7776613a85f348b2b060fcb82f73a3ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22d5bac3f75f4d9bbb60026998e880f4",
            "placeholder": "​",
            "style": "IPY_MODEL_67c3e45f310a4fb89a7b828432cb14aa",
            "value": " 1/1 [00:00&lt;00:00, 17.78it/s]"
          }
        },
        "8b49aade44034f019d196baf626af470": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c3d41a0d2da4e7b91798c96b35353fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02da86c48af049baac5b3aad0e119ed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "659a265b18434d02876baf86852b5ab4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e46d09b026d643f9a1e6bf73f06b3cb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22d5bac3f75f4d9bbb60026998e880f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67c3e45f310a4fb89a7b828432cb14aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}